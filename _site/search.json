[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RFM Project",
    "section": "",
    "text": "Welcome to RFM Project homepage. In this website, you will find my finding prepared for this project."
  },
  {
    "objectID": "Market.html",
    "href": "Market.html",
    "title": "Makert Survey",
    "section": "",
    "text": "To effectively organize and communicate the complex thought process behind why Chinese HNW individuals move their money to Singapore, I created Flowchart. This flowchart helps to:\n\nVisually map the key motivations (such as wealth management, tax benefits, lifestyle advantages, and legacy planning) that drive this financial movement.\nClarify the relationships between client needs, Singaporeâ€™s value proposition, and the strategic advisory angles we can take.\nSupport client-facing engagement by simplifying the narrative for Life Inc advisors\nGuide my survey design and presentation content, ensuring all relevant themesâ€”such as trends, preferences, and asset relocation methodsâ€”are covered logically and comprehensively.\n\nIt acts as both a research framework and a client communication tool to align our understanding and offerings with the target marketâ€™s aspirations.\n\n\n\n\n\n\n\n\nTo support the research in Flowchart, I developed a custom web crawler to systematically collect qualitative and contextual data from Chinese-language sources such as forums (e.g., Zhihu, Xueqiu), financial news sites (e.g., Eastern Wealth), and financial blogs. These platforms offer authentic discussions and opinions from Chinese citizens regarding overseas wealth movementâ€”especially to Singapore.\nMy spider focused on keywords that map directly to the key branches of the flowchart, such as:\n\nâ€œæ–°åŠ å¡ è´¢å¯Œç®¡ç†â€ (Singapore wealth management)\nâ€œä¸­å›½äººç§»æ°‘æ–°åŠ å¡â€ (Chinese migration to Singapore)\nâ€œç¨åŠ¡ä¼˜åŒ– æ–°åŠ å¡â€ (Tax optimization Singapore)\nâ€œèµ„äº§é…ç½® æµ·å¤–â€ (Offshore asset allocation)\n\n\n\n\n\n\n\nWhy choose spider instead of survey\n\n\n\n\nUnderstand Real Conversations First: The spider captures what people are already saying online, so I donâ€™t assume or guess their opinions.\nGather Rich and Honest Insights: Online posts reveal detailed reasons, emotions, and comparisons that people may not share in surveys.\nReach More People Easily: Itâ€™s hard to get high-net-worth individuals to answer surveys, but the spider can access public content instantly.\nBuild a Better Survey Later: By analyzing online themes first, I can design a more focused and relevant survey based on real concerns.\n\n\n\n\n\n\nWe implemented a Python-based web crawler using the SerpAPI platform to scrape real-time search results. The following code initializes the necessary libraries and parameters used in the process.\n\n\nClick to view code\nimport requests\nimport json\nimport time\nimport pandas as pd\nfrom datetime import datetime\nimport os\nimport openpyxl\nimport re\nfrom collections import Counter\nimport jieba\n\n# ç”¨æˆ·é…ç½®åŒºåŸŸ\nSERPAPI_KEY = \"84c798a8d45d1e7cae0b18df778ac06bf2c6169f0249e40756aea0b9d6cd4749\"  \nRESULTS_PER_KEYWORD = 20\nDELAY_BETWEEN_REQUESTS = 1\nDELAY_BETWEEN_KEYWORDS = 2\n\n\n\n\n\nThe keywords used by the crawler are carefully chosen to reflect real user concerns about wealth migration. These terms guide the spider to relevant discussions, articles, and posts.\n\n\nClick to view code\nkeywords = [\n    \"ä¸­å›½é«˜å‡€å€¼äººå£« æ–°åŠ å¡\",\n    \"ä¸­å›½å¯Œäºº èµ„äº§é…ç½® æ–°åŠ å¡\", \n    \"ä¸­å›½å¯Œè±ª ä¸ºä»€ä¹ˆç§»æ°‘æ–°åŠ å¡\",\n    \"ä¸­å›½é«˜å‡€å€¼å®¢æˆ· ç¦»å²¸è´¦æˆ·\",\n    \"æ–°åŠ å¡ CRS é¿ç¨\",\n    \"ä¸­å›½ å®¶æ—ä¿¡æ‰˜ æ–°åŠ å¡\",\n    \"ä¸­å›½é«˜å‡€å€¼äººå£« å­å¥³æ•™è‚² æ–°åŠ å¡\",\n    \"ä¸­å›½ç§»æ°‘æ–°åŠ å¡ è´¢å¯Œç®¡ç†\"\n]\n\n\n\n\nClick to view code\ndef serpapi_search(query, api_key, num=20, start=0):\n    url = \"https://serpapi.com/search\"\n    params = {\n        'q': query,\n        'api_key': api_key,\n        'engine': 'google',\n        'num': min(num, 100),\n        'start': start,\n        'hl': 'zh-cn',\n        'gl': 'cn'\n    }\n    try:\n        response = requests.get(url, params=params)\n        response.raise_for_status()\n        return response.json()\n    except requests.exceptions.RequestException as e:\n        print(f\"æœç´¢è¯·æ±‚å¤±è´¥: {e}\")\n        return {}\n\ndef extract_search_results(results, keyword):\n    extracted_results = []\n    organic_results = results.get('organic_results', [])\n    for item in organic_results:\n        result_info = {\n            'keyword': keyword,\n            'title': item.get('title', ''),\n            'url': item.get('link', ''),\n            'snippet': item.get('snippet', ''),\n            'displayed_link': item.get('displayed_link', ''),\n            'position': item.get('position', 0),\n            'search_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        }\n        extracted_results.append(result_info)\n    return extracted_results\n\n\n\n\nClick to view code\ndef search_keyword_with_pagination(keyword, api_key, total_results=20):\n    all_results = []\n    results_per_page = 10\n    for start in range(0, total_results, results_per_page):\n        remaining = total_results - start\n        num_to_get = min(results_per_page, remaining)\n        print(f\"    è·å–ç¬¬ {start+1}-{start+num_to_get} æ¡ç»“æœ...\")\n        results = serpapi_search(keyword, api_key, num=num_to_get, start=start)\n        if not results or 'organic_results' not in results:\n            print(f\"æ²¡æœ‰æ›´å¤šç»“æœ\")\n            break\n        extracted = extract_search_results(results, keyword)\n        all_results.extend(extracted)\n        if len(extracted) &lt; num_to_get:\n            break\n        time.sleep(DELAY_BETWEEN_REQUESTS)\n    return all_results\n\n\n\n\nClick to view code\ndef main():\n    print(\"å¼€å§‹æœç´¢...\")\n    print(f\"æœç´¢å…³é”®è¯æ•°é‡: {len(keywords)}\")\n    print(f\"æ¯ä¸ªå…³é”®è¯è·å–ç»“æœæ•°: {RESULTS_PER_KEYWORD}\")\n    print(\"-\" * 50)\n    \n    all_search_results = []\n    for i, keyword in enumerate(keywords, 1):\n        print(f\"[{i}/{len(keywords)}] æœç´¢å…³é”®è¯: {keyword}\")\n        try:\n            results = search_keyword_with_pagination(keyword, SERPAPI_KEY, RESULTS_PER_KEYWORD)\n            all_search_results.extend(results)\n            print(f\"æ‰¾åˆ° {len(results)} ä¸ªç»“æœ\")\n            if i &lt; len(keywords):\n                print(f\"ç­‰å¾… {DELAY_BETWEEN_KEYWORDS} ç§’...\")\n                time.sleep(DELAY_BETWEEN_KEYWORDS)\n        except Exception as e:\n            print(f\"é”™è¯¯: {e}\")\n            continue\n    print(\"-\" * 50)\n    print(f\"æ€»å…±æ‰¾åˆ° {len(all_search_results)} ä¸ªç»“æœ\")\n    return all_search_results\n\nsearch_results = main()\n\n\n\n\n\nThe crawler iterates through each keyword, sends queries to SerpAPI, extracts content (including titles, URLs, and full texts), and saves them into a structured format for further processing. This automation ensures scalability and coverage.\n\n\nClick to view code\nif search_results:\n    df_full = pd.DataFrame(search_results)\n    df_unique = df_full.drop_duplicates(subset=['url'], keep='first')\n    print(f\"åŸå§‹ç»“æœ: {len(df_full)}ï¼Œå»é‡å: {len(df_unique)}\")\n\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    df_simple = df_unique[['keyword', 'title', 'url', 'search_timestamp']].copy()\n\n    # ä¿å­˜åˆ°æŒ‡å®š Dataset\n    save_dir = os.path.expanduser(\"/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset\")\n    os.makedirs(save_dir, exist_ok=True)\n\n    df_unique.to_csv(os.path.join(save_dir, f\"search_results_full_{timestamp}.csv\"), index=False, encoding='utf-8-sig')\n    df_simple.to_csv(os.path.join(save_dir, f\"search_results_simple_{timestamp}.csv\"), index=False, encoding='utf-8-sig')\n\n    with pd.ExcelWriter(os.path.join(save_dir, f\"search_results_{timestamp}.xlsx\"), engine='openpyxl') as writer:\n        df_unique.to_excel(writer, sheet_name='å®Œæ•´æ•°æ®', index=False)\n        df_simple.to_excel(writer, sheet_name='ç®€åŒ–æ•°æ®', index=False)\n\n    print(f\"æ–‡ä»¶ä¿å­˜è·¯å¾„: {save_dir}\")\n\n    print(\"\\næ•°æ®é¢„è§ˆ:\")\n    print(df_simple.head())\n\n    print(\"\\næ¯ä¸ªå…³é”®è¯çš„ç»“æœæ•°é‡:\")\n    print(df_unique['keyword'].value_counts())\n\n    globals()['search_data'] = df_unique\n    globals()['search_data_simple'] = df_simple\n\n    print(\"\\nå˜é‡å·²åŠ è½½: search_data, search_data_simple\")\nelse:\n    print(\"æ²¡æœ‰æœç´¢ç»“æœ\")\n    globals()['search_data'] = pd.DataFrame()\n    globals()['search_data_simple'] = pd.DataFrame()\n\n\n\n\n\nOnce raw JSON responses are collected, we convert them into a tabular format using pandas. This step prepares the dataset for text cleaning and analysis.\n\n\nClick to view code\nfrom bs4 import BeautifulSoup\nimport chardet\n\n# æ–‡ä»¶è·¯å¾„è®¾ç½®\ninput_file = \"/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset/search_results_20250706_192824.xlsx\"\noutput_file = \"/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset/search_results_with_content.xlsx\"\n\n# è¯»å–ç®€åŒ–æ•°æ®å·¥ä½œè¡¨\ndf = pd.read_excel(input_file, sheet_name=\"ç®€åŒ–æ•°æ®\")\n\n# æ­£æ–‡æå–å‡½æ•°\ndef fetch_article_content(url, timeout=10):\n    try:\n        response = requests.get(url, timeout=timeout, headers={\"User-Agent\": \"Mozilla/5.0\"})\n        detected = chardet.detect(response.content)\n        response.encoding = detected['encoding'] or 'utf-8'\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        paragraphs = soup.find_all('p')\n        content = '\\n'.join(p.get_text(strip=True) for p in paragraphs)\n        return content if len(content) &gt; 50 else None\n    except Exception:\n        return None\n\n# çˆ¬å–å†…å®¹\nprint(\"å¼€å§‹æŠ“å–æ­£æ–‡å†…å®¹...\")\ndf[\"content\"] = df[\"url\"].apply(fetch_article_content)\nprint(\"æ­£æ–‡æŠ“å–å®Œæˆï¼Œå¼€å§‹ä¿å­˜æ–‡ä»¶...\")\n\n# ä¿å­˜ä¸ºæ–°çš„ Excel æ–‡ä»¶\ndf.to_excel(output_file, index=False)\nprint(f\"æ–‡ä»¶å·²ä¿å­˜è‡³ï¼š{output_file}\")\n\n\n\n\n\n\n\nClick to view code\n# è¾“å…¥è¾“å‡ºè·¯å¾„\ninput_file = \"/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset/search_results_with_content.xlsx\"\noutput_file = \"/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset/search_results_cleaned.xlsx\"\n\n# åŠ è½½ Excel æ–‡ä»¶\ndf = pd.read_excel(input_file)\n\n# å®šä¹‰æ— æ•ˆå†…å®¹å…³é”®è¯\nad_keywords = [\n    \"å…è´£å£°æ˜\", \"å¹¿å‘Šåˆä½œ\", \"è”ç³»ç®¡ç†å‘˜\", \"è¯·åœ¨å¾®ä¿¡ä¸­æ‰“å¼€\", \"æœ¬ç«™æ‰€æœ‰æ–‡ç« \",\n    \"æŠ±æ­‰\", \"é¡µé¢ä¸å­˜åœ¨\", \"å‡ºé”™\", \"404\", \"è¯·è¾“å…¥éªŒè¯ç \",\n    \"ç™»å½•æŸ¥çœ‹å…¨æ–‡\", \"Oops\", \"Something went wrong\", \"è®¿é—®å—é™\"\n]\n\n# åˆ¤æ–­æ˜¯å¦ä¸ºæ— æ•ˆæ­£æ–‡\ndef is_invalid(text):\n    if pd.isna(text):\n        return True\n    if len(text.strip()) &lt; 100:\n        return True\n    if any(kw in text for kw in ad_keywords):\n        return True\n    return False\n\n# æ·»åŠ æ ‡è®°åˆ—\ndf[\"invalid\"] = df[\"content\"].apply(is_invalid)\n\n# ä¿ç•™æœ‰æ•ˆæ­£æ–‡å†…å®¹\ndf_cleaned = df[~df[\"invalid\"]].drop(columns=[\"invalid\"]).copy()\n\n# ä¿å­˜æ¸…æ´—åçš„ç»“æœ\ndf_cleaned.to_excel(output_file, index=False)\nprint(f\"æ¸…æ´—å®Œæˆï¼Œå·²ä¿å­˜ä¸ºï¼š{output_file}\")\n\n\n\n\nClick to view code\n# è¯»å– Excel æ–‡ä»¶\ndf = pd.read_excel(\"search_results_cleaned.xlsx\")\n\n# 1. æŸ¥çœ‹è¡Œæ•°å’Œåˆ—æ•°\nprint(\"è¡Œæ•° Ã— åˆ—æ•°:\", df.shape)\n\n# 2. æŸ¥çœ‹åˆ—åå’Œç±»å‹\nprint(\"\\nåˆ—åä¸æ•°æ®ç±»å‹:\")\nprint(df.dtypes)\n\n# 3. å¿«é€Ÿæ¦‚è§ˆæ¯åˆ—çš„å‰å‡ è¡Œï¼ˆç»“æ„ + å€¼ï¼‰\nprint(\"\\næ ·æœ¬é¢„è§ˆ:\")\nprint(df.head())\n\n# 4. ç¼ºå¤±å€¼ç»Ÿè®¡ï¼ˆNA å€¼ï¼‰\nprint(\"\\nç¼ºå¤±å€¼ç»Ÿè®¡:\")\nprint(df.isna().sum())\n\n\nè¡Œæ•° Ã— åˆ—æ•°: (90, 6)\n\nåˆ—åä¸æ•°æ®ç±»å‹:\nkeyword             object\ntitle               object\nurl                 object\nsearch_timestamp    object\ncontent             object\ninvalid               bool\ndtype: object\n\næ ·æœ¬é¢„è§ˆ:\n       keyword                           title  \\\n0  ä¸­å›½é«˜å‡€å€¼äººå£« æ–°åŠ å¡          æ–°åŠ å¡é‡‘èæœºåˆ¶ç¨³å®šå¸å¼•æœ€å¤šé«˜å‡€å€¼äººå£«è€ƒè™‘ç§»å±…   \n1  ä¸­å›½é«˜å‡€å€¼äººå£« æ–°åŠ å¡          æˆ‘å›½è¶…é«˜å‡€å€¼äººç¾¤è®¾ç«‹å®¶æ—åŠå…¬å®¤ç°çŠ¶åˆ†æä¸åº”å¯¹   \n2  ä¸­å›½é«˜å‡€å€¼äººå£« æ–°åŠ å¡  ä¸­å›½é«˜å‡€å€¼äººå£«åœ¨æ–°åŠ å¡è®¾ç«‹å®¶æ—åŠå…¬å®¤å’ŒæŠ•èµ„åˆåˆ›ä¼ä¸šçš„ ...   \n3  ä¸­å›½é«˜å‡€å€¼äººå£« æ–°åŠ å¡        é«˜å‡€å€¼äººå£«ä¸ºä½•çº·çº·é€‰æ‹©æ–°åŠ å¡ï¼Ÿ_ç§»æ°‘_æ•™è‚²_å·¥ä½œ   \n4  ä¸­å›½é«˜å‡€å€¼äººå£« æ–°åŠ å¡    èšç„¦å®¶åŠ| äºšæ´²è¶…é«˜å‡€å€¼äººç¾¤å¢é•¿æˆ–å°†æ¨åŠ¨å®¶æ—åŠå…¬å®¤çš„å‘å±•   \n\n                                                 url     search_timestamp  \\\n0  https://www.zaobao.com/finance/singapore/story...  2025-07-06 19:19:49   \n1  http://cel.cn/List/FullText?articleId=d37c148c...  2025-07-06 19:19:49   \n2  https://fargowealth.com/en/home/cfsj/cfsj_deta...  2025-07-06 19:19:49   \n3         https://www.sohu.com/a/843292362_121963266  2025-07-06 19:19:49   \n4  https://www.bloombergchina.com/blog/asias-ultr...  2025-07-06 19:19:49   \n\n                                             content  invalid  \n0  \\n\\næ–°åŠ å¡ç¨³å®šçš„é‡‘èæœºåˆ¶å’ŒåŒºåŸŸè”é€šæ€§å¯¹å…¨çƒå¯Œè±ªå…·æœ‰å¼ºå¤§å¸å¼•åŠ›ï¼Œæ˜¯æœ€å¤šå¯Œè±ªé¦–é€‰çš„ç§»å±…ç›®çš„åœ°...    False  \n1  è‚–äº¬2024-07-08æµè§ˆé‡ï¼š1498\\næˆ‘å›½è¶…é«˜å‡€å€¼äººç¾¤è®¾ç«‹å®¶æ—åŠå…¬å®¤çš„éœ€æ±‚æ˜¯å®¢è§‚å­˜åœ¨çš„...    False  \n2  ä¸­å›½é«˜å‡€å€¼äººå£«åœ¨æ–°åŠ å¡è®¾ç«‹å®¶æ—åŠå…¬å®¤å’ŒæŠ•èµ„åˆåˆ›ä¼ä¸šçš„æœ€æ–°è¶‹åŠ¿\\näº”å¹´å‰ï¼Œæ¢ä¿¡å†›å› å¥åº·åŸå› ç¦»å¼€...    False  \n3  è¿‘å¹´æ¥ï¼Œè¶Šæ¥è¶Šå¤šçš„å›½å†…ä¼ä¸šå®¶å’Œé«˜å‡€å€¼äººå£«å°†ç›®å…‰æŠ•å‘æ–°åŠ å¡ï¼Œå°¤å…¶æ˜¯é‚£äº›æ­£åœ¨è€ƒè™‘ç§»æ°‘æˆ–å­å¥³æ•™è‚²çš„...    False  \n4  æœ¬æ–‡ç”±å½­åšè¡Œä¸šç ”ç©¶é«˜çº§åˆ†æå¸ˆé»„é¢–çŠï¼ˆSharnie Wongï¼‰æ’°å†™ï¼Œé¦–å‘äºå½­åšç»ˆç«¯ã€‚\\nç‘é“¶...    False  \n\nç¼ºå¤±å€¼ç»Ÿè®¡:\nkeyword             0\ntitle               0\nurl                 0\nsearch_timestamp    0\ncontent             0\ninvalid             0\ndtype: int64\n\n\n\n\nClick to view code\ndf_cleaned = df[~df[\"invalid\"] & df[\"content\"].notna()].copy()\ndf_cleaned.to_excel(\"search_results_cleaned.xlsx\", index=False)\n\n\n\n\n\nThe raw text contains punctuation, redundant whitespace, and occasionally malformed encoding. We apply basic cleaning to ensure the corpus is suitable for keyword extraction and topic modeling.\n\n\nClick to view code\n# 1. åŠ è½½æ¸…æ´—åçš„æ–‡ç« æ•°æ®\ndf = pd.read_excel(\"Dataset/search_results_cleaned.xlsx\")\ndf = df[df[\"content\"].notna() & (df[\"content\"].str.strip() != \"\")]\n\n# 2. è¯»å–åœç”¨è¯è¡¨\nwith open(\"Dataset/cn_stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n    stopwords = set([line.strip() for line in f])\n\n# 3. æ¸…æ´—å‡½æ•°\ndef clean_text(s):\n    s = re.sub(r'&lt;.*?&gt;', '', s)  # HTMLæ ‡ç­¾\n    s = re.sub(r'[a-zA-Z]+', '', s)  # è‹±æ–‡\n    s = re.sub(r'[\\d\\-:/\\.å¹´æœˆæ—¥\\s]+', '', s)  # æ•°å­—ä¸æ—¥æœŸ\n    s = re.sub(r'[\\u0000-\\u007F]+', '', s)  # ASCIIç¬¦å·\n    return s.strip()\n\n# 4. åˆ†å¥ + æ¸…æ´— + åˆ†è¯ + å»åœç”¨è¯\nwords = []\nfor content in df[\"content\"]:\n    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', content)\n    for s in sentences:\n        s_clean = clean_text(s)\n        if len(s_clean) &gt;= 5:\n            segs = jieba.cut(s_clean)\n            words += [w for w in segs if len(w) &gt; 1 and w not in stopwords and re.match(r'[\\u4e00-\\u9fff]+', w)]\n\n# 5. è¯é¢‘ç»Ÿè®¡\nword_freq = Counter(words)\ndf_freq = pd.DataFrame(word_freq.most_common(100), columns=[\"Word\", \"Frequency\"])\n\n\n\n\nClick to view code\n# 6. ä¿å­˜åˆ°æ–‡ä»¶\ndf_freq.to_excel(\"Dataset/word_frequency_cleaned_with_stopwords.xlsx\", index=False)\n\n\n\n\nClick to view code\n# 7. æ‰“å°ç»“æœ\nprint(df_freq)\n\n\n   Word  Frequency\n0   æ–°åŠ å¡       1895\n1    ä¿¡æ‰˜       1301\n2    å®¶æ—       1028\n3    èµ„äº§        947\n4    æŠ•èµ„        922\n..  ...        ...\n95   æ–¹å¼        133\n96   ç›®å‰        132\n97   é¡¾é—®        132\n98   æ•°é‡        128\n99   äº§å“        126\n\n[100 rows x 2 columns]\n\n\n\n\n\nWe extract high-frequency terms to detect common concerns and motivations. This analysis surfaces dominant themes such as â€œwealth managementâ€, â€œimmigrationâ€, and â€œSingapore advantageâ€."
  },
  {
    "objectID": "Market.html#background",
    "href": "Market.html#background",
    "title": "Makert Survey",
    "section": "",
    "text": "To effectively organize and communicate the complex thought process behind why Chinese HNW individuals move their money to Singapore, I created Flowchart. This flowchart helps to:\n\nVisually map the key motivations (such as wealth management, tax benefits, lifestyle advantages, and legacy planning) that drive this financial movement.\nClarify the relationships between client needs, Singaporeâ€™s value proposition, and the strategic advisory angles we can take.\nSupport client-facing engagement by simplifying the narrative for Life Inc advisors\nGuide my survey design and presentation content, ensuring all relevant themesâ€”such as trends, preferences, and asset relocation methodsâ€”are covered logically and comprehensively.\n\nIt acts as both a research framework and a client communication tool to align our understanding and offerings with the target marketâ€™s aspirations."
  },
  {
    "objectID": "Market.html#data-processing",
    "href": "Market.html#data-processing",
    "title": "Makert Survey",
    "section": "",
    "text": "To support the research in Flowchart, I developed a custom web crawler to systematically collect qualitative and contextual data from Chinese-language sources such as forums (e.g., Zhihu, Xueqiu), financial news sites (e.g., Eastern Wealth), and financial blogs. These platforms offer authentic discussions and opinions from Chinese citizens regarding overseas wealth movementâ€”especially to Singapore.\nMy spider focused on keywords that map directly to the key branches of the flowchart, such as:\n\nâ€œæ–°åŠ å¡ è´¢å¯Œç®¡ç†â€ (Singapore wealth management)\nâ€œä¸­å›½äººç§»æ°‘æ–°åŠ å¡â€ (Chinese migration to Singapore)\nâ€œç¨åŠ¡ä¼˜åŒ– æ–°åŠ å¡â€ (Tax optimization Singapore)\nâ€œèµ„äº§é…ç½® æµ·å¤–â€ (Offshore asset allocation)\n\n\n\n\n\n\n\nWhy choose spider instead of survey\n\n\n\n\nUnderstand Real Conversations First: The spider captures what people are already saying online, so I donâ€™t assume or guess their opinions.\nGather Rich and Honest Insights: Online posts reveal detailed reasons, emotions, and comparisons that people may not share in surveys.\nReach More People Easily: Itâ€™s hard to get high-net-worth individuals to answer surveys, but the spider can access public content instantly.\nBuild a Better Survey Later: By analyzing online themes first, I can design a more focused and relevant survey based on real concerns."
  },
  {
    "objectID": "Market.html#implementation-building-the-spider",
    "href": "Market.html#implementation-building-the-spider",
    "title": "Makert Survey",
    "section": "",
    "text": "We implemented a Python-based web crawler using the SerpAPI platform to scrape real-time search results. The following code initializes the necessary libraries and parameters used in the process.\n\n\nClick to view code\nimport requests\nimport json\nimport time\nimport pandas as pd\nfrom datetime import datetime\nimport os\nimport openpyxl\nimport re\nfrom collections import Counter\nimport jieba\n\n# ç”¨æˆ·é…ç½®åŒºåŸŸ\nSERPAPI_KEY = \"84c798a8d45d1e7cae0b18df778ac06bf2c6169f0249e40756aea0b9d6cd4749\"  \nRESULTS_PER_KEYWORD = 20\nDELAY_BETWEEN_REQUESTS = 1\nDELAY_BETWEEN_KEYWORDS = 2"
  },
  {
    "objectID": "Market.html#targeted-search-keywords",
    "href": "Market.html#targeted-search-keywords",
    "title": "Makert Survey",
    "section": "",
    "text": "The keywords used by the crawler are carefully chosen to reflect real user concerns about wealth migration. These terms guide the spider to relevant discussions, articles, and posts.\n\n\nClick to view code\nkeywords = [\n    \"ä¸­å›½é«˜å‡€å€¼äººå£« æ–°åŠ å¡\",\n    \"ä¸­å›½å¯Œäºº èµ„äº§é…ç½® æ–°åŠ å¡\", \n    \"ä¸­å›½å¯Œè±ª ä¸ºä»€ä¹ˆç§»æ°‘æ–°åŠ å¡\",\n    \"ä¸­å›½é«˜å‡€å€¼å®¢æˆ· ç¦»å²¸è´¦æˆ·\",\n    \"æ–°åŠ å¡ CRS é¿ç¨\",\n    \"ä¸­å›½ å®¶æ—ä¿¡æ‰˜ æ–°åŠ å¡\",\n    \"ä¸­å›½é«˜å‡€å€¼äººå£« å­å¥³æ•™è‚² æ–°åŠ å¡\",\n    \"ä¸­å›½ç§»æ°‘æ–°åŠ å¡ è´¢å¯Œç®¡ç†\"\n]\n\n\n\n\nClick to view code\ndef serpapi_search(query, api_key, num=20, start=0):\n    url = \"https://serpapi.com/search\"\n    params = {\n        'q': query,\n        'api_key': api_key,\n        'engine': 'google',\n        'num': min(num, 100),\n        'start': start,\n        'hl': 'zh-cn',\n        'gl': 'cn'\n    }\n    try:\n        response = requests.get(url, params=params)\n        response.raise_for_status()\n        return response.json()\n    except requests.exceptions.RequestException as e:\n        print(f\"æœç´¢è¯·æ±‚å¤±è´¥: {e}\")\n        return {}\n\ndef extract_search_results(results, keyword):\n    extracted_results = []\n    organic_results = results.get('organic_results', [])\n    for item in organic_results:\n        result_info = {\n            'keyword': keyword,\n            'title': item.get('title', ''),\n            'url': item.get('link', ''),\n            'snippet': item.get('snippet', ''),\n            'displayed_link': item.get('displayed_link', ''),\n            'position': item.get('position', 0),\n            'search_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        }\n        extracted_results.append(result_info)\n    return extracted_results\n\n\n\n\nClick to view code\ndef search_keyword_with_pagination(keyword, api_key, total_results=20):\n    all_results = []\n    results_per_page = 10\n    for start in range(0, total_results, results_per_page):\n        remaining = total_results - start\n        num_to_get = min(results_per_page, remaining)\n        print(f\"    è·å–ç¬¬ {start+1}-{start+num_to_get} æ¡ç»“æœ...\")\n        results = serpapi_search(keyword, api_key, num=num_to_get, start=start)\n        if not results or 'organic_results' not in results:\n            print(f\"æ²¡æœ‰æ›´å¤šç»“æœ\")\n            break\n        extracted = extract_search_results(results, keyword)\n        all_results.extend(extracted)\n        if len(extracted) &lt; num_to_get:\n            break\n        time.sleep(DELAY_BETWEEN_REQUESTS)\n    return all_results\n\n\n\n\nClick to view code\ndef main():\n    print(\"å¼€å§‹æœç´¢...\")\n    print(f\"æœç´¢å…³é”®è¯æ•°é‡: {len(keywords)}\")\n    print(f\"æ¯ä¸ªå…³é”®è¯è·å–ç»“æœæ•°: {RESULTS_PER_KEYWORD}\")\n    print(\"-\" * 50)\n    \n    all_search_results = []\n    for i, keyword in enumerate(keywords, 1):\n        print(f\"[{i}/{len(keywords)}] æœç´¢å…³é”®è¯: {keyword}\")\n        try:\n            results = search_keyword_with_pagination(keyword, SERPAPI_KEY, RESULTS_PER_KEYWORD)\n            all_search_results.extend(results)\n            print(f\"æ‰¾åˆ° {len(results)} ä¸ªç»“æœ\")\n            if i &lt; len(keywords):\n                print(f\"ç­‰å¾… {DELAY_BETWEEN_KEYWORDS} ç§’...\")\n                time.sleep(DELAY_BETWEEN_KEYWORDS)\n        except Exception as e:\n            print(f\"é”™è¯¯: {e}\")\n            continue\n    print(\"-\" * 50)\n    print(f\"æ€»å…±æ‰¾åˆ° {len(all_search_results)} ä¸ªç»“æœ\")\n    return all_search_results\n\nsearch_results = main()"
  },
  {
    "objectID": "Market.html#data-collection-process",
    "href": "Market.html#data-collection-process",
    "title": "Makert Survey",
    "section": "",
    "text": "The crawler iterates through each keyword, sends queries to SerpAPI, extracts content (including titles, URLs, and full texts), and saves them into a structured format for further processing. This automation ensures scalability and coverage.\n\n\nClick to view code\nif search_results:\n    df_full = pd.DataFrame(search_results)\n    df_unique = df_full.drop_duplicates(subset=['url'], keep='first')\n    print(f\"åŸå§‹ç»“æœ: {len(df_full)}ï¼Œå»é‡å: {len(df_unique)}\")\n\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    df_simple = df_unique[['keyword', 'title', 'url', 'search_timestamp']].copy()\n\n    # ä¿å­˜åˆ°æŒ‡å®š Dataset\n    save_dir = os.path.expanduser(\"/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset\")\n    os.makedirs(save_dir, exist_ok=True)\n\n    df_unique.to_csv(os.path.join(save_dir, f\"search_results_full_{timestamp}.csv\"), index=False, encoding='utf-8-sig')\n    df_simple.to_csv(os.path.join(save_dir, f\"search_results_simple_{timestamp}.csv\"), index=False, encoding='utf-8-sig')\n\n    with pd.ExcelWriter(os.path.join(save_dir, f\"search_results_{timestamp}.xlsx\"), engine='openpyxl') as writer:\n        df_unique.to_excel(writer, sheet_name='å®Œæ•´æ•°æ®', index=False)\n        df_simple.to_excel(writer, sheet_name='ç®€åŒ–æ•°æ®', index=False)\n\n    print(f\"æ–‡ä»¶ä¿å­˜è·¯å¾„: {save_dir}\")\n\n    print(\"\\næ•°æ®é¢„è§ˆ:\")\n    print(df_simple.head())\n\n    print(\"\\næ¯ä¸ªå…³é”®è¯çš„ç»“æœæ•°é‡:\")\n    print(df_unique['keyword'].value_counts())\n\n    globals()['search_data'] = df_unique\n    globals()['search_data_simple'] = df_simple\n\n    print(\"\\nå˜é‡å·²åŠ è½½: search_data, search_data_simple\")\nelse:\n    print(\"æ²¡æœ‰æœç´¢ç»“æœ\")\n    globals()['search_data'] = pd.DataFrame()\n    globals()['search_data_simple'] = pd.DataFrame()"
  },
  {
    "objectID": "Market.html#data-transformation",
    "href": "Market.html#data-transformation",
    "title": "Makert Survey",
    "section": "",
    "text": "Once raw JSON responses are collected, we convert them into a tabular format using pandas. This step prepares the dataset for text cleaning and analysis.\n\n\nClick to view code\nfrom bs4 import BeautifulSoup\nimport chardet\n\n# æ–‡ä»¶è·¯å¾„è®¾ç½®\ninput_file = \"/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset/search_results_20250706_192824.xlsx\"\noutput_file = \"/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset/search_results_with_content.xlsx\"\n\n# è¯»å–ç®€åŒ–æ•°æ®å·¥ä½œè¡¨\ndf = pd.read_excel(input_file, sheet_name=\"ç®€åŒ–æ•°æ®\")\n\n# æ­£æ–‡æå–å‡½æ•°\ndef fetch_article_content(url, timeout=10):\n    try:\n        response = requests.get(url, timeout=timeout, headers={\"User-Agent\": \"Mozilla/5.0\"})\n        detected = chardet.detect(response.content)\n        response.encoding = detected['encoding'] or 'utf-8'\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        paragraphs = soup.find_all('p')\n        content = '\\n'.join(p.get_text(strip=True) for p in paragraphs)\n        return content if len(content) &gt; 50 else None\n    except Exception:\n        return None\n\n# çˆ¬å–å†…å®¹\nprint(\"å¼€å§‹æŠ“å–æ­£æ–‡å†…å®¹...\")\ndf[\"content\"] = df[\"url\"].apply(fetch_article_content)\nprint(\"æ­£æ–‡æŠ“å–å®Œæˆï¼Œå¼€å§‹ä¿å­˜æ–‡ä»¶...\")\n\n# ä¿å­˜ä¸ºæ–°çš„ Excel æ–‡ä»¶\ndf.to_excel(output_file, index=False)\nprint(f\"æ–‡ä»¶å·²ä¿å­˜è‡³ï¼š{output_file}\")"
  },
  {
    "objectID": "Market.html#data-clean",
    "href": "Market.html#data-clean",
    "title": "Makert Survey",
    "section": "",
    "text": "Click to view code\n# è¾“å…¥è¾“å‡ºè·¯å¾„\ninput_file = \"/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset/search_results_with_content.xlsx\"\noutput_file = \"/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset/search_results_cleaned.xlsx\"\n\n# åŠ è½½ Excel æ–‡ä»¶\ndf = pd.read_excel(input_file)\n\n# å®šä¹‰æ— æ•ˆå†…å®¹å…³é”®è¯\nad_keywords = [\n    \"å…è´£å£°æ˜\", \"å¹¿å‘Šåˆä½œ\", \"è”ç³»ç®¡ç†å‘˜\", \"è¯·åœ¨å¾®ä¿¡ä¸­æ‰“å¼€\", \"æœ¬ç«™æ‰€æœ‰æ–‡ç« \",\n    \"æŠ±æ­‰\", \"é¡µé¢ä¸å­˜åœ¨\", \"å‡ºé”™\", \"404\", \"è¯·è¾“å…¥éªŒè¯ç \",\n    \"ç™»å½•æŸ¥çœ‹å…¨æ–‡\", \"Oops\", \"Something went wrong\", \"è®¿é—®å—é™\"\n]\n\n# åˆ¤æ–­æ˜¯å¦ä¸ºæ— æ•ˆæ­£æ–‡\ndef is_invalid(text):\n    if pd.isna(text):\n        return True\n    if len(text.strip()) &lt; 100:\n        return True\n    if any(kw in text for kw in ad_keywords):\n        return True\n    return False\n\n# æ·»åŠ æ ‡è®°åˆ—\ndf[\"invalid\"] = df[\"content\"].apply(is_invalid)\n\n# ä¿ç•™æœ‰æ•ˆæ­£æ–‡å†…å®¹\ndf_cleaned = df[~df[\"invalid\"]].drop(columns=[\"invalid\"]).copy()\n\n# ä¿å­˜æ¸…æ´—åçš„ç»“æœ\ndf_cleaned.to_excel(output_file, index=False)\nprint(f\"æ¸…æ´—å®Œæˆï¼Œå·²ä¿å­˜ä¸ºï¼š{output_file}\")\n\n\n\n\nClick to view code\n# è¯»å– Excel æ–‡ä»¶\ndf = pd.read_excel(\"search_results_cleaned.xlsx\")\n\n# 1. æŸ¥çœ‹è¡Œæ•°å’Œåˆ—æ•°\nprint(\"è¡Œæ•° Ã— åˆ—æ•°:\", df.shape)\n\n# 2. æŸ¥çœ‹åˆ—åå’Œç±»å‹\nprint(\"\\nåˆ—åä¸æ•°æ®ç±»å‹:\")\nprint(df.dtypes)\n\n# 3. å¿«é€Ÿæ¦‚è§ˆæ¯åˆ—çš„å‰å‡ è¡Œï¼ˆç»“æ„ + å€¼ï¼‰\nprint(\"\\næ ·æœ¬é¢„è§ˆ:\")\nprint(df.head())\n\n# 4. ç¼ºå¤±å€¼ç»Ÿè®¡ï¼ˆNA å€¼ï¼‰\nprint(\"\\nç¼ºå¤±å€¼ç»Ÿè®¡:\")\nprint(df.isna().sum())\n\n\nè¡Œæ•° Ã— åˆ—æ•°: (90, 6)\n\nåˆ—åä¸æ•°æ®ç±»å‹:\nkeyword             object\ntitle               object\nurl                 object\nsearch_timestamp    object\ncontent             object\ninvalid               bool\ndtype: object\n\næ ·æœ¬é¢„è§ˆ:\n       keyword                           title  \\\n0  ä¸­å›½é«˜å‡€å€¼äººå£« æ–°åŠ å¡          æ–°åŠ å¡é‡‘èæœºåˆ¶ç¨³å®šå¸å¼•æœ€å¤šé«˜å‡€å€¼äººå£«è€ƒè™‘ç§»å±…   \n1  ä¸­å›½é«˜å‡€å€¼äººå£« æ–°åŠ å¡          æˆ‘å›½è¶…é«˜å‡€å€¼äººç¾¤è®¾ç«‹å®¶æ—åŠå…¬å®¤ç°çŠ¶åˆ†æä¸åº”å¯¹   \n2  ä¸­å›½é«˜å‡€å€¼äººå£« æ–°åŠ å¡  ä¸­å›½é«˜å‡€å€¼äººå£«åœ¨æ–°åŠ å¡è®¾ç«‹å®¶æ—åŠå…¬å®¤å’ŒæŠ•èµ„åˆåˆ›ä¼ä¸šçš„ ...   \n3  ä¸­å›½é«˜å‡€å€¼äººå£« æ–°åŠ å¡        é«˜å‡€å€¼äººå£«ä¸ºä½•çº·çº·é€‰æ‹©æ–°åŠ å¡ï¼Ÿ_ç§»æ°‘_æ•™è‚²_å·¥ä½œ   \n4  ä¸­å›½é«˜å‡€å€¼äººå£« æ–°åŠ å¡    èšç„¦å®¶åŠ| äºšæ´²è¶…é«˜å‡€å€¼äººç¾¤å¢é•¿æˆ–å°†æ¨åŠ¨å®¶æ—åŠå…¬å®¤çš„å‘å±•   \n\n                                                 url     search_timestamp  \\\n0  https://www.zaobao.com/finance/singapore/story...  2025-07-06 19:19:49   \n1  http://cel.cn/List/FullText?articleId=d37c148c...  2025-07-06 19:19:49   \n2  https://fargowealth.com/en/home/cfsj/cfsj_deta...  2025-07-06 19:19:49   \n3         https://www.sohu.com/a/843292362_121963266  2025-07-06 19:19:49   \n4  https://www.bloombergchina.com/blog/asias-ultr...  2025-07-06 19:19:49   \n\n                                             content  invalid  \n0  \\n\\næ–°åŠ å¡ç¨³å®šçš„é‡‘èæœºåˆ¶å’ŒåŒºåŸŸè”é€šæ€§å¯¹å…¨çƒå¯Œè±ªå…·æœ‰å¼ºå¤§å¸å¼•åŠ›ï¼Œæ˜¯æœ€å¤šå¯Œè±ªé¦–é€‰çš„ç§»å±…ç›®çš„åœ°...    False  \n1  è‚–äº¬2024-07-08æµè§ˆé‡ï¼š1498\\næˆ‘å›½è¶…é«˜å‡€å€¼äººç¾¤è®¾ç«‹å®¶æ—åŠå…¬å®¤çš„éœ€æ±‚æ˜¯å®¢è§‚å­˜åœ¨çš„...    False  \n2  ä¸­å›½é«˜å‡€å€¼äººå£«åœ¨æ–°åŠ å¡è®¾ç«‹å®¶æ—åŠå…¬å®¤å’ŒæŠ•èµ„åˆåˆ›ä¼ä¸šçš„æœ€æ–°è¶‹åŠ¿\\näº”å¹´å‰ï¼Œæ¢ä¿¡å†›å› å¥åº·åŸå› ç¦»å¼€...    False  \n3  è¿‘å¹´æ¥ï¼Œè¶Šæ¥è¶Šå¤šçš„å›½å†…ä¼ä¸šå®¶å’Œé«˜å‡€å€¼äººå£«å°†ç›®å…‰æŠ•å‘æ–°åŠ å¡ï¼Œå°¤å…¶æ˜¯é‚£äº›æ­£åœ¨è€ƒè™‘ç§»æ°‘æˆ–å­å¥³æ•™è‚²çš„...    False  \n4  æœ¬æ–‡ç”±å½­åšè¡Œä¸šç ”ç©¶é«˜çº§åˆ†æå¸ˆé»„é¢–çŠï¼ˆSharnie Wongï¼‰æ’°å†™ï¼Œé¦–å‘äºå½­åšç»ˆç«¯ã€‚\\nç‘é“¶...    False  \n\nç¼ºå¤±å€¼ç»Ÿè®¡:\nkeyword             0\ntitle               0\nurl                 0\nsearch_timestamp    0\ncontent             0\ninvalid             0\ndtype: int64\n\n\n\n\nClick to view code\ndf_cleaned = df[~df[\"invalid\"] & df[\"content\"].notna()].copy()\ndf_cleaned.to_excel(\"search_results_cleaned.xlsx\", index=False)"
  },
  {
    "objectID": "Market.html#text-cleaning",
    "href": "Market.html#text-cleaning",
    "title": "Makert Survey",
    "section": "",
    "text": "The raw text contains punctuation, redundant whitespace, and occasionally malformed encoding. We apply basic cleaning to ensure the corpus is suitable for keyword extraction and topic modeling.\n\n\nClick to view code\n# 1. åŠ è½½æ¸…æ´—åçš„æ–‡ç« æ•°æ®\ndf = pd.read_excel(\"Dataset/search_results_cleaned.xlsx\")\ndf = df[df[\"content\"].notna() & (df[\"content\"].str.strip() != \"\")]\n\n# 2. è¯»å–åœç”¨è¯è¡¨\nwith open(\"Dataset/cn_stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n    stopwords = set([line.strip() for line in f])\n\n# 3. æ¸…æ´—å‡½æ•°\ndef clean_text(s):\n    s = re.sub(r'&lt;.*?&gt;', '', s)  # HTMLæ ‡ç­¾\n    s = re.sub(r'[a-zA-Z]+', '', s)  # è‹±æ–‡\n    s = re.sub(r'[\\d\\-:/\\.å¹´æœˆæ—¥\\s]+', '', s)  # æ•°å­—ä¸æ—¥æœŸ\n    s = re.sub(r'[\\u0000-\\u007F]+', '', s)  # ASCIIç¬¦å·\n    return s.strip()\n\n# 4. åˆ†å¥ + æ¸…æ´— + åˆ†è¯ + å»åœç”¨è¯\nwords = []\nfor content in df[\"content\"]:\n    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', content)\n    for s in sentences:\n        s_clean = clean_text(s)\n        if len(s_clean) &gt;= 5:\n            segs = jieba.cut(s_clean)\n            words += [w for w in segs if len(w) &gt; 1 and w not in stopwords and re.match(r'[\\u4e00-\\u9fff]+', w)]\n\n# 5. è¯é¢‘ç»Ÿè®¡\nword_freq = Counter(words)\ndf_freq = pd.DataFrame(word_freq.most_common(100), columns=[\"Word\", \"Frequency\"])\n\n\n\n\nClick to view code\n# 6. ä¿å­˜åˆ°æ–‡ä»¶\ndf_freq.to_excel(\"Dataset/word_frequency_cleaned_with_stopwords.xlsx\", index=False)\n\n\n\n\nClick to view code\n# 7. æ‰“å°ç»“æœ\nprint(df_freq)\n\n\n   Word  Frequency\n0   æ–°åŠ å¡       1895\n1    ä¿¡æ‰˜       1301\n2    å®¶æ—       1028\n3    èµ„äº§        947\n4    æŠ•èµ„        922\n..  ...        ...\n95   æ–¹å¼        133\n96   ç›®å‰        132\n97   é¡¾é—®        132\n98   æ•°é‡        128\n99   äº§å“        126\n\n[100 rows x 2 columns]"
  },
  {
    "objectID": "Market.html#keyword-frequency",
    "href": "Market.html#keyword-frequency",
    "title": "Makert Survey",
    "section": "",
    "text": "We extract high-frequency terms to detect common concerns and motivations. This analysis surfaces dominant themes such as â€œwealth managementâ€, â€œimmigrationâ€, and â€œSingapore advantageâ€."
  },
  {
    "objectID": "Market.html#word-cloud",
    "href": "Market.html#word-cloud",
    "title": "Makert Survey",
    "section": "2.1 Word Cloud",
    "text": "2.1 Word Cloud"
  },
  {
    "objectID": "Market.html#visualizing-high-frequency-terms",
    "href": "Market.html#visualizing-high-frequency-terms",
    "title": "Makert Survey",
    "section": "Visualizing High-Frequency Terms",
    "text": "Visualizing High-Frequency Terms\nThe following word cloud ord highlights the most common terms across all retrieved content, offering a visual snapshot of what matters most to the audience.\n\n\nClick to view code\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# 1. åŠ è½½è¯é¢‘æ•°æ®\ndf_freq = pd.read_excel(\"Dataset/word_frequency_cleaned_with_stopwords.xlsx\")\n\n# 2. è½¬æ¢ä¸ºå­—å…¸æ ¼å¼\nfreq_dict = dict(zip(df_freq[\"Word\"], df_freq[\"Frequency\"]))\n\n# 3. åˆ›å»ºè¯äº‘å¯¹è±¡\nwc = WordCloud(\n    font_path=\"/System/Library/Fonts/STHeiti Medium.ttc\",  # æ›¿æ¢ä¸ºä½ æœ¬åœ°æ”¯æŒä¸­æ–‡çš„å­—ä½“è·¯å¾„\n    background_color=\"white\",\n    width=1000,\n    height=700,\n    max_words=200\n).generate_from_frequencies(freq_dict)\n\n# 4. å¯è§†åŒ–è¯äº‘\nplt.figure(figsize=(12, 8))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"WordCloud\", fontsize=18)\nplt.subplots_adjust(top=0.85) \nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights from the Word Cloud\n\n\n\nThe word cloud visually reinforces the central motivations driving Chinese high-net-worth individuals (HNWIs) to move their assets to Singapore.\n\nHigh-frequency terms such as â€œè´¢å¯Œç®¡ç†â€ (wealth management), â€œèµ„äº§é…ç½®â€ (asset allocation), and â€œç¨åŠ¡ä¼˜åŒ–â€ (tax optimization) directly align with the motivations outlined in our initial flowchart.\nKeywords like â€œå®¶æ—ä¿¡æ‰˜â€ (family trust), â€œç¦»å²¸è´¦æˆ·â€ (offshore accounts), and â€œå­å¥³æ•™è‚²â€ (childrenâ€™s education) indicate a strong emphasis on legacy planning, asset protection, and long-term family welfare.\n\nConclusion: These themes confirm that Singapore is not just attractive for its financial incentivesâ€”but as a comprehensive hub for multi-generational wealth security and elite lifestyle planning."
  },
  {
    "objectID": "Market.html#sentiment-analysis",
    "href": "Market.html#sentiment-analysis",
    "title": "Makert Survey",
    "section": "2.2 Sentiment Analysis",
    "text": "2.2 Sentiment Analysis\n\n\nClick to view code\nfrom snownlp import SnowNLP\n\n# è¯»å–å·²æ¸…æ´—çš„æ•°æ®\ndf = pd.read_excel(\"Dataset/search_results_cleaned.xlsx\")\n\n# è¿‡æ»¤æ‰æ— æ­£æ–‡\ndf = df[df[\"content\"].notna() & (df[\"content\"].str.len() &gt; 30)].copy()\n\n# æƒ…æ„Ÿåˆ†æå‡½æ•°ï¼ˆè¿”å›å€¼åœ¨ 0ï½1 ä¹‹é—´ï¼Œ1 è¶Šç§¯æï¼‰\ndef get_sentiment(text):\n    try:\n        return SnowNLP(text).sentiments\n    except:\n        return None\n\n# æ·»åŠ æƒ…æ„Ÿè¯„åˆ†åˆ—\ndf[\"sentiment_score\"] = df[\"content\"].apply(get_sentiment)\n\n# åˆ†ç±»æ ‡ç­¾ï¼šå¤§äº 0.6 ä¸ºæ­£é¢ï¼Œå°äº 0.4 ä¸ºè´Ÿé¢ï¼Œå…¶ä½™ä¸ºä¸­æ€§\ndef classify(score):\n    if score is None:\n        return \"Unknown\"\n    elif score &gt; 0.6:\n        return \"Positive\"\n    elif score &lt; 0.4:\n        return \"Negative\"\n    else:\n        return \"Neutral\"\n\ndf[\"sentiment_label\"] = df[\"sentiment_score\"].apply(classify)\n\n# ä¿å­˜ç»“æœ\ndf.to_excel(\"Dataset/search_results_sentiment.xlsx\", index=False)\n\n# æŸ¥çœ‹ç»Ÿè®¡\nprint(df[\"sentiment_label\"].value_counts())\n\n\nsentiment_label\nPositive    85\nNegative     5\nName: count, dtype: int64\n\n\n\n\nClick to view code\nimport matplotlib.pyplot as plt\n\n# Count sentiment\nsentiment_counts = df[\"sentiment_label\"].value_counts()\n\n# Set labels and colors\nlabels = sentiment_counts.index.tolist()\ncolors = ['#4CAF50' if label == 'Positive' else '#F44336' for label in labels]  \n\n# Define explode to slightly offset each slice\nexplode = [0.05] * len(labels)\n\n# Create figure\nfig, ax = plt.subplots(figsize=(7, 7), facecolor='white')\nwedges, texts, autotexts = ax.pie(\n    sentiment_counts,\n    labels=labels,\n    autopct='%1.1f%%',\n    startangle=140,\n    colors=colors,\n    explode=explode,\n    wedgeprops={'edgecolor': 'white', 'linewidth': 2},\n    textprops={'fontsize': 13}\n)\n\n# Labels and percentages\nfor text in autotexts:\n    text.set_color('white')\n    text.set_fontweight('bold')\n\n# Add title with padding\nplt.title(\"Sentiment Composition\", fontsize=18, weight='bold', pad=20)\n\n# Equal aspect ratio ensures pie is circular\nax.axis('equal')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights from Sentiment Analysis\n\n\n\nThe sentiment analysis reveals a mixed but insightful emotional tone surrounding discussions of wealth migration to Singapore.\n\nA significant portion of posts are positive, reflecting appreciation for Singaporeâ€™s legal stability, tax efficiency, and quality of life.\nNegative sentiments mainly revolve around concerns about regulatory changes, barriers to entry, or uncertainty around immigration pathways.\nNeutral discussions tend to be informational or comparative, providing objective assessments of different offshore options.\n\nConclusion: These findings suggest that while Singapore is generally viewed favorably, there remains a need to address misconceptions and reduce friction in communication when engaging with HNW prospects.\n\n\n\n\nClick to view code\nimport matplotlib.pyplot as plt\n\nplt.rcParams['font.family'] = 'Arial Unicode MS'  \nplt.rcParams['axes.unicode_minus'] = False\n\n# åˆ†ç»„ï¼šç»Ÿè®¡æ¯ä¸ª keyword ä¸‹çš„æ­£è´Ÿé¢æ•°é‡\nsentiment_by_keyword = df.groupby(['keyword', 'sentiment_label']).size().unstack(fill_value=0)\n\n# æ‰“å°æ£€æŸ¥\nprint(sentiment_by_keyword)\n\n# ç»˜å›¾ï¼šå †å æŸ±çŠ¶å›¾\nsentiment_by_keyword.plot(kind='bar', stacked=True, figsize=(12, 6), color=[\"#1f77b4\", \"#ff7f0e\"])  \n\n# è®¾ç½®æ ‡é¢˜å’Œè½´æ ‡ç­¾\nplt.title(\"å…³é”®è¯æƒ…æ„Ÿæ„æˆ\", fontsize=16)\nplt.xlabel(\"å…³é”®è¯\", fontsize=12)\nplt.ylabel(\"æ–‡ç« æ•°é‡\", fontsize=12)\n\n# æ—‹è½¬ x è½´æ–‡å­—é˜²æ­¢é‡å \nplt.xticks(rotation=45, ha='right')\n\n# è‡ªåŠ¨å¸ƒå±€\nplt.tight_layout()\n\n# æ˜¾ç¤ºå›¾åƒ\nplt.show()\n\n\nsentiment_label   Negative  Positive\nkeyword                             \nä¸­å›½ å®¶æ—ä¿¡æ‰˜ æ–°åŠ å¡              0        14\nä¸­å›½å¯Œäºº èµ„äº§é…ç½® æ–°åŠ å¡            1         8\nä¸­å›½å¯Œè±ª ä¸ºä»€ä¹ˆç§»æ°‘æ–°åŠ å¡            1         5\nä¸­å›½ç§»æ°‘æ–°åŠ å¡ è´¢å¯Œç®¡ç†             0        11\nä¸­å›½é«˜å‡€å€¼äººå£« å­å¥³æ•™è‚² æ–°åŠ å¡         0         9\nä¸­å›½é«˜å‡€å€¼äººå£« æ–°åŠ å¡              2        12\nä¸­å›½é«˜å‡€å€¼å®¢æˆ· ç¦»å²¸è´¦æˆ·             0        15\næ–°åŠ å¡ CRS é¿ç¨               1        11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext Step: Deep Dive into Negative Sentiments\n\n\n\nTo strengthen client engagement and tailor advisory strategies, further analysis should focus on uncovering the root causes behind negative or hesitant views. Recommended steps include:\n\nThematic Clustering: Group negative posts into key concernsâ€”e.g., â€œregulatory fearsâ€, â€œimmigration complexityâ€, â€œtrust issuesâ€.\nTemporal Sentiment Tracking: Detect if negative sentiment spikes after specific events (e.g., new financial regulations in China).\n\nBy understanding the friction points in sentiment, Life Inc can refine positioning, improve client confidence, and remove silent blockers in the decision-making journey."
  },
  {
    "objectID": "Market.html#network-analysis",
    "href": "Market.html#network-analysis",
    "title": "Makert Survey",
    "section": "2.3 Network Analysis",
    "text": "2.3 Network Analysis\n\n\nClick to view code\nfrom itertools import combinations\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# ç”¨äºå­˜å‚¨æ¯å¥è¯çš„å…³é”®è¯\nkeyword_sentences = []\n\nfor content in df[\"content\"]:\n    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', content)\n    for s in sentences:\n        s_clean = clean_text(s)\n        if len(s_clean) &gt;= 5:\n            segs = jieba.cut(s_clean)\n            word_list = [w for w in segs if len(w) &gt; 1 and w not in stopwords and re.match(r'[\\u4e00-\\u9fff]+', w)]\n            keyword_sentences.append(word_list)\n\n\n\n\nClick to view code\ntop_100_words = set(df_freq[\"Word\"])\nco_occurrence = Counter()\n\n# åªç»Ÿè®¡ top100 è¯ä¹‹é—´çš„å…±ç°\nfor word_list in keyword_sentences:\n    words_in_top100 = [w for w in word_list if w in top_100_words]\n    for pair in combinations(set(words_in_top100), 2):  # set å»é‡\n        co_occurrence[tuple(sorted(pair))] += 1\n\n\n\n\nClick to view code\nimport matplotlib.font_manager as fm\nfor font in fm.findSystemFonts(fontpaths=None, fontext='ttf'):\n    if 'PingFang' in font or 'Arial' in font or 'Hei' in font:\n        print(font)\n\n\n/System/Library/Fonts/Supplemental/Arial Rounded Bold.ttf\n/System/Library/Fonts/Supplemental/Arial Narrow.ttf\n/System/Library/Fonts/ArialHB.ttc\n/System/Library/Fonts/Supplemental/Arial.ttf\n/System/Library/Fonts/Supplemental/Arial Unicode.ttf\n/System/Library/Fonts/Supplemental/Arial Italic.ttf\n/System/Library/Fonts/Supplemental/Arial Bold Italic.ttf\n/System/Library/Fonts/Supplemental/Arial Narrow Bold.ttf\n/Library/Fonts/Arial Unicode.ttf\n/System/Library/Fonts/Supplemental/Arial Bold.ttf\n/System/Library/Fonts/STHeiti Medium.ttc\n/System/Library/Fonts/PingFang.ttc\n/System/Library/Fonts/STHeiti Light.ttc\n/System/Library/Fonts/Supplemental/Arial Black.ttf\n/System/Library/Fonts/Supplemental/Arial Narrow Bold Italic.ttf\n/System/Library/Fonts/Supplemental/Arial Narrow Italic.ttf\n\n\n\n\nClick to view code\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\nimport networkx as nx\n\n# æ‰‹åŠ¨è®¾ç½®ä¸­æ–‡å­—ä½“\nmy_font = fm.FontProperties(fname=\"/System/Library/Fonts/STHeiti Medium.ttc\")\nplt.rcParams['font.family'] = my_font.get_name()\nplt.rcParams['axes.unicode_minus'] = False\n\n# æ„å»ºå›¾\nG = nx.Graph()\nfor (w1, w2), freq in co_occurrence.items():\n    if freq &gt;= 3:  # è®¾å®šå…±ç°é˜ˆå€¼\n        G.add_edge(w1, w2, weight=freq)\n\n# å¯è§†åŒ–ç»˜å›¾\nplt.figure(figsize=(12, 10))\npos = nx.spring_layout(G, k=0.5, seed=42)  # èŠ‚ç‚¹å¸ƒå±€\nnx.draw_networkx_nodes(G, pos, node_size=500, node_color='lightblue')\nnx.draw_networkx_edges(G, pos, width=[d['weight'] * 0.3 for _, _, d in G.edges(data=True)], alpha=0.6)\nnx.draw_networkx_labels(G, pos, font_size=10, font_family=my_font.get_name())\n\nplt.title(\"Top 100 ä¸­æ–‡å…³é”®è¯å…±ç°ç½‘ç»œ\", fontproperties=my_font)\nplt.axis(\"off\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights from Co-occurrence Keyword Network\n\n\n\nThis network reveals how important themes surrounding wealth migration to Singapore co-appear and reinforce each other in authentic online discussions.\n\nWords like â€œèµ„äº§é…ç½®â€ (asset allocation), â€œæ–°åŠ å¡â€, and â€œå®¶æ—ä¿¡æ‰˜â€ frequently co-occur, forming dense hubs of strategic planning.\nThe structure suggests that users donâ€™t speak of single motivations in isolation â€” tax planning, family trust, and regulatory considerations are often bundled in the same conversation.\nHigh-degree nodes like â€œCRSâ€, â€œç§»æ°‘â€, and â€œç¦»å²¸è´¦æˆ·â€ serve as bridges across different topic communities.\n\nConclusion: These patterns demonstrate that Chinese HNWIs approach offshore wealth management as a multifaceted decision process. Singapore is appealing not for a single advantage, but for its ability to address multiple interconnected concerns simultaneously.\n\n\n\n\nClick to view code\n# è®¡ç®—èŠ‚ç‚¹çš„åº¦ä¸­å¿ƒæ€§\ndegree_centrality = nx.degree_centrality(G)\n\n# æŒ‰ä¸­å¿ƒæ€§æ’åºï¼Œå–å‰10ä¸ªå…³é”®è¯\ntop_nodes = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n\n# æ‰“å°ç»“æœ\nprint(\"Top 10 å…³é”®è¯:\")\nfor word, score in top_nodes:\n    print(f\"{word}: {score:.3f}\")\n\n\nTop 10 å…³é”®è¯:\næ–°åŠ å¡: 1.000\nèµ„äº§: 1.000\næŠ•èµ„: 1.000\nå…¬å¸: 1.000\nç®¡ç†: 0.980\nå‡€å€¼: 0.970\nä¸­å›½: 0.970\né¦™æ¸¯: 0.970\nè¿›è¡Œ: 0.970\nè´¢å¯Œ: 0.960\n\n\n\n\nClick to view code\nimport community as community_louvain  # pip install python-louvain\nimport matplotlib.cm as cm\n\n# ç¤¾åŒºåˆ’åˆ†\npartition = community_louvain.best_partition(G)\n\n# è®¾ç½®é¢œè‰²æ˜ å°„\nsize = float(len(set(partition.values())))\npos = nx.spring_layout(G, k=0.5, seed=42)\ncolors = [cm.tab20(i / size) for i in partition.values()]\n\n# ç»˜å›¾\nplt.figure(figsize=(14, 12))\nnx.draw_networkx_nodes(G, pos, node_size=500, node_color=colors, alpha=0.8)\nnx.draw_networkx_edges(G, pos, width=0.5, alpha=0.3)\nnx.draw_networkx_labels(G, pos, font_size=10, font_family='Arial Unicode MS')\nplt.title(\"å…³é”®è¯å…±ç°ç½‘ç»œä¸­çš„è¯é¢˜ç¤¾åŒº\")\nplt.axis(\"off\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights from Color-Labeled Keyword Communities\n\n\n\nThe co-occurrence network, enhanced with semantic color tagging, reveals four distinct communities that structure online discussions around wealth migration to Singapore:\n\nğŸŒ• Community 1 (Brown) focuses on trust and legal structures, with keywords like â€œtrustâ€, â€œassetsâ€, â€œfundâ€, and â€œlegalâ€ â€” underscoring the importance of compliant, long-term wealth management.\nğŸŸ¢ Community 2 (Green) centers around client advisory and immigration services, showing that service quality and immigration logistics are major touchpoints for Chinese HNWIs.\nğŸ”µ Community 3 (Blue) is tied to cross-border financial assets and tax implications, with mentions of â€œHong Kongâ€, â€œtaxâ€, and â€œnet worthâ€ â€” pointing to regional diversification strategies.\nâš« Community 4 (Gray) contains more generic or connective language, serving as linguistic bridges in user conversations.\n\nConclusion: This structured clustering confirms that Chinese HNWIs are not driven by a single reason to move assets offshore. Instead, their motivations form a multi-layered ecosystem â€” blending financial safety, global mobility, tax planning, and trusted advisory. Singapore is uniquely positioned as a hub that satisfies all these needs simultaneously."
  },
  {
    "objectID": "Market.html#final-takeaways-why-do-chinese-hnwis-move-their-wealth-to-singapore",
    "href": "Market.html#final-takeaways-why-do-chinese-hnwis-move-their-wealth-to-singapore",
    "title": "Makert Survey",
    "section": "Final Takeaways: Why Do Chinese HNWIs Move Their Wealth to Singapore?",
    "text": "Final Takeaways: Why Do Chinese HNWIs Move Their Wealth to Singapore?\nThrough the combined use of structured flowchart thinking and real-world data collection using a customized web crawler, this study provides a grounded answer to our core question.\n\nKey Findings:\n\nWealth Management & Asset Protection: Terms like â€œè´¢å¯Œç®¡ç†â€ (wealth management) and â€œèµ„äº§é…ç½®â€ (asset allocation) appeared frequently, highlighting the strong demand for stable and diversified wealth strategies.\nTax Optimization: Frequent mentions of â€œç¨åŠ¡ä¼˜åŒ–â€ and â€œCRSé¿ç¨â€ reflect concerns over rising tax scrutiny in China and Singaporeâ€™s more favorable policies.\nEducational Planning & Family Legacy: Phrases such as â€œå­å¥³æ•™è‚²â€ and â€œå®¶æ—ä¿¡æ‰˜â€ indicate that many HNWIs are motivated by long-term family goals rather than short-term returns.\nPolitical and Legal Stability: Although more subtle, the preference for Singaporeâ€™s legal infrastructure and business environment emerged from context-rich discussions.\n\n\n\nWhy This Method Worked:\nBy using a spider to extract public sentiment directly from platforms like Zhihu and Xueqiu, we bypassed the sampling bias of traditional surveys and captured more nuanced and emotionally honest reasons behind wealth relocation behavior."
  },
  {
    "objectID": "Market.html#strategic-implications",
    "href": "Market.html#strategic-implications",
    "title": "Makert Survey",
    "section": "Strategic Implications",
    "text": "Strategic Implications\n\nFor Wealth Advisors\nAlign advisory messaging with clientsâ€™ emotional prioritiesâ€”especially trust, family security, and intergenerational control. Use insights from sentiment clustering to reframe conversations from â€œproductsâ€ to â€œprotecting legacy.â€\n\nExample:\nReplace â€œLetâ€™s set up a trust to reduce taxâ€ with\nâ€œLetâ€™s future-proof your familyâ€™s assets so your son can inherit them securelyâ€”no matter what happens in either country.â€\n\n\n\n\nFor Life Inc\nBuild theme-based planning kits (e.g.Â Trust + Tax + Education) mapped to the four identified keyword communities. Invest in bilingual onboarding, WeChat-style content formats, and cross-border collaboration frameworks to serve Chinese HNWIs holistically.\n\nExample:\nDevelop a digital â€œSingapore Welcome Packâ€ with Mandarin guides on:\n- How to legally move funds\n- Trust structure overview\n- PR application tips\nDelivered via a mini-site or WeChat-compatible PDF deck.\n\n\n\n\nFor Company Strategy\nEstablish modular advisory paths for three core personasâ€”Forward Planner, Risk Avoider, and Legacy Seeker. Each path should include tax-legal coordination, lifestyle concierge, and asset structuring guidance anchored in Singapore.\n\nExample:\nCreate a â€œLegacy Seeker Pathwayâ€:\n- Intro session with bilingual legacy advisor\n- Local will & trust setup\n- Heir education roadmap (e.g.Â next-gen wealth workshops)\n\n\n\n\nFor Survey & Research\nRefine survey instruments to probe latent motivations (e.g.Â fear of instability, desire for international lifestyle) and validate which service combinations resonate most. Use network clusters to shape both question content and option phrasing.\n\nExample:\nInclude a ranking question:\nâ€œWhich matters more to you in choosing a jurisdiction for your familyâ€™s assets?â€\n(Options: Political stability / Education system / Inheritance clarity / Low tax burden)\nâ†’ Each option maps to a keyword community in your analysis.\n\n\nIn conclusion, the answer is not singularâ€”but multi-dimensional. Singapore is attractive to Chinese HNWIs not only for its tax and legal advantages, but because it offers stability, safety, and long-term opportunities for families and wealth."
  }
]