[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RFM Project",
    "section": "",
    "text": "Welcome to RFM Project homepage. In this website, you will find my finding prepared for this project."
  },
  {
    "objectID": "Market.html",
    "href": "Market.html",
    "title": "Makert Survey",
    "section": "",
    "text": "To effectively organize and communicate the complex thought process behind why Chinese HNW individuals move their money to Singapore, I created Flowchart. This flowchart helps to:\n\nVisually map the key motivations (such as wealth management, tax benefits, lifestyle advantages, and legacy planning) that drive this financial movement.\nClarify the relationships between client needs, Singapore’s value proposition, and the strategic advisory angles we can take.\nSupport client-facing engagement by simplifying the narrative for Life Inc advisors\nGuide my survey design and presentation content, ensuring all relevant themes—such as trends, preferences, and asset relocation methods—are covered logically and comprehensively.\n\nIt acts as both a research framework and a client communication tool to align our understanding and offerings with the target market’s aspirations.\n\n\n\n\n\n\n\n\nTo support the research in Flowchart, I developed a custom web crawler to systematically collect qualitative and contextual data from Chinese-language sources such as forums (e.g., Zhihu, Xueqiu), financial news sites (e.g., Eastern Wealth), and financial blogs. These platforms offer authentic discussions and opinions from Chinese citizens regarding overseas wealth movement—especially to Singapore.\nMy spider focused on keywords that map directly to the key branches of the flowchart, such as:\n\n“新加坡 财富管理” (Singapore wealth management)\n“中国人移民新加坡” (Chinese migration to Singapore)\n“税务优化 新加坡” (Tax optimization Singapore)\n“资产配置 海外” (Offshore asset allocation)\n\n\n\n\n\n\n\nWhy choose spider instead of survey\n\n\n\n\nUnderstand Real Conversations First: The spider captures what people are already saying online, so I don’t assume or guess their opinions.\nGather Rich and Honest Insights: Online posts reveal detailed reasons, emotions, and comparisons that people may not share in surveys.\nReach More People Easily: It’s hard to get high-net-worth individuals to answer surveys, but the spider can access public content instantly.\nBuild a Better Survey Later: By analyzing online themes first, I can design a more focused and relevant survey based on real concerns.\n\n\n\n\n\n\nWe implemented a Python-based web crawler using the SerpAPI platform to scrape real-time search results. The following code initializes the necessary libraries and parameters used in the process.\n\n\nClick to view code\nimport requests\nimport json\nimport time\nimport pandas as pd\nfrom datetime import datetime\nimport os\nimport openpyxl\nimport re\nfrom collections import Counter\nimport jieba\n\n# 用户配置区域\nSERPAPI_KEY = \"84c798a8d45d1e7cae0b18df778ac06bf2c6169f0249e40756aea0b9d6cd4749\"  \nRESULTS_PER_KEYWORD = 20\nDELAY_BETWEEN_REQUESTS = 1\nDELAY_BETWEEN_KEYWORDS = 2\n\n\n\n\n\nThe keywords used by the crawler are carefully chosen to reflect real user concerns about wealth migration. These terms guide the spider to relevant discussions, articles, and posts.\n\n\nClick to view code\nkeywords = [\n    \"中国高净值人士 新加坡\",\n    \"中国富人 资产配置 新加坡\", \n    \"中国富豪 为什么移民新加坡\",\n    \"中国高净值客户 离岸账户\",\n    \"新加坡 CRS 避税\",\n    \"中国 家族信托 新加坡\",\n    \"中国高净值人士 子女教育 新加坡\",\n    \"中国移民新加坡 财富管理\"\n]\n\n\n\n\nClick to view code\ndef serpapi_search(query, api_key, num=20, start=0):\n    url = \"https://serpapi.com/search\"\n    params = {\n        'q': query,\n        'api_key': api_key,\n        'engine': 'google',\n        'num': min(num, 100),\n        'start': start,\n        'hl': 'zh-cn',\n        'gl': 'cn'\n    }\n    try:\n        response = requests.get(url, params=params)\n        response.raise_for_status()\n        return response.json()\n    except requests.exceptions.RequestException as e:\n        print(f\"搜索请求失败: {e}\")\n        return {}\n\ndef extract_search_results(results, keyword):\n    extracted_results = []\n    organic_results = results.get('organic_results', [])\n    for item in organic_results:\n        result_info = {\n            'keyword': keyword,\n            'title': item.get('title', ''),\n            'url': item.get('link', ''),\n            'snippet': item.get('snippet', ''),\n            'displayed_link': item.get('displayed_link', ''),\n            'position': item.get('position', 0),\n            'search_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        }\n        extracted_results.append(result_info)\n    return extracted_results\n\n\n\n\nClick to view code\ndef search_keyword_with_pagination(keyword, api_key, total_results=20):\n    all_results = []\n    results_per_page = 10\n    for start in range(0, total_results, results_per_page):\n        remaining = total_results - start\n        num_to_get = min(results_per_page, remaining)\n        print(f\"    获取第 {start+1}-{start+num_to_get} 条结果...\")\n        results = serpapi_search(keyword, api_key, num=num_to_get, start=start)\n        if not results or 'organic_results' not in results:\n            print(f\"没有更多结果\")\n            break\n        extracted = extract_search_results(results, keyword)\n        all_results.extend(extracted)\n        if len(extracted) &lt; num_to_get:\n            break\n        time.sleep(DELAY_BETWEEN_REQUESTS)\n    return all_results\n\n\n\n\nClick to view code\ndef main():\n    print(\"开始搜索...\")\n    print(f\"搜索关键词数量: {len(keywords)}\")\n    print(f\"每个关键词获取结果数: {RESULTS_PER_KEYWORD}\")\n    print(\"-\" * 50)\n    \n    all_search_results = []\n    for i, keyword in enumerate(keywords, 1):\n        print(f\"[{i}/{len(keywords)}] 搜索关键词: {keyword}\")\n        try:\n            results = search_keyword_with_pagination(keyword, SERPAPI_KEY, RESULTS_PER_KEYWORD)\n            all_search_results.extend(results)\n            print(f\"找到 {len(results)} 个结果\")\n            if i &lt; len(keywords):\n                print(f\"等待 {DELAY_BETWEEN_KEYWORDS} 秒...\")\n                time.sleep(DELAY_BETWEEN_KEYWORDS)\n        except Exception as e:\n            print(f\"错误: {e}\")\n            continue\n    print(\"-\" * 50)\n    print(f\"总共找到 {len(all_search_results)} 个结果\")\n    return all_search_results\n\nsearch_results = main()\n\n\n\n\n\nThe crawler iterates through each keyword, sends queries to SerpAPI, extracts content (including titles, URLs, and full texts), and saves them into a structured format for further processing. This automation ensures scalability and coverage.\n\n\nClick to view code\nif search_results:\n    df_full = pd.DataFrame(search_results)\n    df_unique = df_full.drop_duplicates(subset=['url'], keep='first')\n    print(f\"原始结果: {len(df_full)}，去重后: {len(df_unique)}\")\n\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    df_simple = df_unique[['keyword', 'title', 'url', 'search_timestamp']].copy()\n\n    # 保存到指定 Dataset\n    save_dir = os.path.expanduser(\"/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset\")\n    os.makedirs(save_dir, exist_ok=True)\n\n    df_unique.to_csv(os.path.join(save_dir, f\"search_results_full_{timestamp}.csv\"), index=False, encoding='utf-8-sig')\n    df_simple.to_csv(os.path.join(save_dir, f\"search_results_simple_{timestamp}.csv\"), index=False, encoding='utf-8-sig')\n\n    with pd.ExcelWriter(os.path.join(save_dir, f\"search_results_{timestamp}.xlsx\"), engine='openpyxl') as writer:\n        df_unique.to_excel(writer, sheet_name='完整数据', index=False)\n        df_simple.to_excel(writer, sheet_name='简化数据', index=False)\n\n    print(f\"文件保存路径: {save_dir}\")\n\n    print(\"\\n数据预览:\")\n    print(df_simple.head())\n\n    print(\"\\n每个关键词的结果数量:\")\n    print(df_unique['keyword'].value_counts())\n\n    globals()['search_data'] = df_unique\n    globals()['search_data_simple'] = df_simple\n\n    print(\"\\n变量已加载: search_data, search_data_simple\")\nelse:\n    print(\"没有搜索结果\")\n    globals()['search_data'] = pd.DataFrame()\n    globals()['search_data_simple'] = pd.DataFrame()\n\n\n\n\n\nOnce raw JSON responses are collected, we convert them into a tabular format using pandas. This step prepares the dataset for text cleaning and analysis.\n\n\nClick to view code\nfrom bs4 import BeautifulSoup\nimport chardet\n\n# 文件路径设置\ninput_file = \"/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset/search_results_20250706_192824.xlsx\"\noutput_file = \"/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset/search_results_with_content.xlsx\"\n\n# 读取简化数据工作表\ndf = pd.read_excel(input_file, sheet_name=\"简化数据\")\n\n# 正文提取函数\ndef fetch_article_content(url, timeout=10):\n    try:\n        response = requests.get(url, timeout=timeout, headers={\"User-Agent\": \"Mozilla/5.0\"})\n        detected = chardet.detect(response.content)\n        response.encoding = detected['encoding'] or 'utf-8'\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        paragraphs = soup.find_all('p')\n        content = '\\n'.join(p.get_text(strip=True) for p in paragraphs)\n        return content if len(content) &gt; 50 else None\n    except Exception:\n        return None\n\n# 爬取内容\nprint(\"开始抓取正文内容...\")\ndf[\"content\"] = df[\"url\"].apply(fetch_article_content)\nprint(\"正文抓取完成，开始保存文件...\")\n\n# 保存为新的 Excel 文件\ndf.to_excel(output_file, index=False)\nprint(f\"文件已保存至：{output_file}\")\n\n\n\n\n\n\n\nClick to view code\n# 输入输出路径\ninput_file = \"/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset/search_results_with_content.xlsx\"\noutput_file = \"/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset/search_results_cleaned.xlsx\"\n\n# 加载 Excel 文件\ndf = pd.read_excel(input_file)\n\n# 定义无效内容关键词\nad_keywords = [\n    \"免责声明\", \"广告合作\", \"联系管理员\", \"请在微信中打开\", \"本站所有文章\",\n    \"抱歉\", \"页面不存在\", \"出错\", \"404\", \"请输入验证码\",\n    \"登录查看全文\", \"Oops\", \"Something went wrong\", \"访问受限\"\n]\n\n# 判断是否为无效正文\ndef is_invalid(text):\n    if pd.isna(text):\n        return True\n    if len(text.strip()) &lt; 100:\n        return True\n    if any(kw in text for kw in ad_keywords):\n        return True\n    return False\n\n# 添加标记列\ndf[\"invalid\"] = df[\"content\"].apply(is_invalid)\n\n# 保留有效正文内容\ndf_cleaned = df[~df[\"invalid\"]].drop(columns=[\"invalid\"]).copy()\n\n# 保存清洗后的结果\ndf_cleaned.to_excel(output_file, index=False)\nprint(f\"清洗完成，已保存为：{output_file}\")\n\n\n\n\nClick to view code\n# 读取 Excel 文件\ndf = pd.read_excel(\"search_results_cleaned.xlsx\")\n\n# 1. 查看行数和列数\nprint(\"行数 × 列数:\", df.shape)\n\n# 2. 查看列名和类型\nprint(\"\\n列名与数据类型:\")\nprint(df.dtypes)\n\n# 3. 快速概览每列的前几行（结构 + 值）\nprint(\"\\n样本预览:\")\nprint(df.head())\n\n# 4. 缺失值统计（NA 值）\nprint(\"\\n缺失值统计:\")\nprint(df.isna().sum())\n\n\n行数 × 列数: (90, 6)\n\n列名与数据类型:\nkeyword             object\ntitle               object\nurl                 object\nsearch_timestamp    object\ncontent             object\ninvalid               bool\ndtype: object\n\n样本预览:\n       keyword                           title  \\\n0  中国高净值人士 新加坡          新加坡金融机制稳定吸引最多高净值人士考虑移居   \n1  中国高净值人士 新加坡          我国超高净值人群设立家族办公室现状分析与应对   \n2  中国高净值人士 新加坡  中国高净值人士在新加坡设立家族办公室和投资初创企业的 ...   \n3  中国高净值人士 新加坡        高净值人士为何纷纷选择新加坡？_移民_教育_工作   \n4  中国高净值人士 新加坡    聚焦家办| 亚洲超高净值人群增长或将推动家族办公室的发展   \n\n                                                 url     search_timestamp  \\\n0  https://www.zaobao.com/finance/singapore/story...  2025-07-06 19:19:49   \n1  http://cel.cn/List/FullText?articleId=d37c148c...  2025-07-06 19:19:49   \n2  https://fargowealth.com/en/home/cfsj/cfsj_deta...  2025-07-06 19:19:49   \n3         https://www.sohu.com/a/843292362_121963266  2025-07-06 19:19:49   \n4  https://www.bloombergchina.com/blog/asias-ultr...  2025-07-06 19:19:49   \n\n                                             content  invalid  \n0  \\n\\n新加坡稳定的金融机制和区域联通性对全球富豪具有强大吸引力，是最多富豪首选的移居目的地...    False  \n1  肖京2024-07-08浏览量：1498\\n我国超高净值人群设立家族办公室的需求是客观存在的...    False  \n2  中国高净值人士在新加坡设立家族办公室和投资初创企业的最新趋势\\n五年前，梁信军因健康原因离开...    False  \n3  近年来，越来越多的国内企业家和高净值人士将目光投向新加坡，尤其是那些正在考虑移民或子女教育的...    False  \n4  本文由彭博行业研究高级分析师黄颖珊（Sharnie Wong）撰写，首发于彭博终端。\\n瑞银...    False  \n\n缺失值统计:\nkeyword             0\ntitle               0\nurl                 0\nsearch_timestamp    0\ncontent             0\ninvalid             0\ndtype: int64\n\n\n\n\nClick to view code\ndf_cleaned = df[~df[\"invalid\"] & df[\"content\"].notna()].copy()\ndf_cleaned.to_excel(\"search_results_cleaned.xlsx\", index=False)\n\n\n\n\n\nThe raw text contains punctuation, redundant whitespace, and occasionally malformed encoding. We apply basic cleaning to ensure the corpus is suitable for keyword extraction and topic modeling.\n\n\nClick to view code\n# 1. 加载清洗后的文章数据\ndf = pd.read_excel(\"Dataset/search_results_cleaned.xlsx\")\ndf = df[df[\"content\"].notna() & (df[\"content\"].str.strip() != \"\")]\n\n# 2. 读取停用词表\nwith open(\"Dataset/cn_stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n    stopwords = set([line.strip() for line in f])\n\n# 3. 清洗函数\ndef clean_text(s):\n    s = re.sub(r'&lt;.*?&gt;', '', s)  # HTML标签\n    s = re.sub(r'[a-zA-Z]+', '', s)  # 英文\n    s = re.sub(r'[\\d\\-:/\\.年月日\\s]+', '', s)  # 数字与日期\n    s = re.sub(r'[\\u0000-\\u007F]+', '', s)  # ASCII符号\n    return s.strip()\n\n# 4. 分句 + 清洗 + 分词 + 去停用词\nwords = []\nfor content in df[\"content\"]:\n    sentences = re.split(r'[。！？]', content)\n    for s in sentences:\n        s_clean = clean_text(s)\n        if len(s_clean) &gt;= 5:\n            segs = jieba.cut(s_clean)\n            words += [w for w in segs if len(w) &gt; 1 and w not in stopwords and re.match(r'[\\u4e00-\\u9fff]+', w)]\n\n# 5. 词频统计\nword_freq = Counter(words)\ndf_freq = pd.DataFrame(word_freq.most_common(100), columns=[\"Word\", \"Frequency\"])\n\n\n\n\nClick to view code\n# 6. 保存到文件\ndf_freq.to_excel(\"Dataset/word_frequency_cleaned_with_stopwords.xlsx\", index=False)\n\n\n\n\nClick to view code\n# 7. 打印结果\nprint(df_freq)\n\n\n   Word  Frequency\n0   新加坡       1895\n1    信托       1301\n2    家族       1028\n3    资产        947\n4    投资        922\n..  ...        ...\n95   方式        133\n96   目前        132\n97   顾问        132\n98   数量        128\n99   产品        126\n\n[100 rows x 2 columns]\n\n\n\n\n\nWe extract high-frequency terms to detect common concerns and motivations. This analysis surfaces dominant themes such as “wealth management”, “immigration”, and “Singapore advantage”."
  },
  {
    "objectID": "Market.html#background",
    "href": "Market.html#background",
    "title": "Makert Survey",
    "section": "",
    "text": "To effectively organize and communicate the complex thought process behind why Chinese HNW individuals move their money to Singapore, I created Flowchart. This flowchart helps to:\n\nVisually map the key motivations (such as wealth management, tax benefits, lifestyle advantages, and legacy planning) that drive this financial movement.\nClarify the relationships between client needs, Singapore’s value proposition, and the strategic advisory angles we can take.\nSupport client-facing engagement by simplifying the narrative for Life Inc advisors\nGuide my survey design and presentation content, ensuring all relevant themes—such as trends, preferences, and asset relocation methods—are covered logically and comprehensively.\n\nIt acts as both a research framework and a client communication tool to align our understanding and offerings with the target market’s aspirations."
  },
  {
    "objectID": "Market.html#data-processing",
    "href": "Market.html#data-processing",
    "title": "Makert Survey",
    "section": "",
    "text": "To support the research in Flowchart, I developed a custom web crawler to systematically collect qualitative and contextual data from Chinese-language sources such as forums (e.g., Zhihu, Xueqiu), financial news sites (e.g., Eastern Wealth), and financial blogs. These platforms offer authentic discussions and opinions from Chinese citizens regarding overseas wealth movement—especially to Singapore.\nMy spider focused on keywords that map directly to the key branches of the flowchart, such as:\n\n“新加坡 财富管理” (Singapore wealth management)\n“中国人移民新加坡” (Chinese migration to Singapore)\n“税务优化 新加坡” (Tax optimization Singapore)\n“资产配置 海外” (Offshore asset allocation)\n\n\n\n\n\n\n\nWhy choose spider instead of survey\n\n\n\n\nUnderstand Real Conversations First: The spider captures what people are already saying online, so I don’t assume or guess their opinions.\nGather Rich and Honest Insights: Online posts reveal detailed reasons, emotions, and comparisons that people may not share in surveys.\nReach More People Easily: It’s hard to get high-net-worth individuals to answer surveys, but the spider can access public content instantly.\nBuild a Better Survey Later: By analyzing online themes first, I can design a more focused and relevant survey based on real concerns."
  },
  {
    "objectID": "Market.html#implementation-building-the-spider",
    "href": "Market.html#implementation-building-the-spider",
    "title": "Makert Survey",
    "section": "",
    "text": "We implemented a Python-based web crawler using the SerpAPI platform to scrape real-time search results. The following code initializes the necessary libraries and parameters used in the process.\n\n\nClick to view code\nimport requests\nimport json\nimport time\nimport pandas as pd\nfrom datetime import datetime\nimport os\nimport openpyxl\nimport re\nfrom collections import Counter\nimport jieba\n\n# 用户配置区域\nSERPAPI_KEY = \"84c798a8d45d1e7cae0b18df778ac06bf2c6169f0249e40756aea0b9d6cd4749\"  \nRESULTS_PER_KEYWORD = 20\nDELAY_BETWEEN_REQUESTS = 1\nDELAY_BETWEEN_KEYWORDS = 2"
  },
  {
    "objectID": "Market.html#targeted-search-keywords",
    "href": "Market.html#targeted-search-keywords",
    "title": "Makert Survey",
    "section": "",
    "text": "The keywords used by the crawler are carefully chosen to reflect real user concerns about wealth migration. These terms guide the spider to relevant discussions, articles, and posts.\n\n\nClick to view code\nkeywords = [\n    \"中国高净值人士 新加坡\",\n    \"中国富人 资产配置 新加坡\", \n    \"中国富豪 为什么移民新加坡\",\n    \"中国高净值客户 离岸账户\",\n    \"新加坡 CRS 避税\",\n    \"中国 家族信托 新加坡\",\n    \"中国高净值人士 子女教育 新加坡\",\n    \"中国移民新加坡 财富管理\"\n]\n\n\n\n\nClick to view code\ndef serpapi_search(query, api_key, num=20, start=0):\n    url = \"https://serpapi.com/search\"\n    params = {\n        'q': query,\n        'api_key': api_key,\n        'engine': 'google',\n        'num': min(num, 100),\n        'start': start,\n        'hl': 'zh-cn',\n        'gl': 'cn'\n    }\n    try:\n        response = requests.get(url, params=params)\n        response.raise_for_status()\n        return response.json()\n    except requests.exceptions.RequestException as e:\n        print(f\"搜索请求失败: {e}\")\n        return {}\n\ndef extract_search_results(results, keyword):\n    extracted_results = []\n    organic_results = results.get('organic_results', [])\n    for item in organic_results:\n        result_info = {\n            'keyword': keyword,\n            'title': item.get('title', ''),\n            'url': item.get('link', ''),\n            'snippet': item.get('snippet', ''),\n            'displayed_link': item.get('displayed_link', ''),\n            'position': item.get('position', 0),\n            'search_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        }\n        extracted_results.append(result_info)\n    return extracted_results\n\n\n\n\nClick to view code\ndef search_keyword_with_pagination(keyword, api_key, total_results=20):\n    all_results = []\n    results_per_page = 10\n    for start in range(0, total_results, results_per_page):\n        remaining = total_results - start\n        num_to_get = min(results_per_page, remaining)\n        print(f\"    获取第 {start+1}-{start+num_to_get} 条结果...\")\n        results = serpapi_search(keyword, api_key, num=num_to_get, start=start)\n        if not results or 'organic_results' not in results:\n            print(f\"没有更多结果\")\n            break\n        extracted = extract_search_results(results, keyword)\n        all_results.extend(extracted)\n        if len(extracted) &lt; num_to_get:\n            break\n        time.sleep(DELAY_BETWEEN_REQUESTS)\n    return all_results\n\n\n\n\nClick to view code\ndef main():\n    print(\"开始搜索...\")\n    print(f\"搜索关键词数量: {len(keywords)}\")\n    print(f\"每个关键词获取结果数: {RESULTS_PER_KEYWORD}\")\n    print(\"-\" * 50)\n    \n    all_search_results = []\n    for i, keyword in enumerate(keywords, 1):\n        print(f\"[{i}/{len(keywords)}] 搜索关键词: {keyword}\")\n        try:\n            results = search_keyword_with_pagination(keyword, SERPAPI_KEY, RESULTS_PER_KEYWORD)\n            all_search_results.extend(results)\n            print(f\"找到 {len(results)} 个结果\")\n            if i &lt; len(keywords):\n                print(f\"等待 {DELAY_BETWEEN_KEYWORDS} 秒...\")\n                time.sleep(DELAY_BETWEEN_KEYWORDS)\n        except Exception as e:\n            print(f\"错误: {e}\")\n            continue\n    print(\"-\" * 50)\n    print(f\"总共找到 {len(all_search_results)} 个结果\")\n    return all_search_results\n\nsearch_results = main()"
  },
  {
    "objectID": "Market.html#data-collection-process",
    "href": "Market.html#data-collection-process",
    "title": "Makert Survey",
    "section": "",
    "text": "The crawler iterates through each keyword, sends queries to SerpAPI, extracts content (including titles, URLs, and full texts), and saves them into a structured format for further processing. This automation ensures scalability and coverage.\n\n\nClick to view code\nif search_results:\n    df_full = pd.DataFrame(search_results)\n    df_unique = df_full.drop_duplicates(subset=['url'], keep='first')\n    print(f\"原始结果: {len(df_full)}，去重后: {len(df_unique)}\")\n\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    df_simple = df_unique[['keyword', 'title', 'url', 'search_timestamp']].copy()\n\n    # 保存到指定 Dataset\n    save_dir = os.path.expanduser(\"/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset\")\n    os.makedirs(save_dir, exist_ok=True)\n\n    df_unique.to_csv(os.path.join(save_dir, f\"search_results_full_{timestamp}.csv\"), index=False, encoding='utf-8-sig')\n    df_simple.to_csv(os.path.join(save_dir, f\"search_results_simple_{timestamp}.csv\"), index=False, encoding='utf-8-sig')\n\n    with pd.ExcelWriter(os.path.join(save_dir, f\"search_results_{timestamp}.xlsx\"), engine='openpyxl') as writer:\n        df_unique.to_excel(writer, sheet_name='完整数据', index=False)\n        df_simple.to_excel(writer, sheet_name='简化数据', index=False)\n\n    print(f\"文件保存路径: {save_dir}\")\n\n    print(\"\\n数据预览:\")\n    print(df_simple.head())\n\n    print(\"\\n每个关键词的结果数量:\")\n    print(df_unique['keyword'].value_counts())\n\n    globals()['search_data'] = df_unique\n    globals()['search_data_simple'] = df_simple\n\n    print(\"\\n变量已加载: search_data, search_data_simple\")\nelse:\n    print(\"没有搜索结果\")\n    globals()['search_data'] = pd.DataFrame()\n    globals()['search_data_simple'] = pd.DataFrame()"
  },
  {
    "objectID": "Market.html#data-transformation",
    "href": "Market.html#data-transformation",
    "title": "Makert Survey",
    "section": "",
    "text": "Once raw JSON responses are collected, we convert them into a tabular format using pandas. This step prepares the dataset for text cleaning and analysis.\n\n\nClick to view code\nfrom bs4 import BeautifulSoup\nimport chardet\n\n# 文件路径设置\ninput_file = \"/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset/search_results_20250706_192824.xlsx\"\noutput_file = \"/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset/search_results_with_content.xlsx\"\n\n# 读取简化数据工作表\ndf = pd.read_excel(input_file, sheet_name=\"简化数据\")\n\n# 正文提取函数\ndef fetch_article_content(url, timeout=10):\n    try:\n        response = requests.get(url, timeout=timeout, headers={\"User-Agent\": \"Mozilla/5.0\"})\n        detected = chardet.detect(response.content)\n        response.encoding = detected['encoding'] or 'utf-8'\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        paragraphs = soup.find_all('p')\n        content = '\\n'.join(p.get_text(strip=True) for p in paragraphs)\n        return content if len(content) &gt; 50 else None\n    except Exception:\n        return None\n\n# 爬取内容\nprint(\"开始抓取正文内容...\")\ndf[\"content\"] = df[\"url\"].apply(fetch_article_content)\nprint(\"正文抓取完成，开始保存文件...\")\n\n# 保存为新的 Excel 文件\ndf.to_excel(output_file, index=False)\nprint(f\"文件已保存至：{output_file}\")"
  },
  {
    "objectID": "Market.html#data-clean",
    "href": "Market.html#data-clean",
    "title": "Makert Survey",
    "section": "",
    "text": "Click to view code\n# 输入输出路径\ninput_file = \"/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset/search_results_with_content.xlsx\"\noutput_file = \"/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset/search_results_cleaned.xlsx\"\n\n# 加载 Excel 文件\ndf = pd.read_excel(input_file)\n\n# 定义无效内容关键词\nad_keywords = [\n    \"免责声明\", \"广告合作\", \"联系管理员\", \"请在微信中打开\", \"本站所有文章\",\n    \"抱歉\", \"页面不存在\", \"出错\", \"404\", \"请输入验证码\",\n    \"登录查看全文\", \"Oops\", \"Something went wrong\", \"访问受限\"\n]\n\n# 判断是否为无效正文\ndef is_invalid(text):\n    if pd.isna(text):\n        return True\n    if len(text.strip()) &lt; 100:\n        return True\n    if any(kw in text for kw in ad_keywords):\n        return True\n    return False\n\n# 添加标记列\ndf[\"invalid\"] = df[\"content\"].apply(is_invalid)\n\n# 保留有效正文内容\ndf_cleaned = df[~df[\"invalid\"]].drop(columns=[\"invalid\"]).copy()\n\n# 保存清洗后的结果\ndf_cleaned.to_excel(output_file, index=False)\nprint(f\"清洗完成，已保存为：{output_file}\")\n\n\n\n\nClick to view code\n# 读取 Excel 文件\ndf = pd.read_excel(\"search_results_cleaned.xlsx\")\n\n# 1. 查看行数和列数\nprint(\"行数 × 列数:\", df.shape)\n\n# 2. 查看列名和类型\nprint(\"\\n列名与数据类型:\")\nprint(df.dtypes)\n\n# 3. 快速概览每列的前几行（结构 + 值）\nprint(\"\\n样本预览:\")\nprint(df.head())\n\n# 4. 缺失值统计（NA 值）\nprint(\"\\n缺失值统计:\")\nprint(df.isna().sum())\n\n\n行数 × 列数: (90, 6)\n\n列名与数据类型:\nkeyword             object\ntitle               object\nurl                 object\nsearch_timestamp    object\ncontent             object\ninvalid               bool\ndtype: object\n\n样本预览:\n       keyword                           title  \\\n0  中国高净值人士 新加坡          新加坡金融机制稳定吸引最多高净值人士考虑移居   \n1  中国高净值人士 新加坡          我国超高净值人群设立家族办公室现状分析与应对   \n2  中国高净值人士 新加坡  中国高净值人士在新加坡设立家族办公室和投资初创企业的 ...   \n3  中国高净值人士 新加坡        高净值人士为何纷纷选择新加坡？_移民_教育_工作   \n4  中国高净值人士 新加坡    聚焦家办| 亚洲超高净值人群增长或将推动家族办公室的发展   \n\n                                                 url     search_timestamp  \\\n0  https://www.zaobao.com/finance/singapore/story...  2025-07-06 19:19:49   \n1  http://cel.cn/List/FullText?articleId=d37c148c...  2025-07-06 19:19:49   \n2  https://fargowealth.com/en/home/cfsj/cfsj_deta...  2025-07-06 19:19:49   \n3         https://www.sohu.com/a/843292362_121963266  2025-07-06 19:19:49   \n4  https://www.bloombergchina.com/blog/asias-ultr...  2025-07-06 19:19:49   \n\n                                             content  invalid  \n0  \\n\\n新加坡稳定的金融机制和区域联通性对全球富豪具有强大吸引力，是最多富豪首选的移居目的地...    False  \n1  肖京2024-07-08浏览量：1498\\n我国超高净值人群设立家族办公室的需求是客观存在的...    False  \n2  中国高净值人士在新加坡设立家族办公室和投资初创企业的最新趋势\\n五年前，梁信军因健康原因离开...    False  \n3  近年来，越来越多的国内企业家和高净值人士将目光投向新加坡，尤其是那些正在考虑移民或子女教育的...    False  \n4  本文由彭博行业研究高级分析师黄颖珊（Sharnie Wong）撰写，首发于彭博终端。\\n瑞银...    False  \n\n缺失值统计:\nkeyword             0\ntitle               0\nurl                 0\nsearch_timestamp    0\ncontent             0\ninvalid             0\ndtype: int64\n\n\n\n\nClick to view code\ndf_cleaned = df[~df[\"invalid\"] & df[\"content\"].notna()].copy()\ndf_cleaned.to_excel(\"search_results_cleaned.xlsx\", index=False)"
  },
  {
    "objectID": "Market.html#text-cleaning",
    "href": "Market.html#text-cleaning",
    "title": "Makert Survey",
    "section": "",
    "text": "The raw text contains punctuation, redundant whitespace, and occasionally malformed encoding. We apply basic cleaning to ensure the corpus is suitable for keyword extraction and topic modeling.\n\n\nClick to view code\n# 1. 加载清洗后的文章数据\ndf = pd.read_excel(\"Dataset/search_results_cleaned.xlsx\")\ndf = df[df[\"content\"].notna() & (df[\"content\"].str.strip() != \"\")]\n\n# 2. 读取停用词表\nwith open(\"Dataset/cn_stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n    stopwords = set([line.strip() for line in f])\n\n# 3. 清洗函数\ndef clean_text(s):\n    s = re.sub(r'&lt;.*?&gt;', '', s)  # HTML标签\n    s = re.sub(r'[a-zA-Z]+', '', s)  # 英文\n    s = re.sub(r'[\\d\\-:/\\.年月日\\s]+', '', s)  # 数字与日期\n    s = re.sub(r'[\\u0000-\\u007F]+', '', s)  # ASCII符号\n    return s.strip()\n\n# 4. 分句 + 清洗 + 分词 + 去停用词\nwords = []\nfor content in df[\"content\"]:\n    sentences = re.split(r'[。！？]', content)\n    for s in sentences:\n        s_clean = clean_text(s)\n        if len(s_clean) &gt;= 5:\n            segs = jieba.cut(s_clean)\n            words += [w for w in segs if len(w) &gt; 1 and w not in stopwords and re.match(r'[\\u4e00-\\u9fff]+', w)]\n\n# 5. 词频统计\nword_freq = Counter(words)\ndf_freq = pd.DataFrame(word_freq.most_common(100), columns=[\"Word\", \"Frequency\"])\n\n\n\n\nClick to view code\n# 6. 保存到文件\ndf_freq.to_excel(\"Dataset/word_frequency_cleaned_with_stopwords.xlsx\", index=False)\n\n\n\n\nClick to view code\n# 7. 打印结果\nprint(df_freq)\n\n\n   Word  Frequency\n0   新加坡       1895\n1    信托       1301\n2    家族       1028\n3    资产        947\n4    投资        922\n..  ...        ...\n95   方式        133\n96   目前        132\n97   顾问        132\n98   数量        128\n99   产品        126\n\n[100 rows x 2 columns]"
  },
  {
    "objectID": "Market.html#keyword-frequency",
    "href": "Market.html#keyword-frequency",
    "title": "Makert Survey",
    "section": "",
    "text": "We extract high-frequency terms to detect common concerns and motivations. This analysis surfaces dominant themes such as “wealth management”, “immigration”, and “Singapore advantage”."
  },
  {
    "objectID": "Market.html#word-cloud",
    "href": "Market.html#word-cloud",
    "title": "Makert Survey",
    "section": "2.1 Word Cloud",
    "text": "2.1 Word Cloud"
  },
  {
    "objectID": "Market.html#visualizing-high-frequency-terms",
    "href": "Market.html#visualizing-high-frequency-terms",
    "title": "Makert Survey",
    "section": "Visualizing High-Frequency Terms",
    "text": "Visualizing High-Frequency Terms\nThe following word cloud ord highlights the most common terms across all retrieved content, offering a visual snapshot of what matters most to the audience.\n\n\nClick to view code\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# 1. 加载词频数据\ndf_freq = pd.read_excel(\"Dataset/word_frequency_cleaned_with_stopwords.xlsx\")\n\n# 2. 转换为字典格式\nfreq_dict = dict(zip(df_freq[\"Word\"], df_freq[\"Frequency\"]))\n\n# 3. 创建词云对象\nwc = WordCloud(\n    font_path=\"/System/Library/Fonts/STHeiti Medium.ttc\",  # 替换为你本地支持中文的字体路径\n    background_color=\"white\",\n    width=1000,\n    height=700,\n    max_words=200\n).generate_from_frequencies(freq_dict)\n\n# 4. 可视化词云\nplt.figure(figsize=(12, 8))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"WordCloud\", fontsize=18)\nplt.subplots_adjust(top=0.85) \nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights from the Word Cloud\n\n\n\nThe word cloud visually reinforces the central motivations driving Chinese high-net-worth individuals (HNWIs) to move their assets to Singapore.\n\nHigh-frequency terms such as “财富管理” (wealth management), “资产配置” (asset allocation), and “税务优化” (tax optimization) directly align with the motivations outlined in our initial flowchart.\nKeywords like “家族信托” (family trust), “离岸账户” (offshore accounts), and “子女教育” (children’s education) indicate a strong emphasis on legacy planning, asset protection, and long-term family welfare.\n\nConclusion: These themes confirm that Singapore is not just attractive for its financial incentives—but as a comprehensive hub for multi-generational wealth security and elite lifestyle planning."
  },
  {
    "objectID": "Market.html#sentiment-analysis",
    "href": "Market.html#sentiment-analysis",
    "title": "Makert Survey",
    "section": "2.2 Sentiment Analysis",
    "text": "2.2 Sentiment Analysis\n\n\nClick to view code\nfrom snownlp import SnowNLP\n\n# 读取已清洗的数据\ndf = pd.read_excel(\"Dataset/search_results_cleaned.xlsx\")\n\n# 过滤掉无正文\ndf = df[df[\"content\"].notna() & (df[\"content\"].str.len() &gt; 30)].copy()\n\n# 情感分析函数（返回值在 0～1 之间，1 越积极）\ndef get_sentiment(text):\n    try:\n        return SnowNLP(text).sentiments\n    except:\n        return None\n\n# 添加情感评分列\ndf[\"sentiment_score\"] = df[\"content\"].apply(get_sentiment)\n\n# 分类标签：大于 0.6 为正面，小于 0.4 为负面，其余为中性\ndef classify(score):\n    if score is None:\n        return \"Unknown\"\n    elif score &gt; 0.6:\n        return \"Positive\"\n    elif score &lt; 0.4:\n        return \"Negative\"\n    else:\n        return \"Neutral\"\n\ndf[\"sentiment_label\"] = df[\"sentiment_score\"].apply(classify)\n\n# 保存结果\ndf.to_excel(\"Dataset/search_results_sentiment.xlsx\", index=False)\n\n# 查看统计\nprint(df[\"sentiment_label\"].value_counts())\n\n\nsentiment_label\nPositive    85\nNegative     5\nName: count, dtype: int64\n\n\n\n\nClick to view code\nimport matplotlib.pyplot as plt\n\n# Count sentiment\nsentiment_counts = df[\"sentiment_label\"].value_counts()\n\n# Set labels and colors\nlabels = sentiment_counts.index.tolist()\ncolors = ['#4CAF50' if label == 'Positive' else '#F44336' for label in labels]  \n\n# Define explode to slightly offset each slice\nexplode = [0.05] * len(labels)\n\n# Create figure\nfig, ax = plt.subplots(figsize=(7, 7), facecolor='white')\nwedges, texts, autotexts = ax.pie(\n    sentiment_counts,\n    labels=labels,\n    autopct='%1.1f%%',\n    startangle=140,\n    colors=colors,\n    explode=explode,\n    wedgeprops={'edgecolor': 'white', 'linewidth': 2},\n    textprops={'fontsize': 13}\n)\n\n# Labels and percentages\nfor text in autotexts:\n    text.set_color('white')\n    text.set_fontweight('bold')\n\n# Add title with padding\nplt.title(\"Sentiment Composition\", fontsize=18, weight='bold', pad=20)\n\n# Equal aspect ratio ensures pie is circular\nax.axis('equal')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights from Sentiment Analysis\n\n\n\nThe sentiment analysis reveals a mixed but insightful emotional tone surrounding discussions of wealth migration to Singapore.\n\nA significant portion of posts are positive, reflecting appreciation for Singapore’s legal stability, tax efficiency, and quality of life.\nNegative sentiments mainly revolve around concerns about regulatory changes, barriers to entry, or uncertainty around immigration pathways.\nNeutral discussions tend to be informational or comparative, providing objective assessments of different offshore options.\n\nConclusion: These findings suggest that while Singapore is generally viewed favorably, there remains a need to address misconceptions and reduce friction in communication when engaging with HNW prospects.\n\n\n\n\nClick to view code\nimport matplotlib.pyplot as plt\n\nplt.rcParams['font.family'] = 'Arial Unicode MS'  \nplt.rcParams['axes.unicode_minus'] = False\n\n# 分组：统计每个 keyword 下的正负面数量\nsentiment_by_keyword = df.groupby(['keyword', 'sentiment_label']).size().unstack(fill_value=0)\n\n# 打印检查\nprint(sentiment_by_keyword)\n\n# 绘图：堆叠柱状图\nsentiment_by_keyword.plot(kind='bar', stacked=True, figsize=(12, 6), color=[\"#1f77b4\", \"#ff7f0e\"])  \n\n# 设置标题和轴标签\nplt.title(\"关键词情感构成\", fontsize=16)\nplt.xlabel(\"关键词\", fontsize=12)\nplt.ylabel(\"文章数量\", fontsize=12)\n\n# 旋转 x 轴文字防止重叠\nplt.xticks(rotation=45, ha='right')\n\n# 自动布局\nplt.tight_layout()\n\n# 显示图像\nplt.show()\n\n\nsentiment_label   Negative  Positive\nkeyword                             \n中国 家族信托 新加坡              0        14\n中国富人 资产配置 新加坡            1         8\n中国富豪 为什么移民新加坡            1         5\n中国移民新加坡 财富管理             0        11\n中国高净值人士 子女教育 新加坡         0         9\n中国高净值人士 新加坡              2        12\n中国高净值客户 离岸账户             0        15\n新加坡 CRS 避税               1        11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext Step: Deep Dive into Negative Sentiments\n\n\n\nTo strengthen client engagement and tailor advisory strategies, further analysis should focus on uncovering the root causes behind negative or hesitant views. Recommended steps include:\n\nThematic Clustering: Group negative posts into key concerns—e.g., “regulatory fears”, “immigration complexity”, “trust issues”.\nTemporal Sentiment Tracking: Detect if negative sentiment spikes after specific events (e.g., new financial regulations in China).\n\nBy understanding the friction points in sentiment, Life Inc can refine positioning, improve client confidence, and remove silent blockers in the decision-making journey."
  },
  {
    "objectID": "Market.html#network-analysis",
    "href": "Market.html#network-analysis",
    "title": "Makert Survey",
    "section": "2.3 Network Analysis",
    "text": "2.3 Network Analysis\n\n\nClick to view code\nfrom itertools import combinations\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# 用于存储每句话的关键词\nkeyword_sentences = []\n\nfor content in df[\"content\"]:\n    sentences = re.split(r'[。！？]', content)\n    for s in sentences:\n        s_clean = clean_text(s)\n        if len(s_clean) &gt;= 5:\n            segs = jieba.cut(s_clean)\n            word_list = [w for w in segs if len(w) &gt; 1 and w not in stopwords and re.match(r'[\\u4e00-\\u9fff]+', w)]\n            keyword_sentences.append(word_list)\n\n\n\n\nClick to view code\ntop_100_words = set(df_freq[\"Word\"])\nco_occurrence = Counter()\n\n# 只统计 top100 词之间的共现\nfor word_list in keyword_sentences:\n    words_in_top100 = [w for w in word_list if w in top_100_words]\n    for pair in combinations(set(words_in_top100), 2):  # set 去重\n        co_occurrence[tuple(sorted(pair))] += 1\n\n\n\n\nClick to view code\nimport matplotlib.font_manager as fm\nfor font in fm.findSystemFonts(fontpaths=None, fontext='ttf'):\n    if 'PingFang' in font or 'Arial' in font or 'Hei' in font:\n        print(font)\n\n\n/System/Library/Fonts/Supplemental/Arial Rounded Bold.ttf\n/System/Library/Fonts/Supplemental/Arial Narrow.ttf\n/System/Library/Fonts/ArialHB.ttc\n/System/Library/Fonts/Supplemental/Arial.ttf\n/System/Library/Fonts/Supplemental/Arial Unicode.ttf\n/System/Library/Fonts/Supplemental/Arial Italic.ttf\n/System/Library/Fonts/Supplemental/Arial Bold Italic.ttf\n/System/Library/Fonts/Supplemental/Arial Narrow Bold.ttf\n/Library/Fonts/Arial Unicode.ttf\n/System/Library/Fonts/Supplemental/Arial Bold.ttf\n/System/Library/Fonts/STHeiti Medium.ttc\n/System/Library/Fonts/PingFang.ttc\n/System/Library/Fonts/STHeiti Light.ttc\n/System/Library/Fonts/Supplemental/Arial Black.ttf\n/System/Library/Fonts/Supplemental/Arial Narrow Bold Italic.ttf\n/System/Library/Fonts/Supplemental/Arial Narrow Italic.ttf\n\n\n\n\nClick to view code\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\nimport networkx as nx\n\n# 手动设置中文字体\nmy_font = fm.FontProperties(fname=\"/System/Library/Fonts/STHeiti Medium.ttc\")\nplt.rcParams['font.family'] = my_font.get_name()\nplt.rcParams['axes.unicode_minus'] = False\n\n# 构建图\nG = nx.Graph()\nfor (w1, w2), freq in co_occurrence.items():\n    if freq &gt;= 3:  # 设定共现阈值\n        G.add_edge(w1, w2, weight=freq)\n\n# 可视化绘图\nplt.figure(figsize=(12, 10))\npos = nx.spring_layout(G, k=0.5, seed=42)  # 节点布局\nnx.draw_networkx_nodes(G, pos, node_size=500, node_color='lightblue')\nnx.draw_networkx_edges(G, pos, width=[d['weight'] * 0.3 for _, _, d in G.edges(data=True)], alpha=0.6)\nnx.draw_networkx_labels(G, pos, font_size=10, font_family=my_font.get_name())\n\nplt.title(\"Top 100 中文关键词共现网络\", fontproperties=my_font)\nplt.axis(\"off\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights from Co-occurrence Keyword Network\n\n\n\nThis network reveals how important themes surrounding wealth migration to Singapore co-appear and reinforce each other in authentic online discussions.\n\nWords like “资产配置” (asset allocation), “新加坡”, and “家族信托” frequently co-occur, forming dense hubs of strategic planning.\nThe structure suggests that users don’t speak of single motivations in isolation — tax planning, family trust, and regulatory considerations are often bundled in the same conversation.\nHigh-degree nodes like “CRS”, “移民”, and “离岸账户” serve as bridges across different topic communities.\n\nConclusion: These patterns demonstrate that Chinese HNWIs approach offshore wealth management as a multifaceted decision process. Singapore is appealing not for a single advantage, but for its ability to address multiple interconnected concerns simultaneously.\n\n\n\n\nClick to view code\n# 计算节点的度中心性\ndegree_centrality = nx.degree_centrality(G)\n\n# 按中心性排序，取前10个关键词\ntop_nodes = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n\n# 打印结果\nprint(\"Top 10 关键词:\")\nfor word, score in top_nodes:\n    print(f\"{word}: {score:.3f}\")\n\n\nTop 10 关键词:\n新加坡: 1.000\n资产: 1.000\n投资: 1.000\n公司: 1.000\n管理: 0.980\n净值: 0.970\n中国: 0.970\n香港: 0.970\n进行: 0.970\n财富: 0.960\n\n\n\n\nClick to view code\nimport community as community_louvain  # pip install python-louvain\nimport matplotlib.cm as cm\n\n# 社区划分\npartition = community_louvain.best_partition(G)\n\n# 设置颜色映射\nsize = float(len(set(partition.values())))\npos = nx.spring_layout(G, k=0.5, seed=42)\ncolors = [cm.tab20(i / size) for i in partition.values()]\n\n# 绘图\nplt.figure(figsize=(14, 12))\nnx.draw_networkx_nodes(G, pos, node_size=500, node_color=colors, alpha=0.8)\nnx.draw_networkx_edges(G, pos, width=0.5, alpha=0.3)\nnx.draw_networkx_labels(G, pos, font_size=10, font_family='Arial Unicode MS')\nplt.title(\"关键词共现网络中的话题社区\")\nplt.axis(\"off\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights from Color-Labeled Keyword Communities\n\n\n\nThe co-occurrence network, enhanced with semantic color tagging, reveals four distinct communities that structure online discussions around wealth migration to Singapore:\n\n🌕 Community 1 (Brown) focuses on trust and legal structures, with keywords like “trust”, “assets”, “fund”, and “legal” — underscoring the importance of compliant, long-term wealth management.\n🟢 Community 2 (Green) centers around client advisory and immigration services, showing that service quality and immigration logistics are major touchpoints for Chinese HNWIs.\n🔵 Community 3 (Blue) is tied to cross-border financial assets and tax implications, with mentions of “Hong Kong”, “tax”, and “net worth” — pointing to regional diversification strategies.\n⚫ Community 4 (Gray) contains more generic or connective language, serving as linguistic bridges in user conversations.\n\nConclusion: This structured clustering confirms that Chinese HNWIs are not driven by a single reason to move assets offshore. Instead, their motivations form a multi-layered ecosystem — blending financial safety, global mobility, tax planning, and trusted advisory. Singapore is uniquely positioned as a hub that satisfies all these needs simultaneously."
  },
  {
    "objectID": "Market.html#final-takeaways-why-do-chinese-hnwis-move-their-wealth-to-singapore",
    "href": "Market.html#final-takeaways-why-do-chinese-hnwis-move-their-wealth-to-singapore",
    "title": "Makert Survey",
    "section": "Final Takeaways: Why Do Chinese HNWIs Move Their Wealth to Singapore?",
    "text": "Final Takeaways: Why Do Chinese HNWIs Move Their Wealth to Singapore?\nThrough the combined use of structured flowchart thinking and real-world data collection using a customized web crawler, this study provides a grounded answer to our core question.\n\nKey Findings:\n\nWealth Management & Asset Protection: Terms like “财富管理” (wealth management) and “资产配置” (asset allocation) appeared frequently, highlighting the strong demand for stable and diversified wealth strategies.\nTax Optimization: Frequent mentions of “税务优化” and “CRS避税” reflect concerns over rising tax scrutiny in China and Singapore’s more favorable policies.\nEducational Planning & Family Legacy: Phrases such as “子女教育” and “家族信托” indicate that many HNWIs are motivated by long-term family goals rather than short-term returns.\nPolitical and Legal Stability: Although more subtle, the preference for Singapore’s legal infrastructure and business environment emerged from context-rich discussions.\n\n\n\nWhy This Method Worked:\nBy using a spider to extract public sentiment directly from platforms like Zhihu and Xueqiu, we bypassed the sampling bias of traditional surveys and captured more nuanced and emotionally honest reasons behind wealth relocation behavior."
  },
  {
    "objectID": "Market.html#strategic-implications",
    "href": "Market.html#strategic-implications",
    "title": "Makert Survey",
    "section": "Strategic Implications",
    "text": "Strategic Implications\n\nFor Wealth Advisors\nAlign advisory messaging with clients’ emotional priorities—especially trust, family security, and intergenerational control. Use insights from sentiment clustering to reframe conversations from “products” to “protecting legacy.”\n\nExample:\nReplace “Let’s set up a trust to reduce tax” with\n“Let’s future-proof your family’s assets so your son can inherit them securely—no matter what happens in either country.”\n\n\n\n\nFor Life Inc\nBuild theme-based planning kits (e.g. Trust + Tax + Education) mapped to the four identified keyword communities. Invest in bilingual onboarding, WeChat-style content formats, and cross-border collaboration frameworks to serve Chinese HNWIs holistically.\n\nExample:\nDevelop a digital “Singapore Welcome Pack” with Mandarin guides on:\n- How to legally move funds\n- Trust structure overview\n- PR application tips\nDelivered via a mini-site or WeChat-compatible PDF deck.\n\n\n\n\nFor Company Strategy\nEstablish modular advisory paths for three core personas—Forward Planner, Risk Avoider, and Legacy Seeker. Each path should include tax-legal coordination, lifestyle concierge, and asset structuring guidance anchored in Singapore.\n\nExample:\nCreate a “Legacy Seeker Pathway”:\n- Intro session with bilingual legacy advisor\n- Local will & trust setup\n- Heir education roadmap (e.g. next-gen wealth workshops)\n\n\n\n\nFor Survey & Research\nRefine survey instruments to probe latent motivations (e.g. fear of instability, desire for international lifestyle) and validate which service combinations resonate most. Use network clusters to shape both question content and option phrasing.\n\nExample:\nInclude a ranking question:\n“Which matters more to you in choosing a jurisdiction for your family’s assets?”\n(Options: Political stability / Education system / Inheritance clarity / Low tax burden)\n→ Each option maps to a keyword community in your analysis.\n\n\nIn conclusion, the answer is not singular—but multi-dimensional. Singapore is attractive to Chinese HNWIs not only for its tax and legal advantages, but because it offers stability, safety, and long-term opportunities for families and wealth."
  }
]