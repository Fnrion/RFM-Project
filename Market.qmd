---
title: "Makert Survey"
author: "Zou Jiaxun"
execute: 
  eval: true
  echo: true
  warning: false
  freeze: true
  cache: true
format:
  html:
    code-fold: true
    code-summary: "Click to view code"
jupyter: python3  
---

# 1 **Overview**

## 1.1 **Background**

To effectively organize and communicate the complex thought process behind why Chinese HNW individuals move their money to Singapore, I created Flowchart. This flowchart helps to:

-   Visually map the key motivations (such as wealth management, tax benefits, lifestyle advantages, and legacy planning) that drive this financial movement.

-   Clarify the relationships between client needs, Singaporeâ€™s value proposition, and the strategic advisory angles we can take.

-   Support client-facing engagement by simplifying the narrative for Life Inc advisors

-   Guide my survey design and presentation content, ensuring all relevant themesâ€”such as trends, preferences, and asset relocation methodsâ€”are covered logically and comprehensively.

It acts as both a research framework and a client communication tool to align our understanding and offerings with the target marketâ€™s aspirations.

![](Picture/Flowchart1.png){fig-align="center"}

## **1.2 Data Processing**

To support the research in Flowchart, I developed a custom web crawler to systematically collect qualitative and contextual data from Chinese-language sources such as forums (e.g., Zhihu, Xueqiu), financial news sites (e.g., Eastern Wealth), and financial blogs. These platforms offer authentic discussions and opinions from Chinese citizens regarding overseas wealth movementâ€”especially to Singapore.

My spider focused on keywords that map directly to the key branches of the flowchart, such as:

-   â€œæ–°åŠ å¡ è´¢å¯Œç®¡ç†â€ (Singapore wealth management)

-   â€œä¸­å›½äººç§»æ°‘æ–°åŠ å¡â€ (Chinese migration to Singapore)

-   â€œç¨åŠ¡ä¼˜åŒ– æ–°åŠ å¡â€ (Tax optimization Singapore)

-   â€œèµ„äº§é…ç½® æµ·å¤–â€ (Offshore asset allocation)

::: callout-note
## Why choose spider instead of survey

-   **Understand Real Conversations First**: The spider captures what people are already saying online, so I donâ€™t assume or guess their opinions.
-   **Gather Rich and Honest Insights**: Online posts reveal detailed reasons, emotions, and comparisons that people may not share in surveys.
-   **Reach More People Easily**: Itâ€™s hard to get high-net-worth individuals to answer surveys, but the spider can access public content instantly.
-   **Build a Better Survey Later**: By analyzing online themes first, I can design a more focused and relevant survey based on real concerns.
:::

## Implementation: Building the Spider

We implemented a Python-based web crawler using the SerpAPI platform to scrape real-time search results. The following code initializes the necessary libraries and parameters used in the process.

```{python}
import requests
import json
import time
import pandas as pd
from datetime import datetime
import os
import openpyxl

# ç”¨æˆ·é…ç½®åŒºåŸŸ
SERPAPI_KEY = "84c798a8d45d1e7cae0b18df778ac06bf2c6169f0249e40756aea0b9d6cd4749"  
RESULTS_PER_KEYWORD = 20
DELAY_BETWEEN_REQUESTS = 1
DELAY_BETWEEN_KEYWORDS = 2
```

## Targeted Search Keywords

The keywords used by the crawler are carefully chosen to reflect real user concerns about wealth migration. These terms guide the spider to relevant discussions, articles, and posts.

```{python}
keywords = [
    "ä¸­å›½é«˜å‡€å€¼äººå£« æ–°åŠ å¡",
    "ä¸­å›½å¯Œäºº èµ„äº§é…ç½® æ–°åŠ å¡", 
    "ä¸­å›½å¯Œè±ª ä¸ºä»€ä¹ˆç§»æ°‘æ–°åŠ å¡",
    "ä¸­å›½é«˜å‡€å€¼å®¢æˆ· ç¦»å²¸è´¦æˆ·",
    "æ–°åŠ å¡ CRS é¿ç¨",
    "ä¸­å›½ å®¶æ—ä¿¡æ‰˜ æ–°åŠ å¡",
    "ä¸­å›½é«˜å‡€å€¼äººå£« å­å¥³æ•™è‚² æ–°åŠ å¡",
    "ä¸­å›½ç§»æ°‘æ–°åŠ å¡ è´¢å¯Œç®¡ç†"
]
```

```{python}
def serpapi_search(query, api_key, num=20, start=0):
    url = "https://serpapi.com/search"
    params = {
        'q': query,
        'api_key': api_key,
        'engine': 'google',
        'num': min(num, 100),
        'start': start,
        'hl': 'zh-cn',
        'gl': 'cn'
    }
    try:
        response = requests.get(url, params=params)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"æœç´¢è¯·æ±‚å¤±è´¥: {e}")
        return {}

def extract_search_results(results, keyword):
    extracted_results = []
    organic_results = results.get('organic_results', [])
    for item in organic_results:
        result_info = {
            'keyword': keyword,
            'title': item.get('title', ''),
            'url': item.get('link', ''),
            'snippet': item.get('snippet', ''),
            'displayed_link': item.get('displayed_link', ''),
            'position': item.get('position', 0),
            'search_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        extracted_results.append(result_info)
    return extracted_results
```

```{python}
def search_keyword_with_pagination(keyword, api_key, total_results=20):
    all_results = []
    results_per_page = 10
    for start in range(0, total_results, results_per_page):
        remaining = total_results - start
        num_to_get = min(results_per_page, remaining)
        print(f"    è·å–ç¬¬ {start+1}-{start+num_to_get} æ¡ç»“æœ...")
        results = serpapi_search(keyword, api_key, num=num_to_get, start=start)
        if not results or 'organic_results' not in results:
            print(f"æ²¡æœ‰æ›´å¤šç»“æœ")
            break
        extracted = extract_search_results(results, keyword)
        all_results.extend(extracted)
        if len(extracted) < num_to_get:
            break
        time.sleep(DELAY_BETWEEN_REQUESTS)
    return all_results
```

```{python}
#| eval: false
def main():
    print("å¼€å§‹æœç´¢...")
    print(f"æœç´¢å…³é”®è¯æ•°é‡: {len(keywords)}")
    print(f"æ¯ä¸ªå…³é”®è¯è·å–ç»“æœæ•°: {RESULTS_PER_KEYWORD}")
    print("-" * 50)
    
    all_search_results = []
    for i, keyword in enumerate(keywords, 1):
        print(f"[{i}/{len(keywords)}] æœç´¢å…³é”®è¯: {keyword}")
        try:
            results = search_keyword_with_pagination(keyword, SERPAPI_KEY, RESULTS_PER_KEYWORD)
            all_search_results.extend(results)
            print(f"æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
            if i < len(keywords):
                print(f"ç­‰å¾… {DELAY_BETWEEN_KEYWORDS} ç§’...")
                time.sleep(DELAY_BETWEEN_KEYWORDS)
        except Exception as e:
            print(f"é”™è¯¯: {e}")
            continue
    print("-" * 50)
    print(f"æ€»å…±æ‰¾åˆ° {len(all_search_results)} ä¸ªç»“æœ")
    return all_search_results

search_results = main()
```

## Data Collection Process

The crawler iterates through each keyword, sends queries to SerpAPI, extracts content (including titles, URLs, and full texts), and saves them into a structured format for further processing. This automation ensures scalability and coverage.

```{python}
#| eval: false
if search_results:
    df_full = pd.DataFrame(search_results)
    df_unique = df_full.drop_duplicates(subset=['url'], keep='first')
    print(f"åŸå§‹ç»“æœ: {len(df_full)}ï¼Œå»é‡å: {len(df_unique)}")

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    df_simple = df_unique[['keyword', 'title', 'url', 'search_timestamp']].copy()

    # ä¿å­˜åˆ°æŒ‡å®š Dataset
    save_dir = os.path.expanduser("/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset")
    os.makedirs(save_dir, exist_ok=True)

    df_unique.to_csv(os.path.join(save_dir, f"search_results_full_{timestamp}.csv"), index=False, encoding='utf-8-sig')
    df_simple.to_csv(os.path.join(save_dir, f"search_results_simple_{timestamp}.csv"), index=False, encoding='utf-8-sig')

    with pd.ExcelWriter(os.path.join(save_dir, f"search_results_{timestamp}.xlsx"), engine='openpyxl') as writer:
        df_unique.to_excel(writer, sheet_name='å®Œæ•´æ•°æ®', index=False)
        df_simple.to_excel(writer, sheet_name='ç®€åŒ–æ•°æ®', index=False)

    print(f"æ–‡ä»¶ä¿å­˜è·¯å¾„: {save_dir}")

    print("\næ•°æ®é¢„è§ˆ:")
    print(df_simple.head())

    print("\næ¯ä¸ªå…³é”®è¯çš„ç»“æœæ•°é‡:")
    print(df_unique['keyword'].value_counts())

    globals()['search_data'] = df_unique
    globals()['search_data_simple'] = df_simple

    print("\nå˜é‡å·²åŠ è½½: search_data, search_data_simple")
else:
    print("æ²¡æœ‰æœç´¢ç»“æœ")
    globals()['search_data'] = pd.DataFrame()
    globals()['search_data_simple'] = pd.DataFrame()
```

## Data Transformation

Once raw JSON responses are collected, we convert them into a tabular format using `pandas`. This step prepares the dataset for text cleaning and analysis.
```{python}
#| eval: false
from bs4 import BeautifulSoup
import chardet

# æ–‡ä»¶è·¯å¾„è®¾ç½®
input_file = "/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset/search_results_20250706_192824.xlsx"
output_file = "/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset/search_results_with_content.xlsx"

# è¯»å–ç®€åŒ–æ•°æ®å·¥ä½œè¡¨
df = pd.read_excel(input_file, sheet_name="ç®€åŒ–æ•°æ®")

# æ­£æ–‡æå–å‡½æ•°
def fetch_article_content(url, timeout=10):
    try:
        response = requests.get(url, timeout=timeout, headers={"User-Agent": "Mozilla/5.0"})
        detected = chardet.detect(response.content)
        response.encoding = detected['encoding'] or 'utf-8'
        soup = BeautifulSoup(response.text, "html.parser")
        paragraphs = soup.find_all('p')
        content = '\n'.join(p.get_text(strip=True) for p in paragraphs)
        return content if len(content) > 50 else None
    except Exception:
        return None

# çˆ¬å–å†…å®¹
print("å¼€å§‹æŠ“å–æ­£æ–‡å†…å®¹...")
df["content"] = df["url"].apply(fetch_article_content)
print("æ­£æ–‡æŠ“å–å®Œæˆï¼Œå¼€å§‹ä¿å­˜æ–‡ä»¶...")

# ä¿å­˜ä¸ºæ–°çš„ Excel æ–‡ä»¶
df.to_excel(output_file, index=False)
print(f"æ–‡ä»¶å·²ä¿å­˜è‡³ï¼š{output_file}")
```

## **1.3 Data Clean**

```{python}
# è¾“å…¥è¾“å‡ºè·¯å¾„
input_file = "/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset/search_results_with_content.xlsx"
output_file = "/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset/search_results_cleaned.xlsx"

# åŠ è½½ Excel æ–‡ä»¶
df = pd.read_excel(input_file)

# å®šä¹‰æ— æ•ˆå†…å®¹å…³é”®è¯
ad_keywords = [
    "å…è´£å£°æ˜", "å¹¿å‘Šåˆä½œ", "è”ç³»ç®¡ç†å‘˜", "è¯·åœ¨å¾®ä¿¡ä¸­æ‰“å¼€", "æœ¬ç«™æ‰€æœ‰æ–‡ç« ",
    "æŠ±æ­‰", "é¡µé¢ä¸å­˜åœ¨", "å‡ºé”™", "404", "è¯·è¾“å…¥éªŒè¯ç ",
    "ç™»å½•æŸ¥çœ‹å…¨æ–‡", "Oops", "Something went wrong", "è®¿é—®å—é™"
]

# åˆ¤æ–­æ˜¯å¦ä¸ºæ— æ•ˆæ­£æ–‡
def is_invalid(text):
    if pd.isna(text):
        return True
    if len(text.strip()) < 100:
        return True
    if any(kw in text for kw in ad_keywords):
        return True
    return False

# æ·»åŠ æ ‡è®°åˆ—
df["invalid"] = df["content"].apply(is_invalid)

# ä¿ç•™æœ‰æ•ˆæ­£æ–‡å†…å®¹
df_cleaned = df[~df["invalid"]].drop(columns=["invalid"]).copy()

# ä¿å­˜æ¸…æ´—åçš„ç»“æœ
df_cleaned.to_excel(output_file, index=False)
print(f"æ¸…æ´—å®Œæˆï¼Œå·²ä¿å­˜ä¸ºï¼š{output_file}")
```

```{python}
# è¯»å– Excel æ–‡ä»¶
df = pd.read_excel("search_results_cleaned.xlsx")

# 1. æŸ¥çœ‹è¡Œæ•°å’Œåˆ—æ•°
print("è¡Œæ•° Ã— åˆ—æ•°:", df.shape)

# 2. æŸ¥çœ‹åˆ—åå’Œç±»å‹
print("\nåˆ—åä¸æ•°æ®ç±»å‹:")
print(df.dtypes)

# 3. å¿«é€Ÿæ¦‚è§ˆæ¯åˆ—çš„å‰å‡ è¡Œï¼ˆç»“æ„ + å€¼ï¼‰
print("\næ ·æœ¬é¢„è§ˆ:")
print(df.head())

# 4. ç¼ºå¤±å€¼ç»Ÿè®¡ï¼ˆNA å€¼ï¼‰
print("\nç¼ºå¤±å€¼ç»Ÿè®¡:")
print(df.isna().sum())
```

```{python}
df_cleaned = df[~df["invalid"] & df["content"].notna()].copy()
df_cleaned.to_excel("search_results_cleaned.xlsx", index=False)
```

## Text Cleaning

The raw text contains punctuation, redundant whitespace, and occasionally malformed encoding. We apply basic cleaning to ensure the corpus is suitable for keyword extraction and topic modeling.

```{python}
import re
from collections import Counter
import jieba

# 1. åŠ è½½æ¸…æ´—åçš„æ–‡ç« æ•°æ®
df = pd.read_excel("Dataset/search_results_cleaned.xlsx")
df = df[df["content"].notna() & (df["content"].str.strip() != "")]

# 2. è¯»å–åœç”¨è¯è¡¨
with open("Dataset/cn_stopwords.txt", "r", encoding="utf-8") as f:
    stopwords = set([line.strip() for line in f])

# 3. æ¸…æ´—å‡½æ•°
def clean_text(s):
    s = re.sub(r'<.*?>', '', s)  # HTMLæ ‡ç­¾
    s = re.sub(r'[a-zA-Z]+', '', s)  # è‹±æ–‡
    s = re.sub(r'[\d\-:/\.å¹´æœˆæ—¥\s]+', '', s)  # æ•°å­—ä¸æ—¥æœŸ
    s = re.sub(r'[\u0000-\u007F]+', '', s)  # ASCIIç¬¦å·
    return s.strip()

# 4. åˆ†å¥ + æ¸…æ´— + åˆ†è¯ + å»åœç”¨è¯
words = []
for content in df["content"]:
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', content)
    for s in sentences:
        s_clean = clean_text(s)
        if len(s_clean) >= 5:
            segs = jieba.cut(s_clean)
            words += [w for w in segs if len(w) > 1 and w not in stopwords and re.match(r'[\u4e00-\u9fff]+', w)]

# 5. è¯é¢‘ç»Ÿè®¡
word_freq = Counter(words)
df_freq = pd.DataFrame(word_freq.most_common(100), columns=["Word", "Frequency"])

# 6. ä¿å­˜åˆ°æ–‡ä»¶
df_freq.to_excel("Dataset/word_frequency_cleaned_with_stopwords.xlsx", index=False)

# 7. æ‰“å°ç»“æœ
print(df_freq)
```

## Keyword Frequency

We extract high-frequency terms to detect common concerns and motivations. This analysis surfaces dominant themes such as â€œwealth managementâ€, â€œimmigrationâ€, and â€œSingapore advantageâ€.

# 2 **Text Analysis**

## **2.1 Word Cloud**

## Visualizing High-Frequency Terms

The following word cloud ord highlights the most common terms across all retrieved content, offering a visual snapshot of what matters most to the audience.

```{python}
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# 1. åŠ è½½è¯é¢‘æ•°æ®
df_freq = pd.read_excel("Dataset/word_frequency_cleaned_with_stopwords.xlsx")

# 2. è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
freq_dict = dict(zip(df_freq["Word"], df_freq["Frequency"]))

# 3. åˆ›å»ºè¯äº‘å¯¹è±¡
wc = WordCloud(
    font_path="/System/Library/Fonts/STHeiti Medium.ttc",  # æ›¿æ¢ä¸ºä½ æœ¬åœ°æ”¯æŒä¸­æ–‡çš„å­—ä½“è·¯å¾„
    background_color="white",
    width=1000,
    height=700,
    max_words=200
).generate_from_frequencies(freq_dict)

# 4. å¯è§†åŒ–è¯äº‘
plt.figure(figsize=(12, 8))
plt.imshow(wc, interpolation="bilinear")
plt.axis("off")
plt.title("WordCloud", fontsize=18)
plt.subplots_adjust(top=0.85) 
plt.show()
```

::: callout-important
## Insights from the Word Cloud

The word cloud visually reinforces the central motivations driving Chinese high-net-worth individuals (HNWIs) to move their assets to Singapore.

- High-frequency terms such as **â€œè´¢å¯Œç®¡ç†â€** (wealth management), **â€œèµ„äº§é…ç½®â€** (asset allocation), and **â€œç¨åŠ¡ä¼˜åŒ–â€** (tax optimization) directly align with the motivations outlined in our initial flowchart.
- Keywords like **â€œå®¶æ—ä¿¡æ‰˜â€** (family trust), **â€œç¦»å²¸è´¦æˆ·â€** (offshore accounts), and **â€œå­å¥³æ•™è‚²â€** (childrenâ€™s education) indicate a strong emphasis on legacy planning, asset protection, and long-term family welfare.

**Conclusion**: These themes confirm that Singapore is not just attractive for its financial incentivesâ€”but as a comprehensive hub for multi-generational wealth security and elite lifestyle planning.
:::

## **2.2 Sentiment Analysis**

```{python}
from snownlp import SnowNLP

# è¯»å–å·²æ¸…æ´—çš„æ•°æ®
df = pd.read_excel("Dataset/search_results_cleaned.xlsx")

# è¿‡æ»¤æ‰æ— æ­£æ–‡
df = df[df["content"].notna() & (df["content"].str.len() > 30)].copy()

# æƒ…æ„Ÿåˆ†æå‡½æ•°ï¼ˆè¿”å›å€¼åœ¨ 0ï½1 ä¹‹é—´ï¼Œ1 è¶Šç§¯æï¼‰
def get_sentiment(text):
    try:
        return SnowNLP(text).sentiments
    except:
        return None

# æ·»åŠ æƒ…æ„Ÿè¯„åˆ†åˆ—
df["sentiment_score"] = df["content"].apply(get_sentiment)

# åˆ†ç±»æ ‡ç­¾ï¼šå¤§äº 0.6 ä¸ºæ­£é¢ï¼Œå°äº 0.4 ä¸ºè´Ÿé¢ï¼Œå…¶ä½™ä¸ºä¸­æ€§
def classify(score):
    if score is None:
        return "Unknown"
    elif score > 0.6:
        return "Positive"
    elif score < 0.4:
        return "Negative"
    else:
        return "Neutral"

df["sentiment_label"] = df["sentiment_score"].apply(classify)

# ä¿å­˜ç»“æœ
df.to_excel("Dataset/search_results_sentiment.xlsx", index=False)

# æŸ¥çœ‹ç»Ÿè®¡
print(df["sentiment_label"].value_counts())
```

```{python}
import matplotlib.pyplot as plt

# Count sentiment
sentiment_counts = df["sentiment_label"].value_counts()

# Set labels and colors
labels = sentiment_counts.index.tolist()
colors = ['#4CAF50' if label == 'Positive' else '#F44336' for label in labels]  

# Define explode to slightly offset each slice
explode = [0.05] * len(labels)

# Create figure
fig, ax = plt.subplots(figsize=(7, 7), facecolor='white')
wedges, texts, autotexts = ax.pie(
    sentiment_counts,
    labels=labels,
    autopct='%1.1f%%',
    startangle=140,
    colors=colors,
    explode=explode,
    wedgeprops={'edgecolor': 'white', 'linewidth': 2},
    textprops={'fontsize': 13}
)

# Labels and percentages
for text in autotexts:
    text.set_color('white')
    text.set_fontweight('bold')

# Add title with padding
plt.title("Sentiment Composition", fontsize=18, weight='bold', pad=20)

# Equal aspect ratio ensures pie is circular
ax.axis('equal')

plt.tight_layout()
plt.show()
```

::: callout-important
## Insights from Sentiment Analysis

The sentiment analysis reveals a **mixed but insightful emotional tone** surrounding discussions of wealth migration to Singapore.

- A significant portion of posts are **positive**, reflecting appreciation for Singaporeâ€™s legal stability, tax efficiency, and quality of life.
- Negative sentiments mainly revolve around **concerns about regulatory changes**, **barriers to entry**, or **uncertainty around immigration pathways**.
- Neutral discussions tend to be **informational or comparative**, providing objective assessments of different offshore options.

**Conclusion**: These findings suggest that while Singapore is generally viewed favorably, there remains a need to **address misconceptions** and **reduce friction in communication** when engaging with HNW prospects.
:::

```{python}
import matplotlib.pyplot as plt

plt.rcParams['font.family'] = 'Arial Unicode MS'  # æ›´é€šç”¨çš„ä¸­è‹±æ··æ’å­—ä½“
plt.rcParams['axes.unicode_minus'] = False

# åˆ†ç»„ï¼šç»Ÿè®¡æ¯ä¸ª keyword ä¸‹çš„æ­£è´Ÿé¢æ•°é‡
sentiment_by_keyword = df.groupby(['keyword', 'sentiment_label']).size().unstack(fill_value=0)

# æ‰“å°æ£€æŸ¥
print(sentiment_by_keyword)

# ç»˜å›¾ï¼šå †å æŸ±çŠ¶å›¾
sentiment_by_keyword.plot(kind='bar', stacked=True, figsize=(12, 6), color=["#1f77b4", "#ff7f0e"])  

# è®¾ç½®æ ‡é¢˜å’Œè½´æ ‡ç­¾
plt.title("å…³é”®è¯æƒ…æ„Ÿæ„æˆ", fontsize=16)
plt.xlabel("å…³é”®è¯", fontsize=12)
plt.ylabel("æ–‡ç« æ•°é‡", fontsize=12)

# æ—‹è½¬ x è½´æ–‡å­—é˜²æ­¢é‡å 
plt.xticks(rotation=45, ha='right')

# è‡ªåŠ¨å¸ƒå±€
plt.tight_layout()

# æ˜¾ç¤ºå›¾åƒ
plt.show()
```

::: callout-note
## Next Step: Deep Dive into Negative Sentiments

To strengthen client engagement and tailor advisory strategies, further analysis should focus on **uncovering the root causes behind negative or hesitant views**. Recommended steps include:

- **Thematic Clustering**: Group negative posts into key concernsâ€”e.g., â€œregulatory fearsâ€, â€œimmigration complexityâ€, â€œtrust issuesâ€.
- **Temporal Sentiment Tracking**: Detect if negative sentiment spikes after specific events (e.g., new financial regulations in China).

By understanding the friction points in sentiment, Life Inc can refine positioning, improve client confidence, and remove silent blockers in the decision-making journey.
:::

## **2.3 Network Analysis**

```{python}
from itertools import combinations
import networkx as nx
import matplotlib.pyplot as plt

# ç”¨äºå­˜å‚¨æ¯å¥è¯çš„å…³é”®è¯
keyword_sentences = []

for content in df["content"]:
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', content)
    for s in sentences:
        s_clean = clean_text(s)
        if len(s_clean) >= 5:
            segs = jieba.cut(s_clean)
            word_list = [w for w in segs if len(w) > 1 and w not in stopwords and re.match(r'[\u4e00-\u9fff]+', w)]
            keyword_sentences.append(word_list)
```

```{python}
top_100_words = set(df_freq["Word"])
co_occurrence = Counter()

# åªç»Ÿè®¡ top100 è¯ä¹‹é—´çš„å…±ç°
for word_list in keyword_sentences:
    words_in_top100 = [w for w in word_list if w in top_100_words]
    for pair in combinations(set(words_in_top100), 2):  # set å»é‡
        co_occurrence[tuple(sorted(pair))] += 1
```

```{python}
import matplotlib.font_manager as fm
for font in fm.findSystemFonts(fontpaths=None, fontext='ttf'):
    if 'PingFang' in font or 'Arial' in font or 'Hei' in font:
        print(font)
```

```{python}
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import networkx as nx

# ä¸­æ–‡å­—ä½“è®¾ç½®
my_font = fm.FontProperties(fname="/System/Library/Fonts/STHeiti Medium.ttc")
plt.rcParams['font.family'] = my_font.get_name()
plt.rcParams['axes.unicode_minus'] = False

# æ„å»ºå®Œæ•´å›¾
G_full = nx.Graph()
for (w1, w2), freq in co_occurrence.items():
    if freq >= 3:
        G_full.add_edge(w1, w2, weight=freq)

# æŒ‰åº¦æ•°æ’åºï¼Œè·å–å‰20ä¸ªé«˜è¿æ¥åº¦çš„èŠ‚ç‚¹
top_nodes = sorted(G_full.degree, key=lambda x: x[1], reverse=True)[:10]
top_node_names = set(node for node, _ in top_nodes)

# åˆ›å»ºå­å›¾ï¼šåªåŒ…å«å‰20ä¸ªèŠ‚ç‚¹åŠå…¶ç›¸è¿è¾¹
G = G_full.subgraph(top_node_names).copy()

# å¯è§†åŒ–å‡†å¤‡
pos = nx.spring_layout(G, k=0.8, seed=42)
degrees = dict(G.degree())
node_colors = [degrees[node] for node in G.nodes()]
node_sizes = [v * 40 for v in degrees.values()]  # é€‚åº¦æ”¾å¤§

# ç»˜å›¾
plt.figure(figsize=(12, 10))
nx.draw_networkx_nodes(G, pos,
                       node_size=node_sizes,
                       node_color=node_colors,
                       cmap=plt.cm.Blues,
                       edgecolors='white',
                       linewidths=0.8)

nx.draw_networkx_edges(G, pos,
                       width=[d['weight'] * 0.3 for _, _, d in G.edges(data=True)],
                       edge_color='gray',
                       alpha=0.5)

nx.draw_networkx_labels(G, pos,
                        font_size=12,
                        font_family=my_font.get_name())

plt.title("Top 20 ä¸­æ–‡å…³é”®è¯å…±ç°ç½‘ç»œ", fontsize=18, fontproperties=my_font, pad=20, weight='bold')
plt.axis('off')
plt.tight_layout()
plt.show()
```

::: callout-important
## Insights from Co-occurrence Keyword Network

This network reveals how important themes surrounding wealth migration to Singapore **co-appear and reinforce each other** in authentic online discussions.

- Words like **â€œèµ„äº§é…ç½®â€** (asset allocation), **â€œæ–°åŠ å¡â€**, and **â€œå®¶æ—ä¿¡æ‰˜â€** frequently co-occur, forming dense hubs of strategic planning.
- The structure suggests that users donâ€™t speak of single motivations in isolation â€” tax planning, family trust, and regulatory considerations are often bundled in the same conversation.
- High-degree nodes like **â€œCRSâ€**, **â€œç§»æ°‘â€**, and **â€œç¦»å²¸è´¦æˆ·â€** serve as bridges across different topic communities.

**Conclusion**: These patterns demonstrate that Chinese HNWIs approach offshore wealth management as a **multifaceted decision process**. Singapore is appealing not for a single advantage, but for its ability to address **multiple interconnected concerns** simultaneously.
:::

```{python}
# è®¡ç®—èŠ‚ç‚¹çš„åº¦ä¸­å¿ƒæ€§
degree_centrality = nx.degree_centrality(G)

# æŒ‰ä¸­å¿ƒæ€§æ’åºï¼Œå–å‰10ä¸ªå…³é”®è¯
top_nodes = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]

# æ‰“å°ç»“æœ
print("Top 10 å…³é”®è¯:")
for word, score in top_nodes:
    print(f"{word}: {score:.3f}")
```

```{python}
import community as community_louvain  # pip install python-louvain
import matplotlib.cm as cm

# ç¤¾åŒºåˆ’åˆ†
partition = community_louvain.best_partition(G)

# è®¾ç½®é¢œè‰²æ˜ å°„
size = float(len(set(partition.values())))
pos = nx.spring_layout(G, k=0.5, seed=42)
colors = [cm.tab20(i / size) for i in partition.values()]

# ç»˜å›¾
plt.figure(figsize=(14, 12))
nx.draw_networkx_nodes(G, pos, node_size=500, node_color=colors, alpha=0.8)
nx.draw_networkx_edges(G, pos, width=0.5, alpha=0.3)
nx.draw_networkx_labels(G, pos, font_size=10, font_family='Arial Unicode MS')
plt.title("å…³é”®è¯å…±ç°ç½‘ç»œä¸­çš„è¯é¢˜ç¤¾åŒº")
plt.axis("off")
plt.tight_layout()
plt.show()
```
::: callout-important
## Insights from Color-Labeled Keyword Communities

The co-occurrence network, enhanced with semantic color tagging, reveals four distinct communities that structure online discussions around wealth migration to Singapore:

- ğŸŒ• **Community 1 (Brown)** focuses on **trust and legal structures**, with keywords like â€œtrustâ€, â€œassetsâ€, â€œfundâ€, and â€œlegalâ€ â€” underscoring the importance of compliant, long-term wealth management.
- ğŸŸ¢ **Community 2 (Green)** centers around **client advisory and immigration services**, showing that service quality and immigration logistics are major touchpoints for Chinese HNWIs.
- ğŸ”µ **Community 3 (Blue)** is tied to **cross-border financial assets and tax implications**, with mentions of â€œHong Kongâ€, â€œtaxâ€, and â€œnet worthâ€ â€” pointing to regional diversification strategies.
- âš« **Community 4 (Gray)** contains more **generic or connective language**, serving as linguistic bridges in user conversations.

**Conclusion**: This structured clustering confirms that Chinese HNWIs are not driven by a single reason to move assets offshore. Instead, their motivations form **a multi-layered ecosystem** â€” blending financial safety, global mobility, tax planning, and trusted advisory. Singapore is uniquely positioned as **a hub that satisfies all these needs simultaneously**.
:::



# 3 **Conclusion**

## Final Takeaways: Why Do Chinese HNWIs Move Their Wealth to Singapore?

Through the combined use of structured flowchart thinking and real-world data collection using a customized web crawler, this study provides a grounded answer to our core question.

### Key Findings:
- **Wealth Management & Asset Protection**: Terms like â€œè´¢å¯Œç®¡ç†â€ (wealth management) and â€œèµ„äº§é…ç½®â€ (asset allocation) appeared frequently, highlighting the strong demand for stable and diversified wealth strategies.
- **Tax Optimization**: Frequent mentions of â€œç¨åŠ¡ä¼˜åŒ–â€ and â€œCRSé¿ç¨â€ reflect concerns over rising tax scrutiny in China and Singaporeâ€™s more favorable policies.
- **Educational Planning & Family Legacy**: Phrases such as â€œå­å¥³æ•™è‚²â€ and â€œå®¶æ—ä¿¡æ‰˜â€ indicate that many HNWIs are motivated by long-term family goals rather than short-term returns.
- **Political and Legal Stability**: Although more subtle, the preference for Singaporeâ€™s legal infrastructure and business environment emerged from context-rich discussions.

### Why This Method Worked:
By using a spider to extract public sentiment directly from platforms like Zhihu and Xueqiu, we bypassed the sampling bias of traditional surveys and captured more nuanced and emotionally honest reasons behind wealth relocation behavior.

### Strategic Implications:
- **For Wealth Advisors**: Focus messaging on intergenerational planning and offshore security.
- **For Life Inc**: Develop advisory frameworks that align with education, legacy, and compliance-oriented strategies.
- **For Survey Design**: The themes identified here form the foundation of more targeted and relevant survey questions in the next phase of this project.

---

In conclusion, the answer is not singularâ€”but multi-dimensional. Singapore is attractive to Chinese HNWIs not only for its tax and legal advantages, but because it offers stability, safety, and long-term opportunities for families and wealth.