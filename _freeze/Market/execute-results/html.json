{
  "hash": "57030fcda211f9888f62580ea4427c87",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Makert Survey\"\nauthor: \"Zou Jiaxun\"\nexecute: \n  eval: true\n  echo: true\n  warning: false\n  freeze: true\n  cache: true\nformat:\n  html:\n    code-fold: true\n    code-summary: \"Click to view code\"\njupyter: python3  \n---\n\n\n\n\n# 1 **Overview**\n\n## 1.1 **Background**\n\nTo effectively organize and communicate the complex thought process behind why Chinese HNW individuals move their money to Singapore, I created Flowchart. This flowchart helps to:\n\n-   Visually map the key motivations (such as wealth management, tax benefits, lifestyle advantages, and legacy planning) that drive this financial movement.\n\n-   Clarify the relationships between client needs, Singaporeâ€™s value proposition, and the strategic advisory angles we can take.\n\n-   Support client-facing engagement by simplifying the narrative for Life Inc advisors to present during weekly meetings or consultations.\n\n-   Guide my survey design and presentation content, ensuring all relevant themesâ€”such as trends, preferences, and asset relocation methodsâ€”are covered logically and comprehensively.\n\nIt acts as both a research framework and a client communication tool to align our understanding and offerings with the target marketâ€™s aspirations.\n\n![](Picture/Flowchart1.png){fig-align=\"center\"}\n\n## **1.2 Data Collect**\n\nTo support the research in Flowchart, I developed a custom web crawler to systematically collect qualitative and contextual data from Chinese-language sources such as forums (e.g., Zhihu, Xueqiu), financial news sites (e.g., Eastern Wealth), and financial blogs. These platforms offer authentic discussions and opinions from Chinese citizens regarding overseas wealth movementâ€”especially to Singapore.\n\nMy spider focused on keywords that map directly to the key branches of the flowchart, such as:\n\n-   â€œæ–°åŠ å¡ è´¢å¯Œç®¡ç†â€ (Singapore wealth management)\n\n-   â€œä¸­å›½äººç§»æ°‘æ–°åŠ å¡â€ (Chinese migration to Singapore)\n\n-   â€œç¨åŠ¡ä¼˜åŒ– æ–°åŠ å¡â€ (Tax optimization Singapore)\n\n-   â€œèµ„äº§é…ç½® æµ·å¤–â€ (Offshore asset allocation)\n\n::: callout-note\n## Why choose spider instead of survey\n\n-   **Understand Real Conversations First**: The spider captures what people are already saying online, so I donâ€™t assume or guess their opinions.\n-   **Gather Rich and Honest Insights**: Online posts reveal detailed reasons, emotions, and comparisons that people may not share in surveys.\n-   **Reach More People Easily**: Itâ€™s hard to get high-net-worth individuals to answer surveys, but the spider can access public content instantly.\n-   **Build a Better Survey Later**: By analyzing online themes first, I can design a more focused and relevant survey based on real concerns.\n:::\n\n::: {#4208403e .cell execution_count=1}\n``` {.python .cell-code}\nimport requests\nimport json\nimport time\nimport pandas as pd\nfrom datetime import datetime\nimport os\nimport openpyxl\n\n# ç”¨æˆ·é…ç½®åŒºåŸŸ\nSERPAPI_KEY = \"84c798a8d45d1e7cae0b18df778ac06bf2c6169f0249e40756aea0b9d6cd4749\"  \nRESULTS_PER_KEYWORD = 20\nDELAY_BETWEEN_REQUESTS = 1\nDELAY_BETWEEN_KEYWORDS = 2\n```\n:::\n\n\n::: {#bd335e46 .cell execution_count=2}\n``` {.python .cell-code}\nkeywords = [\n    \"ä¸­å›½é«˜å‡€å€¼äººå£« æ–°åŠ å¡\",\n    \"ä¸­å›½å¯Œäºº èµ„äº§é…ç½® æ–°åŠ å¡\", \n    \"ä¸­å›½å¯Œè±ª ä¸ºä»€ä¹ˆç§»æ°‘æ–°åŠ å¡\",\n    \"ä¸­å›½é«˜å‡€å€¼å®¢æˆ· ç¦»å²¸è´¦æˆ·\",\n    \"æ–°åŠ å¡ CRS é¿ç¨\",\n    \"ä¸­å›½ å®¶æ—ä¿¡æ‰˜ æ–°åŠ å¡\",\n    \"ä¸­å›½é«˜å‡€å€¼äººå£« å­å¥³æ•™è‚² æ–°åŠ å¡\",\n    \"ä¸­å›½ç§»æ°‘æ–°åŠ å¡ è´¢å¯Œç®¡ç†\"\n]\n```\n:::\n\n\n::: {#84bf7aa5 .cell execution_count=3}\n``` {.python .cell-code}\ndef serpapi_search(query, api_key, num=20, start=0):\n    url = \"https://serpapi.com/search\"\n    params = {\n        'q': query,\n        'api_key': api_key,\n        'engine': 'google',\n        'num': min(num, 100),\n        'start': start,\n        'hl': 'zh-cn',\n        'gl': 'cn'\n    }\n    try:\n        response = requests.get(url, params=params)\n        response.raise_for_status()\n        return response.json()\n    except requests.exceptions.RequestException as e:\n        print(f\"æœç´¢è¯·æ±‚å¤±è´¥: {e}\")\n        return {}\n\ndef extract_search_results(results, keyword):\n    extracted_results = []\n    organic_results = results.get('organic_results', [])\n    for item in organic_results:\n        result_info = {\n            'keyword': keyword,\n            'title': item.get('title', ''),\n            'url': item.get('link', ''),\n            'snippet': item.get('snippet', ''),\n            'displayed_link': item.get('displayed_link', ''),\n            'position': item.get('position', 0),\n            'search_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        }\n        extracted_results.append(result_info)\n    return extracted_results\n```\n:::\n\n\n::: {#27494f80 .cell execution_count=4}\n``` {.python .cell-code}\ndef search_keyword_with_pagination(keyword, api_key, total_results=20):\n    all_results = []\n    results_per_page = 10\n    for start in range(0, total_results, results_per_page):\n        remaining = total_results - start\n        num_to_get = min(results_per_page, remaining)\n        print(f\"    è·å–ç¬¬ {start+1}-{start+num_to_get} æ¡ç»“æœ...\")\n        results = serpapi_search(keyword, api_key, num=num_to_get, start=start)\n        if not results or 'organic_results' not in results:\n            print(f\"æ²¡æœ‰æ›´å¤šç»“æœ\")\n            break\n        extracted = extract_search_results(results, keyword)\n        all_results.extend(extracted)\n        if len(extracted) < num_to_get:\n            break\n        time.sleep(DELAY_BETWEEN_REQUESTS)\n    return all_results\n```\n:::\n\n\n::: {#c9fc74d4 .cell execution_count=5}\n``` {.python .cell-code}\ndef main():\n    print(\"å¼€å§‹æœç´¢...\")\n    print(f\"æœç´¢å…³é”®è¯æ•°é‡: {len(keywords)}\")\n    print(f\"æ¯ä¸ªå…³é”®è¯è·å–ç»“æœæ•°: {RESULTS_PER_KEYWORD}\")\n    print(\"-\" * 50)\n    \n    all_search_results = []\n    for i, keyword in enumerate(keywords, 1):\n        print(f\"[{i}/{len(keywords)}] æœç´¢å…³é”®è¯: {keyword}\")\n        try:\n            results = search_keyword_with_pagination(keyword, SERPAPI_KEY, RESULTS_PER_KEYWORD)\n            all_search_results.extend(results)\n            print(f\"æ‰¾åˆ° {len(results)} ä¸ªç»“æœ\")\n            if i < len(keywords):\n                print(f\"ç­‰å¾… {DELAY_BETWEEN_KEYWORDS} ç§’...\")\n                time.sleep(DELAY_BETWEEN_KEYWORDS)\n        except Exception as e:\n            print(f\"é”™è¯¯: {e}\")\n            continue\n    print(\"-\" * 50)\n    print(f\"æ€»å…±æ‰¾åˆ° {len(all_search_results)} ä¸ªç»“æœ\")\n    return all_search_results\n\nsearch_results = main()\n```\n:::\n\n\n::: {#fc54e904 .cell execution_count=6}\n``` {.python .cell-code}\nif search_results:\n    df_full = pd.DataFrame(search_results)\n    df_unique = df_full.drop_duplicates(subset=['url'], keep='first')\n    print(f\"åŸå§‹ç»“æœ: {len(df_full)}ï¼Œå»é‡å: {len(df_unique)}\")\n\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    df_simple = df_unique[['keyword', 'title', 'url', 'search_timestamp']].copy()\n\n    # ä¿å­˜åˆ°æŒ‡å®š Dataset\n    save_dir = os.path.expanduser(\"/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset\")\n    os.makedirs(save_dir, exist_ok=True)\n\n    df_unique.to_csv(os.path.join(save_dir, f\"search_results_full_{timestamp}.csv\"), index=False, encoding='utf-8-sig')\n    df_simple.to_csv(os.path.join(save_dir, f\"search_results_simple_{timestamp}.csv\"), index=False, encoding='utf-8-sig')\n\n    with pd.ExcelWriter(os.path.join(save_dir, f\"search_results_{timestamp}.xlsx\"), engine='openpyxl') as writer:\n        df_unique.to_excel(writer, sheet_name='å®Œæ•´æ•°æ®', index=False)\n        df_simple.to_excel(writer, sheet_name='ç®€åŒ–æ•°æ®', index=False)\n\n    print(f\"æ–‡ä»¶ä¿å­˜è·¯å¾„: {save_dir}\")\n\n    print(\"\\næ•°æ®é¢„è§ˆ:\")\n    print(df_simple.head())\n\n    print(\"\\næ¯ä¸ªå…³é”®è¯çš„ç»“æœæ•°é‡:\")\n    print(df_unique['keyword'].value_counts())\n\n    globals()['search_data'] = df_unique\n    globals()['search_data_simple'] = df_simple\n\n    print(\"\\nå˜é‡å·²åŠ è½½: search_data, search_data_simple\")\nelse:\n    print(\"æ²¡æœ‰æœç´¢ç»“æœ\")\n    globals()['search_data'] = pd.DataFrame()\n    globals()['search_data_simple'] = pd.DataFrame()\n```\n:::\n\n\n::: {#c929b482 .cell execution_count=7}\n``` {.python .cell-code}\nfrom bs4 import BeautifulSoup\nimport chardet\n\n# æ–‡ä»¶è·¯å¾„è®¾ç½®\ninput_file = \"/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset/search_results_20250706_192824.xlsx\"\noutput_file = \"/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset/search_results_with_content.xlsx\"\n\n# è¯»å–ç®€åŒ–æ•°æ®å·¥ä½œè¡¨\ndf = pd.read_excel(input_file, sheet_name=\"ç®€åŒ–æ•°æ®\")\n\n# æ­£æ–‡æå–å‡½æ•°\ndef fetch_article_content(url, timeout=10):\n    try:\n        response = requests.get(url, timeout=timeout, headers={\"User-Agent\": \"Mozilla/5.0\"})\n        detected = chardet.detect(response.content)\n        response.encoding = detected['encoding'] or 'utf-8'\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        paragraphs = soup.find_all('p')\n        content = '\\n'.join(p.get_text(strip=True) for p in paragraphs)\n        return content if len(content) > 50 else None\n    except Exception:\n        return None\n\n# çˆ¬å–å†…å®¹\nprint(\"å¼€å§‹æŠ“å–æ­£æ–‡å†…å®¹...\")\ndf[\"content\"] = df[\"url\"].apply(fetch_article_content)\nprint(\"æ­£æ–‡æŠ“å–å®Œæˆï¼Œå¼€å§‹ä¿å­˜æ–‡ä»¶...\")\n\n# ä¿å­˜ä¸ºæ–°çš„ Excel æ–‡ä»¶\ndf.to_excel(output_file, index=False)\nprint(f\"æ–‡ä»¶å·²ä¿å­˜è‡³ï¼š{output_file}\")\n```\n:::\n\n\n## **1.3 Data Clean**\n\n::: {#62d8dc0f .cell execution_count=8}\n``` {.python .cell-code}\n# è¾“å…¥è¾“å‡ºè·¯å¾„\ninput_file = \"/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset/search_results_with_content.xlsx\"\noutput_file = \"/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset/search_results_cleaned.xlsx\"\n\n# åŠ è½½ Excel æ–‡ä»¶\ndf = pd.read_excel(input_file)\n\n# å®šä¹‰æ— æ•ˆå†…å®¹å…³é”®è¯\nad_keywords = [\n    \"å…è´£å£°æ˜\", \"å¹¿å‘Šåˆä½œ\", \"è”ç³»ç®¡ç†å‘˜\", \"è¯·åœ¨å¾®ä¿¡ä¸­æ‰“å¼€\", \"æœ¬ç«™æ‰€æœ‰æ–‡ç« \",\n    \"æŠ±æ­‰\", \"é¡µé¢ä¸å­˜åœ¨\", \"å‡ºé”™\", \"404\", \"è¯·è¾“å…¥éªŒè¯ç \",\n    \"ç™»å½•æŸ¥çœ‹å…¨æ–‡\", \"Oops\", \"Something went wrong\", \"è®¿é—®å—é™\"\n]\n\n# åˆ¤æ–­æ˜¯å¦ä¸ºæ— æ•ˆæ­£æ–‡\ndef is_invalid(text):\n    if pd.isna(text):\n        return True\n    if len(text.strip()) < 100:\n        return True\n    if any(kw in text for kw in ad_keywords):\n        return True\n    return False\n\n# æ·»åŠ æ ‡è®°åˆ—\ndf[\"invalid\"] = df[\"content\"].apply(is_invalid)\n\n# ä¿ç•™æœ‰æ•ˆæ­£æ–‡å†…å®¹\ndf_cleaned = df[~df[\"invalid\"]].drop(columns=[\"invalid\"]).copy()\n\n# ä¿å­˜æ¸…æ´—åçš„ç»“æœ\ndf_cleaned.to_excel(output_file, index=False)\nprint(f\"æ¸…æ´—å®Œæˆï¼Œå·²ä¿å­˜ä¸ºï¼š{output_file}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\næ¸…æ´—å®Œæˆï¼Œå·²ä¿å­˜ä¸ºï¼š/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset/search_results_cleaned.xlsx\n```\n:::\n:::\n\n\n::: {#0e8ccd5d .cell execution_count=9}\n``` {.python .cell-code}\n# è¯»å– Excel æ–‡ä»¶\ndf = pd.read_excel(\"search_results_cleaned.xlsx\")\n\n# 1. æŸ¥çœ‹è¡Œæ•°å’Œåˆ—æ•°\nprint(\"è¡Œæ•° Ã— åˆ—æ•°:\", df.shape)\n\n# 2. æŸ¥çœ‹åˆ—åå’Œç±»å‹\nprint(\"\\nåˆ—åä¸æ•°æ®ç±»å‹:\")\nprint(df.dtypes)\n\n# 3. å¿«é€Ÿæ¦‚è§ˆæ¯åˆ—çš„å‰å‡ è¡Œï¼ˆç»“æ„ + å€¼ï¼‰\nprint(\"\\næ ·æœ¬é¢„è§ˆ:\")\nprint(df.head())\n\n# 4. ç¼ºå¤±å€¼ç»Ÿè®¡ï¼ˆNA å€¼ï¼‰\nprint(\"\\nâš ç¼ºå¤±å€¼ç»Ÿè®¡:\")\nprint(df.isna().sum())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nè¡Œæ•° Ã— åˆ—æ•°: (90, 6)\n\nåˆ—åä¸æ•°æ®ç±»å‹:\nkeyword             object\ntitle               object\nurl                 object\nsearch_timestamp    object\ncontent             object\ninvalid               bool\ndtype: object\n\næ ·æœ¬é¢„è§ˆ:\n       keyword                           title  \\\n0  ä¸­å›½é«˜å‡€å€¼äººå£« æ–°åŠ å¡          æ–°åŠ å¡é‡‘èæœºåˆ¶ç¨³å®šå¸å¼•æœ€å¤šé«˜å‡€å€¼äººå£«è€ƒè™‘ç§»å±…   \n1  ä¸­å›½é«˜å‡€å€¼äººå£« æ–°åŠ å¡          æˆ‘å›½è¶…é«˜å‡€å€¼äººç¾¤è®¾ç«‹å®¶æ—åŠå…¬å®¤ç°çŠ¶åˆ†æä¸åº”å¯¹   \n2  ä¸­å›½é«˜å‡€å€¼äººå£« æ–°åŠ å¡  ä¸­å›½é«˜å‡€å€¼äººå£«åœ¨æ–°åŠ å¡è®¾ç«‹å®¶æ—åŠå…¬å®¤å’ŒæŠ•èµ„åˆåˆ›ä¼ä¸šçš„ ...   \n3  ä¸­å›½é«˜å‡€å€¼äººå£« æ–°åŠ å¡        é«˜å‡€å€¼äººå£«ä¸ºä½•çº·çº·é€‰æ‹©æ–°åŠ å¡ï¼Ÿ_ç§»æ°‘_æ•™è‚²_å·¥ä½œ   \n4  ä¸­å›½é«˜å‡€å€¼äººå£« æ–°åŠ å¡    èšç„¦å®¶åŠ| äºšæ´²è¶…é«˜å‡€å€¼äººç¾¤å¢é•¿æˆ–å°†æ¨åŠ¨å®¶æ—åŠå…¬å®¤çš„å‘å±•   \n\n                                                 url     search_timestamp  \\\n0  https://www.zaobao.com/finance/singapore/story...  2025-07-06 19:19:49   \n1  http://cel.cn/List/FullText?articleId=d37c148c...  2025-07-06 19:19:49   \n2  https://fargowealth.com/en/home/cfsj/cfsj_deta...  2025-07-06 19:19:49   \n3         https://www.sohu.com/a/843292362_121963266  2025-07-06 19:19:49   \n4  https://www.bloombergchina.com/blog/asias-ultr...  2025-07-06 19:19:49   \n\n                                             content  invalid  \n0  \\n\\næ–°åŠ å¡ç¨³å®šçš„é‡‘èæœºåˆ¶å’ŒåŒºåŸŸè”é€šæ€§å¯¹å…¨çƒå¯Œè±ªå…·æœ‰å¼ºå¤§å¸å¼•åŠ›ï¼Œæ˜¯æœ€å¤šå¯Œè±ªé¦–é€‰çš„ç§»å±…ç›®çš„åœ°...    False  \n1  è‚–äº¬2024-07-08æµè§ˆé‡ï¼š1498\\næˆ‘å›½è¶…é«˜å‡€å€¼äººç¾¤è®¾ç«‹å®¶æ—åŠå…¬å®¤çš„éœ€æ±‚æ˜¯å®¢è§‚å­˜åœ¨çš„...    False  \n2  ä¸­å›½é«˜å‡€å€¼äººå£«åœ¨æ–°åŠ å¡è®¾ç«‹å®¶æ—åŠå…¬å®¤å’ŒæŠ•èµ„åˆåˆ›ä¼ä¸šçš„æœ€æ–°è¶‹åŠ¿\\näº”å¹´å‰ï¼Œæ¢ä¿¡å†›å› å¥åº·åŸå› ç¦»å¼€...    False  \n3  è¿‘å¹´æ¥ï¼Œè¶Šæ¥è¶Šå¤šçš„å›½å†…ä¼ä¸šå®¶å’Œé«˜å‡€å€¼äººå£«å°†ç›®å…‰æŠ•å‘æ–°åŠ å¡ï¼Œå°¤å…¶æ˜¯é‚£äº›æ­£åœ¨è€ƒè™‘ç§»æ°‘æˆ–å­å¥³æ•™è‚²çš„...    False  \n4  æœ¬æ–‡ç”±å½­åšè¡Œä¸šç ”ç©¶é«˜çº§åˆ†æå¸ˆé»„é¢–çŠï¼ˆSharnie Wongï¼‰æ’°å†™ï¼Œé¦–å‘äºå½­åšç»ˆç«¯ã€‚\\nç‘é“¶...    False  \n\nâš ç¼ºå¤±å€¼ç»Ÿè®¡:\nkeyword             0\ntitle               0\nurl                 0\nsearch_timestamp    0\ncontent             0\ninvalid             0\ndtype: int64\n```\n:::\n:::\n\n\n::: {#47974b01 .cell execution_count=10}\n``` {.python .cell-code}\ndf_cleaned = df[~df[\"invalid\"] & df[\"content\"].notna()].copy()\ndf_cleaned.to_excel(\"search_results_cleaned.xlsx\", index=False)\n```\n:::\n\n\n::: {#90abc203 .cell execution_count=11}\n``` {.python .cell-code}\nimport re\nfrom collections import Counter\nimport jieba\n\n# 1. åŠ è½½æ¸…æ´—åçš„æ–‡ç« æ•°æ®\ndf = pd.read_excel(\"Dataset/search_results_cleaned.xlsx\")\ndf = df[df[\"content\"].notna() & (df[\"content\"].str.strip() != \"\")]\n\n# 2. è¯»å–åœç”¨è¯è¡¨\nwith open(\"Dataset/cn_stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n    stopwords = set([line.strip() for line in f])\n\n# 3. æ¸…æ´—å‡½æ•°ï¼šå»é™¤è‹±æ–‡ã€æ—¥æœŸã€æ•°å­—ã€HTMLã€è‹±æ–‡æœ¯è¯­ç­‰\ndef clean_text(s):\n    s = re.sub(r'<.*?>', '', s)  # HTMLæ ‡ç­¾\n    s = re.sub(r'[a-zA-Z]+', '', s)  # è‹±æ–‡\n    s = re.sub(r'[\\d\\-:/\\.å¹´æœˆæ—¥\\s]+', '', s)  # æ•°å­—ä¸æ—¥æœŸ\n    s = re.sub(r'[\\u0000-\\u007F]+', '', s)  # ASCIIç¬¦å·\n    return s.strip()\n\n# 4. åˆ†å¥ + æ¸…æ´— + åˆ†è¯ + å»åœç”¨è¯\nwords = []\nfor content in df[\"content\"]:\n    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', content)\n    for s in sentences:\n        s_clean = clean_text(s)\n        if len(s_clean) >= 5:\n            segs = jieba.cut(s_clean)\n            words += [w for w in segs if len(w) > 1 and w not in stopwords and re.match(r'[\\u4e00-\\u9fff]+', w)]\n\n# 5. è¯é¢‘ç»Ÿè®¡\nword_freq = Counter(words)\ndf_freq = pd.DataFrame(word_freq.most_common(100), columns=[\"Word\", \"Frequency\"])\n\n# 6. ä¿å­˜åˆ°æ–‡ä»¶\ndf_freq.to_excel(\"Dataset/word_frequency_cleaned_with_stopwords.xlsx\", index=False)\n\n# 7. æ‰“å°ç»“æœ\nprint(df_freq)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Word  Frequency\n0   æ–°åŠ å¡       1895\n1    ä¿¡æ‰˜       1301\n2    å®¶æ—       1028\n3    èµ„äº§        947\n4    æŠ•èµ„        922\n..  ...        ...\n95   æ–¹å¼        133\n96   ç›®å‰        132\n97   é¡¾é—®        132\n98   æ•°é‡        128\n99   äº§å“        126\n\n[100 rows x 2 columns]\n```\n:::\n:::\n\n\n# 2 **Text Analysis**\n\n## **2.1 Frequency Analysis**\n\n::: {#fbb22bdf .cell execution_count=12}\n``` {.python .cell-code}\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# 1. åŠ è½½è¯é¢‘æ•°æ®\ndf_freq = pd.read_excel(\"Dataset/word_frequency_cleaned_with_stopwords.xlsx\")\n\n# 2. è½¬æ¢ä¸ºå­—å…¸æ ¼å¼\nfreq_dict = dict(zip(df_freq[\"Word\"], df_freq[\"Frequency\"]))\n\n# 3. åˆ›å»ºè¯äº‘å¯¹è±¡ï¼ˆä½¿ç”¨ç³»ç»Ÿä¸­ä¸­æ–‡å­—ä½“ï¼‰\nwc = WordCloud(\n    font_path=\"/System/Library/Fonts/STHeiti Medium.ttc\",  # æ›¿æ¢ä¸ºä½ æœ¬åœ°æ”¯æŒä¸­æ–‡çš„å­—ä½“è·¯å¾„\n    background_color=\"white\",\n    width=1000,\n    height=700,\n    max_words=200\n).generate_from_frequencies(freq_dict)\n\n# 4. å¯è§†åŒ–è¯äº‘\nplt.figure(figsize=(12, 8))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"WordCloud\", fontsize=18)\nplt.subplots_adjust(top=0.85) \nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Market_files/figure-html/cell-13-output-1.png){}\n:::\n:::\n\n\n## **2.2 Sentiment Analysis**\n\n::: {#41ad8085 .cell execution_count=13}\n``` {.python .cell-code}\nfrom snownlp import SnowNLP\n\n# è¯»å–å·²æ¸…æ´—çš„æ•°æ®ï¼ˆç¡®ä¿è·¯å¾„æ­£ç¡®ï¼‰\ndf = pd.read_excel(\"Dataset/search_results_cleaned.xlsx\")\n\n# è¿‡æ»¤æ‰æ— æ­£æ–‡\ndf = df[df[\"content\"].notna() & (df[\"content\"].str.len() > 30)].copy()\n\n# æƒ…æ„Ÿåˆ†æå‡½æ•°ï¼ˆè¿”å›å€¼åœ¨ 0ï½1 ä¹‹é—´ï¼Œ1 è¶Šç§¯æï¼‰\ndef get_sentiment(text):\n    try:\n        return SnowNLP(text).sentiments\n    except:\n        return None\n\n# æ·»åŠ æƒ…æ„Ÿè¯„åˆ†åˆ—\ndf[\"sentiment_score\"] = df[\"content\"].apply(get_sentiment)\n\n# åˆ†ç±»æ ‡ç­¾ï¼ˆå¯é€‰ï¼‰ï¼šå¤§äº 0.6 ä¸ºæ­£é¢ï¼Œå°äº 0.4 ä¸ºè´Ÿé¢ï¼Œå…¶ä½™ä¸ºä¸­æ€§\ndef classify(score):\n    if score is None:\n        return \"Unknown\"\n    elif score > 0.6:\n        return \"Positive\"\n    elif score < 0.4:\n        return \"Negative\"\n    else:\n        return \"Neutral\"\n\ndf[\"sentiment_label\"] = df[\"sentiment_score\"].apply(classify)\n\n# ä¿å­˜ç»“æœ\ndf.to_excel(\"Dataset/search_results_sentiment.xlsx\", index=False)\n\n# æŸ¥çœ‹ç»Ÿè®¡\nprint(df[\"sentiment_label\"].value_counts())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nsentiment_label\nPositive    85\nNegative     5\nName: count, dtype: int64\n```\n:::\n:::\n\n\n::: {#16c161d0 .cell execution_count=14}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\n# Count sentiment\nsentiment_counts = df[\"sentiment_label\"].value_counts()\n\n# è®¾ç½®é¢œè‰²å’Œæ ‡ç­¾é¡ºåº\nlabels = sentiment_counts.index.tolist()\ncolors = ['green' if label == 'Positive' else 'red' for label in labels]\n\n# ç»˜å›¾\nplt.figure(figsize=(6,6))\nplt.pie(\n    sentiment_counts,\n    labels=labels,\n    autopct='%1.1f%%',\n    startangle=140,\n    colors=colors,\n    textprops={'fontsize': 14}\n)\nplt.title(\"Sentiment Composition\", fontsize=16)\nplt.tight_layout()\nplt.axis('equal')  \nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Market_files/figure-html/cell-15-output-1.png){}\n:::\n:::\n\n\n::: {#c3b94d8b .cell execution_count=15}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\nplt.rcParams['font.family'] = 'Arial Unicode MS'  # æ›´é€šç”¨çš„ä¸­è‹±æ··æ’å­—ä½“\nplt.rcParams['axes.unicode_minus'] = False\n\n# åˆ†ç»„ï¼šç»Ÿè®¡æ¯ä¸ª keyword ä¸‹çš„æ­£è´Ÿé¢æ•°é‡\nsentiment_by_keyword = df.groupby(['keyword', 'sentiment_label']).size().unstack(fill_value=0)\n\n# æ‰“å°æ£€æŸ¥\nprint(sentiment_by_keyword)\n\n# ç»˜å›¾ï¼šå †å æŸ±çŠ¶å›¾\nsentiment_by_keyword.plot(kind='bar', stacked=True, figsize=(12, 6), color=[\"#1f77b4\", \"#ff7f0e\"])  # å¯è‡ªå®šä¹‰é¢œè‰²\n\n# è®¾ç½®æ ‡é¢˜å’Œè½´æ ‡ç­¾\nplt.title(\"å…³é”®è¯æƒ…æ„Ÿæ„æˆ\", fontsize=16)\nplt.xlabel(\"å…³é”®è¯\", fontsize=12)\nplt.ylabel(\"æ–‡ç« æ•°é‡\", fontsize=12)\n\n# æ—‹è½¬ x è½´æ–‡å­—é˜²æ­¢é‡å \nplt.xticks(rotation=45, ha='right')\n\n# è‡ªåŠ¨å¸ƒå±€\nplt.tight_layout()\n\n# æ˜¾ç¤ºå›¾åƒ\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nsentiment_label   Negative  Positive\nkeyword                             \nä¸­å›½ å®¶æ—ä¿¡æ‰˜ æ–°åŠ å¡              0        14\nä¸­å›½å¯Œäºº èµ„äº§é…ç½® æ–°åŠ å¡            1         8\nä¸­å›½å¯Œè±ª ä¸ºä»€ä¹ˆç§»æ°‘æ–°åŠ å¡            1         5\nä¸­å›½ç§»æ°‘æ–°åŠ å¡ è´¢å¯Œç®¡ç†             0        11\nä¸­å›½é«˜å‡€å€¼äººå£« å­å¥³æ•™è‚² æ–°åŠ å¡         0         9\nä¸­å›½é«˜å‡€å€¼äººå£« æ–°åŠ å¡              2        12\nä¸­å›½é«˜å‡€å€¼å®¢æˆ· ç¦»å²¸è´¦æˆ·             0        15\næ–°åŠ å¡ CRS é¿ç¨               1        11\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Market_files/figure-html/cell-16-output-2.png){}\n:::\n:::\n\n\n## **2.3 Network Analysis**\n\n::: {#30a42d87 .cell execution_count=16}\n``` {.python .cell-code}\nfrom itertools import combinations\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# ç”¨äºå­˜å‚¨æ¯å¥è¯çš„å…³é”®è¯ï¼ˆåµŒå¥—åˆ—è¡¨ï¼‰\nkeyword_sentences = []\n\nfor content in df[\"content\"]:\n    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', content)\n    for s in sentences:\n        s_clean = clean_text(s)\n        if len(s_clean) >= 5:\n            segs = jieba.cut(s_clean)\n            word_list = [w for w in segs if len(w) > 1 and w not in stopwords and re.match(r'[\\u4e00-\\u9fff]+', w)]\n            keyword_sentences.append(word_list)\n```\n:::\n\n\n::: {#27d7b77b .cell execution_count=17}\n``` {.python .cell-code}\ntop_100_words = set(df_freq[\"Word\"])\nco_occurrence = Counter()\n\n# åªç»Ÿè®¡ top100 è¯ä¹‹é—´çš„å…±ç°\nfor word_list in keyword_sentences:\n    words_in_top100 = [w for w in word_list if w in top_100_words]\n    for pair in combinations(set(words_in_top100), 2):  # set å»é‡\n        co_occurrence[tuple(sorted(pair))] += 1\n```\n:::\n\n\n::: {#7faaeba8 .cell execution_count=18}\n``` {.python .cell-code}\nimport matplotlib.font_manager as fm\nfor font in fm.findSystemFonts(fontpaths=None, fontext='ttf'):\n    if 'PingFang' in font or 'Arial' in font or 'Hei' in font:\n        print(font)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n/System/Library/Fonts/Supplemental/Arial Bold Italic.ttf\n/Library/Fonts/Arial Unicode.ttf\n/System/Library/Fonts/Supplemental/Arial Narrow Bold.ttf\n/System/Library/Fonts/Supplemental/Arial Narrow Italic.ttf\n/System/Library/Fonts/Supplemental/Arial Unicode.ttf\n/System/Library/Fonts/Supplemental/Arial Rounded Bold.ttf\n/System/Library/Fonts/Supplemental/Arial Narrow Bold Italic.ttf\n/System/Library/Fonts/STHeiti Light.ttc\n/System/Library/Fonts/Supplemental/Arial Bold.ttf\n/System/Library/Fonts/Supplemental/Arial Black.ttf\n/System/Library/Fonts/Supplemental/Arial.ttf\n/System/Library/Fonts/PingFang.ttc\n/System/Library/Fonts/Supplemental/Arial Narrow.ttf\n/System/Library/Fonts/STHeiti Medium.ttc\n/System/Library/Fonts/Supplemental/Arial Italic.ttf\n/System/Library/Fonts/ArialHB.ttc\n```\n:::\n:::\n\n\n::: {#bd751648 .cell execution_count=19}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\nimport networkx as nx\n\n# æ‰‹åŠ¨è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆå¯æ›¿æ¢ä¸ºä½ æœºå™¨å·²æœ‰çš„å…¶ä»–ä¸­æ–‡å­—ä½“è·¯å¾„ï¼‰\nmy_font = fm.FontProperties(fname=\"/System/Library/Fonts/STHeiti Medium.ttc\")\nplt.rcParams['font.family'] = my_font.get_name()\nplt.rcParams['axes.unicode_minus'] = False\n\n# æ„å»ºå›¾ï¼ˆä½ éœ€è¦å·²ç»å‡†å¤‡å¥½ co_occurrence å­—å…¸ï¼‰\nG = nx.Graph()\nfor (w1, w2), freq in co_occurrence.items():\n    if freq >= 3:  # è®¾å®šå…±ç°é˜ˆå€¼\n        G.add_edge(w1, w2, weight=freq)\n\n# å¯è§†åŒ–ç»˜å›¾\nplt.figure(figsize=(12, 10))\npos = nx.spring_layout(G, k=0.5, seed=42)  # èŠ‚ç‚¹å¸ƒå±€\nnx.draw_networkx_nodes(G, pos, node_size=500, node_color='lightblue')\nnx.draw_networkx_edges(G, pos, width=[d['weight'] * 0.3 for _, _, d in G.edges(data=True)], alpha=0.6)\nnx.draw_networkx_labels(G, pos, font_size=10, font_family=my_font.get_name())\n\nplt.title(\"Top 100 ä¸­æ–‡å…³é”®è¯å…±ç°ç½‘ç»œ\", fontproperties=my_font)\nplt.axis(\"off\")\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Market_files/figure-html/cell-20-output-1.png){}\n:::\n:::\n\n\n::: {#e7da644a .cell execution_count=20}\n``` {.python .cell-code}\n# è®¡ç®—èŠ‚ç‚¹çš„åº¦ä¸­å¿ƒæ€§\ndegree_centrality = nx.degree_centrality(G)\n\n# æŒ‰ä¸­å¿ƒæ€§æ’åºï¼Œå–å‰10ä¸ªå…³é”®è¯\ntop_nodes = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n\n# æ‰“å°ç»“æœ\nprint(\"Top 10 å…³é”®è¯ï¼ˆæŒ‰ç½‘ç»œä¸­å¿ƒæ€§ï¼‰:\")\nfor word, score in top_nodes:\n    print(f\"{word}: {score:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTop 10 å…³é”®è¯ï¼ˆæŒ‰ç½‘ç»œä¸­å¿ƒæ€§ï¼‰:\næ–°åŠ å¡: 1.000\nèµ„äº§: 1.000\næŠ•èµ„: 1.000\nå…¬å¸: 1.000\nç®¡ç†: 0.980\nå‡€å€¼: 0.970\nä¸­å›½: 0.970\né¦™æ¸¯: 0.970\nè¿›è¡Œ: 0.970\nè´¢å¯Œ: 0.960\n```\n:::\n:::\n\n\n::: {#2c16adfb .cell execution_count=21}\n``` {.python .cell-code}\nimport community as community_louvain  # pip install python-louvain\nimport matplotlib.cm as cm\n\n# ç¤¾åŒºåˆ’åˆ†\npartition = community_louvain.best_partition(G)\n\n# è®¾ç½®é¢œè‰²æ˜ å°„\nsize = float(len(set(partition.values())))\npos = nx.spring_layout(G, k=0.5, seed=42)\ncolors = [cm.tab20(i / size) for i in partition.values()]\n\n# ç»˜å›¾\nplt.figure(figsize=(14, 12))\nnx.draw_networkx_nodes(G, pos, node_size=500, node_color=colors, alpha=0.8)\nnx.draw_networkx_edges(G, pos, width=0.5, alpha=0.3)\nnx.draw_networkx_labels(G, pos, font_size=10, font_family='Arial Unicode MS')\nplt.title(\"å…³é”®è¯å…±ç°ç½‘ç»œä¸­çš„è¯é¢˜ç¤¾åŒº\")\nplt.axis(\"off\")\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Market_files/figure-html/cell-22-output-1.png){}\n:::\n:::\n\n\n::: {.table .striped .hover caption=\"Semantic Categories of Keyword Communities\"}\n| Community Color | Community ID | Semantic Theme | Sample Keywords |\n|-----------------|-----------------|--------------------|--------------------|\n| ğŸŒ• Brown | Community 1 | Trust & Wealth Management | trust, assets, fund, legal |\n| ğŸŸ¢ Green | Community 2 | Client Service & Immigration | advisor, immigration, client, plan |\n| ğŸ”µ Blue | Community 3 | HNW Assets & Cross-border Tax | bank, net worth, Hong Kong, tax |\n| âš« Gray | Community 4 | General/Neutral Words | include, mainly, plan, one |\n:::\n\n# 3 **Conclusion**\n\n",
    "supporting": [
      "Market_files"
    ],
    "filters": [],
    "includes": {}
  }
}