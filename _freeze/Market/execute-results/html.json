{
  "hash": "57030fcda211f9888f62580ea4427c87",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Makert Survey\"\nauthor: \"Zou Jiaxun\"\nexecute: \n  eval: true\n  echo: true\n  warning: false\n  freeze: true\n  cache: true\nformat:\n  html:\n    code-fold: true\n    code-summary: \"Click to view code\"\njupyter: python3  \n---\n\n\n\n\n# 1 **Overview**\n\n## 1.1 **Background**\n\nTo effectively organize and communicate the complex thought process behind why Chinese HNW individuals move their money to Singapore, I created Flowchart. This flowchart helps to:\n\n-   Visually map the key motivations (such as wealth management, tax benefits, lifestyle advantages, and legacy planning) that drive this financial movement.\n\n-   Clarify the relationships between client needs, Singapore’s value proposition, and the strategic advisory angles we can take.\n\n-   Support client-facing engagement by simplifying the narrative for Life Inc advisors to present during weekly meetings or consultations.\n\n-   Guide my survey design and presentation content, ensuring all relevant themes—such as trends, preferences, and asset relocation methods—are covered logically and comprehensively.\n\nIt acts as both a research framework and a client communication tool to align our understanding and offerings with the target market’s aspirations.\n\n![](Picture/Flowchart1.png){fig-align=\"center\"}\n\n## **1.2 Data Collect**\n\nTo support the research in Flowchart, I developed a custom web crawler to systematically collect qualitative and contextual data from Chinese-language sources such as forums (e.g., Zhihu, Xueqiu), financial news sites (e.g., Eastern Wealth), and financial blogs. These platforms offer authentic discussions and opinions from Chinese citizens regarding overseas wealth movement—especially to Singapore.\n\nMy spider focused on keywords that map directly to the key branches of the flowchart, such as:\n\n-   “新加坡 财富管理” (Singapore wealth management)\n\n-   “中国人移民新加坡” (Chinese migration to Singapore)\n\n-   “税务优化 新加坡” (Tax optimization Singapore)\n\n-   “资产配置 海外” (Offshore asset allocation)\n\n::: callout-note\n## Why choose spider instead of survey\n\n-   **Understand Real Conversations First**: The spider captures what people are already saying online, so I don’t assume or guess their opinions.\n-   **Gather Rich and Honest Insights**: Online posts reveal detailed reasons, emotions, and comparisons that people may not share in surveys.\n-   **Reach More People Easily**: It’s hard to get high-net-worth individuals to answer surveys, but the spider can access public content instantly.\n-   **Build a Better Survey Later**: By analyzing online themes first, I can design a more focused and relevant survey based on real concerns.\n:::\n\n::: {#4208403e .cell execution_count=1}\n``` {.python .cell-code}\nimport requests\nimport json\nimport time\nimport pandas as pd\nfrom datetime import datetime\nimport os\nimport openpyxl\n\n# 用户配置区域\nSERPAPI_KEY = \"84c798a8d45d1e7cae0b18df778ac06bf2c6169f0249e40756aea0b9d6cd4749\"  \nRESULTS_PER_KEYWORD = 20\nDELAY_BETWEEN_REQUESTS = 1\nDELAY_BETWEEN_KEYWORDS = 2\n```\n:::\n\n\n::: {#bd335e46 .cell execution_count=2}\n``` {.python .cell-code}\nkeywords = [\n    \"中国高净值人士 新加坡\",\n    \"中国富人 资产配置 新加坡\", \n    \"中国富豪 为什么移民新加坡\",\n    \"中国高净值客户 离岸账户\",\n    \"新加坡 CRS 避税\",\n    \"中国 家族信托 新加坡\",\n    \"中国高净值人士 子女教育 新加坡\",\n    \"中国移民新加坡 财富管理\"\n]\n```\n:::\n\n\n::: {#84bf7aa5 .cell execution_count=3}\n``` {.python .cell-code}\ndef serpapi_search(query, api_key, num=20, start=0):\n    url = \"https://serpapi.com/search\"\n    params = {\n        'q': query,\n        'api_key': api_key,\n        'engine': 'google',\n        'num': min(num, 100),\n        'start': start,\n        'hl': 'zh-cn',\n        'gl': 'cn'\n    }\n    try:\n        response = requests.get(url, params=params)\n        response.raise_for_status()\n        return response.json()\n    except requests.exceptions.RequestException as e:\n        print(f\"搜索请求失败: {e}\")\n        return {}\n\ndef extract_search_results(results, keyword):\n    extracted_results = []\n    organic_results = results.get('organic_results', [])\n    for item in organic_results:\n        result_info = {\n            'keyword': keyword,\n            'title': item.get('title', ''),\n            'url': item.get('link', ''),\n            'snippet': item.get('snippet', ''),\n            'displayed_link': item.get('displayed_link', ''),\n            'position': item.get('position', 0),\n            'search_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        }\n        extracted_results.append(result_info)\n    return extracted_results\n```\n:::\n\n\n::: {#27494f80 .cell execution_count=4}\n``` {.python .cell-code}\ndef search_keyword_with_pagination(keyword, api_key, total_results=20):\n    all_results = []\n    results_per_page = 10\n    for start in range(0, total_results, results_per_page):\n        remaining = total_results - start\n        num_to_get = min(results_per_page, remaining)\n        print(f\"    获取第 {start+1}-{start+num_to_get} 条结果...\")\n        results = serpapi_search(keyword, api_key, num=num_to_get, start=start)\n        if not results or 'organic_results' not in results:\n            print(f\"没有更多结果\")\n            break\n        extracted = extract_search_results(results, keyword)\n        all_results.extend(extracted)\n        if len(extracted) < num_to_get:\n            break\n        time.sleep(DELAY_BETWEEN_REQUESTS)\n    return all_results\n```\n:::\n\n\n::: {#c9fc74d4 .cell execution_count=5}\n``` {.python .cell-code}\ndef main():\n    print(\"开始搜索...\")\n    print(f\"搜索关键词数量: {len(keywords)}\")\n    print(f\"每个关键词获取结果数: {RESULTS_PER_KEYWORD}\")\n    print(\"-\" * 50)\n    \n    all_search_results = []\n    for i, keyword in enumerate(keywords, 1):\n        print(f\"[{i}/{len(keywords)}] 搜索关键词: {keyword}\")\n        try:\n            results = search_keyword_with_pagination(keyword, SERPAPI_KEY, RESULTS_PER_KEYWORD)\n            all_search_results.extend(results)\n            print(f\"找到 {len(results)} 个结果\")\n            if i < len(keywords):\n                print(f\"等待 {DELAY_BETWEEN_KEYWORDS} 秒...\")\n                time.sleep(DELAY_BETWEEN_KEYWORDS)\n        except Exception as e:\n            print(f\"错误: {e}\")\n            continue\n    print(\"-\" * 50)\n    print(f\"总共找到 {len(all_search_results)} 个结果\")\n    return all_search_results\n\nsearch_results = main()\n```\n:::\n\n\n::: {#fc54e904 .cell execution_count=6}\n``` {.python .cell-code}\nif search_results:\n    df_full = pd.DataFrame(search_results)\n    df_unique = df_full.drop_duplicates(subset=['url'], keep='first')\n    print(f\"原始结果: {len(df_full)}，去重后: {len(df_unique)}\")\n\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    df_simple = df_unique[['keyword', 'title', 'url', 'search_timestamp']].copy()\n\n    # 保存到指定 Dataset\n    save_dir = os.path.expanduser(\"/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset\")\n    os.makedirs(save_dir, exist_ok=True)\n\n    df_unique.to_csv(os.path.join(save_dir, f\"search_results_full_{timestamp}.csv\"), index=False, encoding='utf-8-sig')\n    df_simple.to_csv(os.path.join(save_dir, f\"search_results_simple_{timestamp}.csv\"), index=False, encoding='utf-8-sig')\n\n    with pd.ExcelWriter(os.path.join(save_dir, f\"search_results_{timestamp}.xlsx\"), engine='openpyxl') as writer:\n        df_unique.to_excel(writer, sheet_name='完整数据', index=False)\n        df_simple.to_excel(writer, sheet_name='简化数据', index=False)\n\n    print(f\"文件保存路径: {save_dir}\")\n\n    print(\"\\n数据预览:\")\n    print(df_simple.head())\n\n    print(\"\\n每个关键词的结果数量:\")\n    print(df_unique['keyword'].value_counts())\n\n    globals()['search_data'] = df_unique\n    globals()['search_data_simple'] = df_simple\n\n    print(\"\\n变量已加载: search_data, search_data_simple\")\nelse:\n    print(\"没有搜索结果\")\n    globals()['search_data'] = pd.DataFrame()\n    globals()['search_data_simple'] = pd.DataFrame()\n```\n:::\n\n\n::: {#c929b482 .cell execution_count=7}\n``` {.python .cell-code}\nfrom bs4 import BeautifulSoup\nimport chardet\n\n# 文件路径设置\ninput_file = \"/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset/search_results_20250706_192824.xlsx\"\noutput_file = \"/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset/search_results_with_content.xlsx\"\n\n# 读取简化数据工作表\ndf = pd.read_excel(input_file, sheet_name=\"简化数据\")\n\n# 正文提取函数\ndef fetch_article_content(url, timeout=10):\n    try:\n        response = requests.get(url, timeout=timeout, headers={\"User-Agent\": \"Mozilla/5.0\"})\n        detected = chardet.detect(response.content)\n        response.encoding = detected['encoding'] or 'utf-8'\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        paragraphs = soup.find_all('p')\n        content = '\\n'.join(p.get_text(strip=True) for p in paragraphs)\n        return content if len(content) > 50 else None\n    except Exception:\n        return None\n\n# 爬取内容\nprint(\"开始抓取正文内容...\")\ndf[\"content\"] = df[\"url\"].apply(fetch_article_content)\nprint(\"正文抓取完成，开始保存文件...\")\n\n# 保存为新的 Excel 文件\ndf.to_excel(output_file, index=False)\nprint(f\"文件已保存至：{output_file}\")\n```\n:::\n\n\n## **1.3 Data Clean**\n\n::: {#62d8dc0f .cell execution_count=8}\n``` {.python .cell-code}\n# 输入输出路径\ninput_file = \"/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset/search_results_with_content.xlsx\"\noutput_file = \"/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset/search_results_cleaned.xlsx\"\n\n# 加载 Excel 文件\ndf = pd.read_excel(input_file)\n\n# 定义无效内容关键词\nad_keywords = [\n    \"免责声明\", \"广告合作\", \"联系管理员\", \"请在微信中打开\", \"本站所有文章\",\n    \"抱歉\", \"页面不存在\", \"出错\", \"404\", \"请输入验证码\",\n    \"登录查看全文\", \"Oops\", \"Something went wrong\", \"访问受限\"\n]\n\n# 判断是否为无效正文\ndef is_invalid(text):\n    if pd.isna(text):\n        return True\n    if len(text.strip()) < 100:\n        return True\n    if any(kw in text for kw in ad_keywords):\n        return True\n    return False\n\n# 添加标记列\ndf[\"invalid\"] = df[\"content\"].apply(is_invalid)\n\n# 保留有效正文内容\ndf_cleaned = df[~df[\"invalid\"]].drop(columns=[\"invalid\"]).copy()\n\n# 保存清洗后的结果\ndf_cleaned.to_excel(output_file, index=False)\nprint(f\"清洗完成，已保存为：{output_file}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n清洗完成，已保存为：/Applications/SMU/Great Eastern/Personal Project/RFM Project/Dataset/search_results_cleaned.xlsx\n```\n:::\n:::\n\n\n::: {#0e8ccd5d .cell execution_count=9}\n``` {.python .cell-code}\n# 读取 Excel 文件\ndf = pd.read_excel(\"search_results_cleaned.xlsx\")\n\n# 1. 查看行数和列数\nprint(\"行数 × 列数:\", df.shape)\n\n# 2. 查看列名和类型\nprint(\"\\n列名与数据类型:\")\nprint(df.dtypes)\n\n# 3. 快速概览每列的前几行（结构 + 值）\nprint(\"\\n样本预览:\")\nprint(df.head())\n\n# 4. 缺失值统计（NA 值）\nprint(\"\\n⚠缺失值统计:\")\nprint(df.isna().sum())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n行数 × 列数: (90, 6)\n\n列名与数据类型:\nkeyword             object\ntitle               object\nurl                 object\nsearch_timestamp    object\ncontent             object\ninvalid               bool\ndtype: object\n\n样本预览:\n       keyword                           title  \\\n0  中国高净值人士 新加坡          新加坡金融机制稳定吸引最多高净值人士考虑移居   \n1  中国高净值人士 新加坡          我国超高净值人群设立家族办公室现状分析与应对   \n2  中国高净值人士 新加坡  中国高净值人士在新加坡设立家族办公室和投资初创企业的 ...   \n3  中国高净值人士 新加坡        高净值人士为何纷纷选择新加坡？_移民_教育_工作   \n4  中国高净值人士 新加坡    聚焦家办| 亚洲超高净值人群增长或将推动家族办公室的发展   \n\n                                                 url     search_timestamp  \\\n0  https://www.zaobao.com/finance/singapore/story...  2025-07-06 19:19:49   \n1  http://cel.cn/List/FullText?articleId=d37c148c...  2025-07-06 19:19:49   \n2  https://fargowealth.com/en/home/cfsj/cfsj_deta...  2025-07-06 19:19:49   \n3         https://www.sohu.com/a/843292362_121963266  2025-07-06 19:19:49   \n4  https://www.bloombergchina.com/blog/asias-ultr...  2025-07-06 19:19:49   \n\n                                             content  invalid  \n0  \\n\\n新加坡稳定的金融机制和区域联通性对全球富豪具有强大吸引力，是最多富豪首选的移居目的地...    False  \n1  肖京2024-07-08浏览量：1498\\n我国超高净值人群设立家族办公室的需求是客观存在的...    False  \n2  中国高净值人士在新加坡设立家族办公室和投资初创企业的最新趋势\\n五年前，梁信军因健康原因离开...    False  \n3  近年来，越来越多的国内企业家和高净值人士将目光投向新加坡，尤其是那些正在考虑移民或子女教育的...    False  \n4  本文由彭博行业研究高级分析师黄颖珊（Sharnie Wong）撰写，首发于彭博终端。\\n瑞银...    False  \n\n⚠缺失值统计:\nkeyword             0\ntitle               0\nurl                 0\nsearch_timestamp    0\ncontent             0\ninvalid             0\ndtype: int64\n```\n:::\n:::\n\n\n::: {#47974b01 .cell execution_count=10}\n``` {.python .cell-code}\ndf_cleaned = df[~df[\"invalid\"] & df[\"content\"].notna()].copy()\ndf_cleaned.to_excel(\"search_results_cleaned.xlsx\", index=False)\n```\n:::\n\n\n::: {#90abc203 .cell execution_count=11}\n``` {.python .cell-code}\nimport re\nfrom collections import Counter\nimport jieba\n\n# 1. 加载清洗后的文章数据\ndf = pd.read_excel(\"Dataset/search_results_cleaned.xlsx\")\ndf = df[df[\"content\"].notna() & (df[\"content\"].str.strip() != \"\")]\n\n# 2. 读取停用词表\nwith open(\"Dataset/cn_stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n    stopwords = set([line.strip() for line in f])\n\n# 3. 清洗函数：去除英文、日期、数字、HTML、英文术语等\ndef clean_text(s):\n    s = re.sub(r'<.*?>', '', s)  # HTML标签\n    s = re.sub(r'[a-zA-Z]+', '', s)  # 英文\n    s = re.sub(r'[\\d\\-:/\\.年月日\\s]+', '', s)  # 数字与日期\n    s = re.sub(r'[\\u0000-\\u007F]+', '', s)  # ASCII符号\n    return s.strip()\n\n# 4. 分句 + 清洗 + 分词 + 去停用词\nwords = []\nfor content in df[\"content\"]:\n    sentences = re.split(r'[。！？]', content)\n    for s in sentences:\n        s_clean = clean_text(s)\n        if len(s_clean) >= 5:\n            segs = jieba.cut(s_clean)\n            words += [w for w in segs if len(w) > 1 and w not in stopwords and re.match(r'[\\u4e00-\\u9fff]+', w)]\n\n# 5. 词频统计\nword_freq = Counter(words)\ndf_freq = pd.DataFrame(word_freq.most_common(100), columns=[\"Word\", \"Frequency\"])\n\n# 6. 保存到文件\ndf_freq.to_excel(\"Dataset/word_frequency_cleaned_with_stopwords.xlsx\", index=False)\n\n# 7. 打印结果\nprint(df_freq)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Word  Frequency\n0   新加坡       1895\n1    信托       1301\n2    家族       1028\n3    资产        947\n4    投资        922\n..  ...        ...\n95   方式        133\n96   目前        132\n97   顾问        132\n98   数量        128\n99   产品        126\n\n[100 rows x 2 columns]\n```\n:::\n:::\n\n\n# 2 **Text Analysis**\n\n## **2.1 Frequency Analysis**\n\n::: {#fbb22bdf .cell execution_count=12}\n``` {.python .cell-code}\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# 1. 加载词频数据\ndf_freq = pd.read_excel(\"Dataset/word_frequency_cleaned_with_stopwords.xlsx\")\n\n# 2. 转换为字典格式\nfreq_dict = dict(zip(df_freq[\"Word\"], df_freq[\"Frequency\"]))\n\n# 3. 创建词云对象（使用系统中中文字体）\nwc = WordCloud(\n    font_path=\"/System/Library/Fonts/STHeiti Medium.ttc\",  # 替换为你本地支持中文的字体路径\n    background_color=\"white\",\n    width=1000,\n    height=700,\n    max_words=200\n).generate_from_frequencies(freq_dict)\n\n# 4. 可视化词云\nplt.figure(figsize=(12, 8))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"WordCloud\", fontsize=18)\nplt.subplots_adjust(top=0.85) \nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Market_files/figure-html/cell-13-output-1.png){}\n:::\n:::\n\n\n## **2.2 Sentiment Analysis**\n\n::: {#41ad8085 .cell execution_count=13}\n``` {.python .cell-code}\nfrom snownlp import SnowNLP\n\n# 读取已清洗的数据（确保路径正确）\ndf = pd.read_excel(\"Dataset/search_results_cleaned.xlsx\")\n\n# 过滤掉无正文\ndf = df[df[\"content\"].notna() & (df[\"content\"].str.len() > 30)].copy()\n\n# 情感分析函数（返回值在 0～1 之间，1 越积极）\ndef get_sentiment(text):\n    try:\n        return SnowNLP(text).sentiments\n    except:\n        return None\n\n# 添加情感评分列\ndf[\"sentiment_score\"] = df[\"content\"].apply(get_sentiment)\n\n# 分类标签（可选）：大于 0.6 为正面，小于 0.4 为负面，其余为中性\ndef classify(score):\n    if score is None:\n        return \"Unknown\"\n    elif score > 0.6:\n        return \"Positive\"\n    elif score < 0.4:\n        return \"Negative\"\n    else:\n        return \"Neutral\"\n\ndf[\"sentiment_label\"] = df[\"sentiment_score\"].apply(classify)\n\n# 保存结果\ndf.to_excel(\"Dataset/search_results_sentiment.xlsx\", index=False)\n\n# 查看统计\nprint(df[\"sentiment_label\"].value_counts())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nsentiment_label\nPositive    85\nNegative     5\nName: count, dtype: int64\n```\n:::\n:::\n\n\n::: {#16c161d0 .cell execution_count=14}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\n# Count sentiment\nsentiment_counts = df[\"sentiment_label\"].value_counts()\n\n# 设置颜色和标签顺序\nlabels = sentiment_counts.index.tolist()\ncolors = ['green' if label == 'Positive' else 'red' for label in labels]\n\n# 绘图\nplt.figure(figsize=(6,6))\nplt.pie(\n    sentiment_counts,\n    labels=labels,\n    autopct='%1.1f%%',\n    startangle=140,\n    colors=colors,\n    textprops={'fontsize': 14}\n)\nplt.title(\"Sentiment Composition\", fontsize=16)\nplt.tight_layout()\nplt.axis('equal')  \nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Market_files/figure-html/cell-15-output-1.png){}\n:::\n:::\n\n\n::: {#c3b94d8b .cell execution_count=15}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\nplt.rcParams['font.family'] = 'Arial Unicode MS'  # 更通用的中英混排字体\nplt.rcParams['axes.unicode_minus'] = False\n\n# 分组：统计每个 keyword 下的正负面数量\nsentiment_by_keyword = df.groupby(['keyword', 'sentiment_label']).size().unstack(fill_value=0)\n\n# 打印检查\nprint(sentiment_by_keyword)\n\n# 绘图：堆叠柱状图\nsentiment_by_keyword.plot(kind='bar', stacked=True, figsize=(12, 6), color=[\"#1f77b4\", \"#ff7f0e\"])  # 可自定义颜色\n\n# 设置标题和轴标签\nplt.title(\"关键词情感构成\", fontsize=16)\nplt.xlabel(\"关键词\", fontsize=12)\nplt.ylabel(\"文章数量\", fontsize=12)\n\n# 旋转 x 轴文字防止重叠\nplt.xticks(rotation=45, ha='right')\n\n# 自动布局\nplt.tight_layout()\n\n# 显示图像\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nsentiment_label   Negative  Positive\nkeyword                             \n中国 家族信托 新加坡              0        14\n中国富人 资产配置 新加坡            1         8\n中国富豪 为什么移民新加坡            1         5\n中国移民新加坡 财富管理             0        11\n中国高净值人士 子女教育 新加坡         0         9\n中国高净值人士 新加坡              2        12\n中国高净值客户 离岸账户             0        15\n新加坡 CRS 避税               1        11\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Market_files/figure-html/cell-16-output-2.png){}\n:::\n:::\n\n\n## **2.3 Network Analysis**\n\n::: {#30a42d87 .cell execution_count=16}\n``` {.python .cell-code}\nfrom itertools import combinations\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# 用于存储每句话的关键词（嵌套列表）\nkeyword_sentences = []\n\nfor content in df[\"content\"]:\n    sentences = re.split(r'[。！？]', content)\n    for s in sentences:\n        s_clean = clean_text(s)\n        if len(s_clean) >= 5:\n            segs = jieba.cut(s_clean)\n            word_list = [w for w in segs if len(w) > 1 and w not in stopwords and re.match(r'[\\u4e00-\\u9fff]+', w)]\n            keyword_sentences.append(word_list)\n```\n:::\n\n\n::: {#27d7b77b .cell execution_count=17}\n``` {.python .cell-code}\ntop_100_words = set(df_freq[\"Word\"])\nco_occurrence = Counter()\n\n# 只统计 top100 词之间的共现\nfor word_list in keyword_sentences:\n    words_in_top100 = [w for w in word_list if w in top_100_words]\n    for pair in combinations(set(words_in_top100), 2):  # set 去重\n        co_occurrence[tuple(sorted(pair))] += 1\n```\n:::\n\n\n::: {#7faaeba8 .cell execution_count=18}\n``` {.python .cell-code}\nimport matplotlib.font_manager as fm\nfor font in fm.findSystemFonts(fontpaths=None, fontext='ttf'):\n    if 'PingFang' in font or 'Arial' in font or 'Hei' in font:\n        print(font)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n/System/Library/Fonts/Supplemental/Arial Bold Italic.ttf\n/Library/Fonts/Arial Unicode.ttf\n/System/Library/Fonts/Supplemental/Arial Narrow Bold.ttf\n/System/Library/Fonts/Supplemental/Arial Narrow Italic.ttf\n/System/Library/Fonts/Supplemental/Arial Unicode.ttf\n/System/Library/Fonts/Supplemental/Arial Rounded Bold.ttf\n/System/Library/Fonts/Supplemental/Arial Narrow Bold Italic.ttf\n/System/Library/Fonts/STHeiti Light.ttc\n/System/Library/Fonts/Supplemental/Arial Bold.ttf\n/System/Library/Fonts/Supplemental/Arial Black.ttf\n/System/Library/Fonts/Supplemental/Arial.ttf\n/System/Library/Fonts/PingFang.ttc\n/System/Library/Fonts/Supplemental/Arial Narrow.ttf\n/System/Library/Fonts/STHeiti Medium.ttc\n/System/Library/Fonts/Supplemental/Arial Italic.ttf\n/System/Library/Fonts/ArialHB.ttc\n```\n:::\n:::\n\n\n::: {#bd751648 .cell execution_count=19}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\nimport networkx as nx\n\n# 手动设置中文字体（可替换为你机器已有的其他中文字体路径）\nmy_font = fm.FontProperties(fname=\"/System/Library/Fonts/STHeiti Medium.ttc\")\nplt.rcParams['font.family'] = my_font.get_name()\nplt.rcParams['axes.unicode_minus'] = False\n\n# 构建图（你需要已经准备好 co_occurrence 字典）\nG = nx.Graph()\nfor (w1, w2), freq in co_occurrence.items():\n    if freq >= 3:  # 设定共现阈值\n        G.add_edge(w1, w2, weight=freq)\n\n# 可视化绘图\nplt.figure(figsize=(12, 10))\npos = nx.spring_layout(G, k=0.5, seed=42)  # 节点布局\nnx.draw_networkx_nodes(G, pos, node_size=500, node_color='lightblue')\nnx.draw_networkx_edges(G, pos, width=[d['weight'] * 0.3 for _, _, d in G.edges(data=True)], alpha=0.6)\nnx.draw_networkx_labels(G, pos, font_size=10, font_family=my_font.get_name())\n\nplt.title(\"Top 100 中文关键词共现网络\", fontproperties=my_font)\nplt.axis(\"off\")\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Market_files/figure-html/cell-20-output-1.png){}\n:::\n:::\n\n\n::: {#e7da644a .cell execution_count=20}\n``` {.python .cell-code}\n# 计算节点的度中心性\ndegree_centrality = nx.degree_centrality(G)\n\n# 按中心性排序，取前10个关键词\ntop_nodes = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n\n# 打印结果\nprint(\"Top 10 关键词（按网络中心性）:\")\nfor word, score in top_nodes:\n    print(f\"{word}: {score:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTop 10 关键词（按网络中心性）:\n新加坡: 1.000\n资产: 1.000\n投资: 1.000\n公司: 1.000\n管理: 0.980\n净值: 0.970\n中国: 0.970\n香港: 0.970\n进行: 0.970\n财富: 0.960\n```\n:::\n:::\n\n\n::: {#2c16adfb .cell execution_count=21}\n``` {.python .cell-code}\nimport community as community_louvain  # pip install python-louvain\nimport matplotlib.cm as cm\n\n# 社区划分\npartition = community_louvain.best_partition(G)\n\n# 设置颜色映射\nsize = float(len(set(partition.values())))\npos = nx.spring_layout(G, k=0.5, seed=42)\ncolors = [cm.tab20(i / size) for i in partition.values()]\n\n# 绘图\nplt.figure(figsize=(14, 12))\nnx.draw_networkx_nodes(G, pos, node_size=500, node_color=colors, alpha=0.8)\nnx.draw_networkx_edges(G, pos, width=0.5, alpha=0.3)\nnx.draw_networkx_labels(G, pos, font_size=10, font_family='Arial Unicode MS')\nplt.title(\"关键词共现网络中的话题社区\")\nplt.axis(\"off\")\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Market_files/figure-html/cell-22-output-1.png){}\n:::\n:::\n\n\n::: {.table .striped .hover caption=\"Semantic Categories of Keyword Communities\"}\n| Community Color | Community ID | Semantic Theme | Sample Keywords |\n|-----------------|-----------------|--------------------|--------------------|\n| 🌕 Brown | Community 1 | Trust & Wealth Management | trust, assets, fund, legal |\n| 🟢 Green | Community 2 | Client Service & Immigration | advisor, immigration, client, plan |\n| 🔵 Blue | Community 3 | HNW Assets & Cross-border Tax | bank, net worth, Hong Kong, tax |\n| ⚫ Gray | Community 4 | General/Neutral Words | include, mainly, plan, one |\n:::\n\n# 3 **Conclusion**\n\n",
    "supporting": [
      "Market_files"
    ],
    "filters": [],
    "includes": {}
  }
}